{
  "cells": [
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-09-25T02:45:48.506025Z",
          "start_time": "2018-09-25T02:45:48.474111Z"
        },
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "trusted": false,
        "id": "XLU2CdsvNg59"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "import sklearn\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Activation, Dropout\n",
        "from keras.optimizers import SGD\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import lightgbm as lgb\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import os\n",
        "import gc\n",
        "from itertools import combinations, chain\n",
        "from datetime import datetime\n",
        "print(os.listdir(\"../input\"))\n",
        "\n",
        "\n",
        "\n",
        "# Any results you write to the current directory are saved as output."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random forests? Cover trees? Not so fast, computer nerds. We're talking about the real thing.\n",
        "\n",
        "In this competition you are asked to predict the forest cover type (the predominant kind of tree cover) from cartographic variables. The actual forest cover type for a given 30 x 30 meter cell was determined from US Forest Service (USFS) Region 2 Resource Information System data. Independent variables were then derived from data obtained from the US Geological Survey and USFS. The data is in raw form and contains binary columns of data for qualitative independent variables such as wilderness areas and soil type.\n",
        "\n",
        "This study area includes four wilderness areas located in the Roosevelt National Forest of northern Colorado. These areas represent forests with minimal human-caused disturbances, so that existing forest cover types are more a result of ecological processes rather than forest management practices."
      ],
      "metadata": {
        "id": "O15RPXzSNwZS"
      }
    },
    {
      "metadata": {
        "_uuid": "1fc04f3cb613ee725c681a18f8bbcbcdb88af57d",
        "id": "FbiBDHo7Ng6K"
      },
      "cell_type": "markdown",
      "source": [
        "# summary"
      ]
    },
    {
      "metadata": {
        "_uuid": "968a5603b4bf10a83bc0054c21465141c8a9fcce",
        "id": "LSFbGJ-3Ng6L"
      },
      "cell_type": "markdown",
      "source": [
        "## model summary\n",
        "We created a total of 10 learning models and stacked their predicted by LightGBM.\n",
        "\n",
        "table of contents\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "_uuid": "fc395abfff45e2d6f729efe6ac68a8262aa5e863",
        "id": "cVFfjkZwNg6M"
      },
      "cell_type": "markdown",
      "source": [
        "# nadare's kernel"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-09-24T23:30:05.797806Z",
          "start_time": "2018-09-24T23:30:04.059462Z"
        },
        "trusted": false,
        "_uuid": "9e91ca19c0142cb111e1acabaeb5a27c22eef5a4",
        "id": "_9wpbOhgNg6M"
      },
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(\"../input/train.csv\")\n",
        "test_df = pd.read_csv(\"../input/test.csv\")\n",
        "smpsb = pd.read_csv(\"../input/sample_submission.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "885a2a8875b9a7f134c3cb64673fe32d1bb1d7c2",
        "id": "8VG3QNkXNg6N"
      },
      "cell_type": "markdown",
      "source": [
        "## preprocessing"
      ]
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "id": "Ifbz3espNg6N"
      },
      "cell_type": "markdown",
      "source": [
        "### EDA & leader board hacking"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-09-24T23:34:14.130390Z",
          "start_time": "2018-09-24T23:34:12.528688Z"
        },
        "trusted": false,
        "_uuid": "bdbb097f03fc4d277079c5fccc01fd9574728ece",
        "id": "L8vkw1x2Ng6N"
      },
      "cell_type": "code",
      "source": [
        "# First of all, let's see the distribution of each variable.\n",
        "# You can see that there is a big difference in distribution between training data and test data.\n",
        "\n",
        "from scipy.stats import gaussian_kde\n",
        "\n",
        "def compare_dist(ax, feature, i=0):\n",
        "    sns.kdeplot(train_df[feature], label=\"train\", ax=ax)\n",
        "    sns.kdeplot(test_df[feature], label=\"test\", ax=ax)\n",
        "\n",
        "\n",
        "def numeric_tile(plot_func):\n",
        "    fig, axs = plt.subplots(2, 5, figsize=(24, 6))\n",
        "    axs = axs.flatten()\n",
        "    \n",
        "    for i, (ax, col) in enumerate(zip(axs, train_df.columns.tolist()[1:11])):\n",
        "        plot_func(ax, col, i)\n",
        "        ax.set_title(col)\n",
        "    plt.tight_layout()\n",
        "    \n",
        "numeric_tile(compare_dist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-09-24T23:43:39.706816Z",
          "start_time": "2018-09-24T23:43:37.597405Z"
        },
        "trusted": false,
        "_uuid": "3ed05251dda674f52765dfb71298c93c2ee979f6",
        "id": "Qf0meehnNg6O"
      },
      "cell_type": "code",
      "source": [
        "# For the training data, display the distribution of variables for each target.\n",
        "\n",
        "# Please pay attention to \"Elevation\". The difference between the training data and the test data distribution is\n",
        "# thought to be due to the difference between the proportion of the target variables in the training data and the test data.\n",
        "\n",
        "def compare_target(ax, feature, i=0):\n",
        "    sns.kdeplot(train_df.loc[:, feature], label=\"train\", ax=ax)\n",
        "    sns.kdeplot(test_df.loc[:, feature], label=\"test\", ax=ax)\n",
        "    for target in range(1, 8):\n",
        "        sns.kdeplot(train_df.loc[train_df[\"Cover_Type\"] == target, feature], label=target, alpha=0.5, lw=1, ax=ax)\n",
        "\n",
        "numeric_tile(compare_target)        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-09-25T00:15:51.003553Z",
          "start_time": "2018-09-25T00:15:46.599325Z"
        },
        "trusted": false,
        "_uuid": "b3f75b110dc9d589866d8d6de4e0a1d6fcb25316",
        "id": "0FrPd29XNg6P"
      },
      "cell_type": "code",
      "source": [
        "# I was able to obtain the distribution of the test data by submitting prediction data with all the same purpose variables.\n",
        "\n",
        "\"\"\"\n",
        "smpsb = pd.read_csv(\"../input/sample_submission.csv\")\n",
        "for i in range(1, 8):\n",
        "    smpsb[\"Cover_Type\"] = i\n",
        "    smpsb.to_csv(\"all_{}.csv\".format(i), index=None)\"\"\"\n",
        "\n",
        "# and this is the magic number of this competition.\n",
        "type_ratio = np.array([0.37053, 0.49681, 0.05936, 0.00103, 0.01295, 0.02687, 0.03242])\n",
        "class_weight = {k: v for k, v in enumerate(type_ratio, start=1)}\n",
        "\n",
        "# By using these numbers, you can mimic the distribution of the test data from the training data.\n",
        "def compare_balanced_dist(ax, feature, i=0):\n",
        "    min_ = min(train_df[feature].min(), test_df[feature].min())\n",
        "    max_ = max(train_df[feature].max(), test_df[feature].max())\n",
        "    X = np.linspace(min_, max_, 1000)\n",
        "\n",
        "    sns.kdeplot(train_df[feature], label=\"train\", ax=ax)\n",
        "    sns.kdeplot(test_df[feature], label=\"test\", ax=ax)\n",
        "    btest = np.zeros(1000)\n",
        "    \n",
        "    for target in range(1, 8):\n",
        "        btest += gaussian_kde(train_df.loc[train_df[\"Cover_Type\"] == target, feature])(X) * type_ratio[target-1]\n",
        "    \n",
        "    ax.plot(X, btest, label=\"balanced\")\n",
        "    ax.legend()\n",
        "\n",
        "numeric_tile(compare_balanced_dist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "the goal is to compare the distribution of numerical variables between the training and test datasets, and also to create a balanced distribution that mimics the distribution of the test dataset.\n",
        "\n",
        "The compare_balanced_dist function is using the gaussian_kde function from the scipy.stats library to estimate the kernel density of each target variable in the training dataset. Then, the function is using the type_ratio array to adjust the density of each target variable, which is equivalent to mimicking the distribution of the test dataset.\n",
        "\n",
        "Finally, the function is plotting the kernel density of the training dataset, test dataset, and the balanced dataset, which allows for a comparison of the distribution of each variable between the three datasets.\n",
        "\n",
        "The numeric_tile function is generating a grid of plots for the first 10 numerical variables in the dataset, using the compare_balanced_dist function to plot the distributions.\n",
        "\n",
        "Overall, this code seems to be a useful way to compare and balance the distribution of numerical variables between training and test datasets in a classification problem."
      ],
      "metadata": {
        "id": "0XhOn365Oa-K"
      }
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "e00a36ba7782b587a997871baf314f3d934fdd78",
        "id": "8ksrhfUONg6Q"
      },
      "cell_type": "code",
      "source": [
        "# By using the following functions, it is possible to perform almost the same evaluation\n",
        "# as the leader board even in the local environment.\n",
        "\n",
        "def balanced_accuracy_score(y_true, y_pred):\n",
        "    return accuracy_score(y_true, y_pred, sample_weight=np.apply_along_axis(lambda x: type_ratio[x], 0, y_true-1))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-09-24T23:32:11.119296Z",
          "start_time": "2018-09-24T23:32:10.796827Z"
        },
        "_uuid": "11d9cb5a1a6a43a5e8e7b7a3e1b777822de29b12",
        "id": "xlRF5tyDNg6Q"
      },
      "cell_type": "markdown",
      "source": [
        "### feature engineering 1"
      ]
    },
    {
      "metadata": {
        "_uuid": "6e90c46e4445d14719f145e7a2cea75270424680",
        "id": "_off2IY4Ng6Q"
      },
      "cell_type": "markdown",
      "source": [
        "I will explain some of the features I consider important or unique."
      ]
    },
    {
      "metadata": {
        "_uuid": "8c2d853cbc8c1e198bb5c5c009bc6cb44652bc1e",
        "id": "qkael2rbNg6R"
      },
      "cell_type": "markdown",
      "source": [
        "#### Aspect"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-09-25T00:54:04.818905Z",
          "start_time": "2018-09-25T00:54:04.553586Z"
        },
        "trusted": false,
        "_uuid": "b595095fcd707c0a30ff4e3234c7f5bc5ad9c934",
        "id": "8jbriCdGNg6R"
      },
      "cell_type": "code",
      "source": [
        "# The angle can be divided into sine and cosine\n",
        "sin_ = np.sin(np.pi*train_df[\"Aspect\"]/180)\n",
        "cos_ = np.cos(np.pi*train_df[\"Aspect\"]/180)\n",
        "\n",
        "# However, if this feature quantity alone, the effect seems to be light.\n",
        "plt.figure(figsize=(5, 4))\n",
        "for i in range(1, 8):\n",
        "    cat = np.where(train_df[\"Cover_Type\"] == i)[0]\n",
        "    r = (.5+0.2*i)\n",
        "    plt.scatter(cos_[cat]*(r), sin_[cat]*(r), alpha=0.02*r, s=6, label=i)\n",
        "plt.xlim(-2, 3)\n",
        "plt.legend()\n",
        "plt.savefig(\"aspect.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-09-25T00:51:00.805728Z",
          "start_time": "2018-09-25T00:51:00.802709Z"
        },
        "_uuid": "a1199fd2d205a22f5d91bc988b2591de4e1922c5",
        "id": "YksysB0VNg6R"
      },
      "cell_type": "markdown",
      "source": [
        "#### degree to hydrology"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-09-25T00:52:23.588848Z",
          "start_time": "2018-09-25T00:52:23.585856Z"
        },
        "trusted": false,
        "_uuid": "933e53d39a16bab8ce40efe36e5a909211f4e6d4",
        "id": "dyRIybNSNg6R"
      },
      "cell_type": "code",
      "source": [
        "# this may be good feature but unfortunally i forgot to add my data\n",
        "hydro_h = train_df[\"Vertical_Distance_To_Hydrology\"]\n",
        "hydro_v = train_df[\"Horizontal_Distance_To_Hydrology\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-09-25T00:52:24.960833Z",
          "start_time": "2018-09-25T00:52:24.433217Z"
        },
        "trusted": false,
        "_uuid": "7e7c1fc8875a1b0ac9e70dc60a15cb60509543f2",
        "id": "GYJUkJ4BNg6S"
      },
      "cell_type": "code",
      "source": [
        "plt.scatter(hydro_h, hydro_v, s=1, c=train_df[\"Cover_Type\"], cmap=\"Set1\", alpha=0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-09-25T00:54:20.502007Z",
          "start_time": "2018-09-25T00:54:20.386289Z"
        },
        "trusted": false,
        "_uuid": "b48277e4eb95cdd55c415a4d6e25dc687447b0de",
        "id": "xipMMSI9Ng6S"
      },
      "cell_type": "code",
      "source": [
        "hydro_arctan = np.arctan((hydro_h+0.0001) / (hydro_v+0.0001))\n",
        "for i in range(1, 8):\n",
        "    cat = np.where(train_df[\"Cover_Type\"] == i)[0]\n",
        "    sns.kdeplot(hydro_arctan[cat])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-09-25T00:54:57.520582Z",
          "start_time": "2018-09-25T00:54:56.978022Z"
        },
        "trusted": false,
        "_uuid": "64ffc12e5f344534a878b0aea2e2c99bacde16ae",
        "id": "WPIDfGHiNg6S"
      },
      "cell_type": "code",
      "source": [
        "plt.scatter(hydro_arctan, np.pi*train_df[\"Slope\"]/180, c=train_df[\"Cover_Type\"], cmap=\"Set1\", s=1.5, alpha=0.7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "da688e40e90ed0f0b0a3bc2abd96a444d64a3318",
        "id": "NaL3oF8iNg6T"
      },
      "cell_type": "markdown",
      "source": [
        "#### target_encoding "
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-09-25T02:22:22.284518Z",
          "start_time": "2018-09-25T02:22:22.201740Z"
        },
        "trusted": false,
        "_uuid": "ea912ece3fa7e6022d5202a54c727ed3e9aaae4f",
        "id": "jRoGMsoSNg6T"
      },
      "cell_type": "code",
      "source": [
        "# this is the ratio of Wilderness_Area\n",
        "plt.figure(figsize=(6, 6))\n",
        "train_df.filter(regex=\"Wilder\").sum(axis=0).plot(\"pie\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-09-25T02:25:22.054405Z",
          "start_time": "2018-09-25T02:25:21.866933Z"
        },
        "trusted": false,
        "_uuid": "51deabb9bcfa6bc33ca903deeafa9f75ce1b9d8f",
        "id": "IUqXF5SFNg6T"
      },
      "cell_type": "code",
      "source": [
        "# and this is ratio of \"over_Type\" in each \"Wildereness_area\"\n",
        "wilder = (train_df.filter(regex=\"Wilder\") * np.array([1, 2, 3, 4])).sum(axis=1)\n",
        "fig, axs = plt.subplots(2, 2, figsize=(8, 8))\n",
        "axs = axs.flatten()\n",
        "for i, ax in enumerate(axs, start=1):\n",
        "    train_df.loc[wilder==i, \"Cover_Type\"].value_counts().sort_index().plot(\"pie\", ax=ax)\n",
        "    ax.set_title(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-09-25T02:30:23.381172Z",
          "start_time": "2018-09-25T02:30:21.736568Z"
        },
        "trusted": false,
        "_uuid": "e9de8a38df1a7a5206740188264757cc53ec70bf",
        "id": "d5Pw1Pr3Ng6U"
      },
      "cell_type": "code",
      "source": [
        "# This shows the expression of Soil_Type for the objective variable.\n",
        "plt.figure(figsize=(12, 4))\n",
        "sns.heatmap(train_df.iloc[:, -41:].sort_values(by=\"Cover_Type\").iloc[:, :-1].T, cmap=\"Greys_r\")\n",
        "for i in np.linspace(0, train_df.shape[0], 8)[1:]:\n",
        "    plt.axvline(i, c=\"r\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0459bec5c4616fa7cffaaa6ca41800962e9fd976",
        "id": "1WIelRydNg6U"
      },
      "cell_type": "markdown",
      "source": [
        "As indicated above, category values are considered to have a major role in classification.\n",
        "\n",
        "Therefore, in order to handle category values effectively, the ratio of object variables in each category value is added as a feature quantity.\n",
        "\n",
        "In order to prevent data leakage and not to excessively trust category values which have only a small number, we added values for 10 data as prior distribution to each category."
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-09-25T02:39:16.296728Z",
          "start_time": "2018-09-25T02:39:16.291741Z"
        },
        "trusted": false,
        "_uuid": "89a5c509e52da8f96bf25b285472e86f579682af",
        "id": "knT9V14RNg6U"
      },
      "cell_type": "code",
      "source": [
        "# this is the code\n",
        "def categorical_post_mean(x):\n",
        "    p = (x.values)*type_ratio\n",
        "    p = p/p.sum()*x.sum() + 10*type_ratio\n",
        "    return p/p.sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "85cf81e78ae32a4c0b98caf4262c3b67ad4c70c9",
        "id": "OKSOwP4MNg6V"
      },
      "cell_type": "markdown",
      "source": [
        "#### summarizes preprocessing"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-09-25T02:43:59.392818Z",
          "start_time": "2018-09-25T02:43:45.346194Z"
        },
        "trusted": false,
        "_uuid": "31f82acb720296e835f66fc6d3cb9dc7abd2cc27",
        "id": "B2dN_0VJNg6V"
      },
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(\"../input/train.csv\")\n",
        "test_df = pd.read_csv(\"../input/test.csv\")\n",
        "smpsb = pd.read_csv(\"../input/sample_submission.csv\")\n",
        "\n",
        "def main(train_df, test_df):\n",
        "    # this is public leaderboard ratio\n",
        "    start = datetime.now()\n",
        "    type_ratio = np.array([0.37053, 0.49681, 0.05936, 0.00103, 0.01295, 0.02687, 0.03242])\n",
        "    \n",
        "    total_df = pd.concat([train_df.iloc[:, :-1], test_df])\n",
        "    \n",
        "    # Aspect\n",
        "    total_df[\"Aspect_Sin\"] = np.sin(np.pi*total_df[\"Aspect\"]/180)\n",
        "    total_df[\"Aspect_Cos\"] = np.cos(np.pi*total_df[\"Aspect\"]/180)\n",
        "    print(\"Aspect\", (datetime.now() - start).seconds)\n",
        "    \n",
        "    # Hillshade\n",
        "    hillshade_col = [\"Hillshade_9am\", \"Hillshade_Noon\", \"Hillshade_3pm\"]\n",
        "    for col1, col2 in combinations(hillshade_col, 2):\n",
        "        total_df[col1 + \"_add_\" + col2] = total_df[col2] + total_df[col1]\n",
        "        total_df[col1 + \"_dif_\" + col2] = total_df[col2] - total_df[col1]\n",
        "        total_df[col1 + \"_div_\" + col2] = (total_df[col2]+0.01) / (total_df[col1]+0.01)\n",
        "        total_df[col1 + \"_abs_\" + col2] = np.abs(total_df[col2] - total_df[col1])\n",
        "    \n",
        "    total_df[\"Hillshade_mean\"] = total_df[hillshade_col].mean(axis=1)\n",
        "    total_df[\"Hillshade_std\"] = total_df[hillshade_col].std(axis=1)\n",
        "    total_df[\"Hillshade_max\"] = total_df[hillshade_col].max(axis=1)\n",
        "    total_df[\"Hillshade_min\"] = total_df[hillshade_col].min(axis=1)\n",
        "    print(\"Hillshade\", (datetime.now() - start).seconds)\n",
        "    \n",
        "    # Hydrology ** I forgot to add arctan\n",
        "    total_df[\"Degree_to_Hydrology\"] = ((total_df[\"Vertical_Distance_To_Hydrology\"] + 0.001) /\n",
        "                                       (total_df[\"Horizontal_Distance_To_Hydrology\"] + 0.01))\n",
        "    \n",
        "    # Holizontal\n",
        "    horizontal_col = [\"Horizontal_Distance_To_Hydrology\",\n",
        "                      \"Horizontal_Distance_To_Roadways\",\n",
        "                      \"Horizontal_Distance_To_Fire_Points\"]\n",
        "    \n",
        "    \n",
        "    for col1, col2 in combinations(hillshade_col, 2):\n",
        "        total_df[col1 + \"_add_\" + col2] = total_df[col2] + total_df[col1]\n",
        "        total_df[col1 + \"_dif_\" + col2] = total_df[col2] - total_df[col1]\n",
        "        total_df[col1 + \"_div_\" + col2] = (total_df[col2]+0.01) / (total_df[col1]+0.01)\n",
        "        total_df[col1 + \"_abs_\" + col2] = np.abs(total_df[col2] - total_df[col1])\n",
        "    print(\"Holizontal\", (datetime.now() - start).seconds)\n",
        "    \n",
        "    \n",
        "    def categorical_post_mean(x):\n",
        "        p = (x.values)*type_ratio\n",
        "        p = p/p.sum()*x.sum() + 10*type_ratio\n",
        "        return p/p.sum()\n",
        "    \n",
        "    # Wilder\n",
        "    wilder = pd.DataFrame([(train_df.iloc[:, 11:15] * np.arange(1, 5)).sum(axis=1),\n",
        "                          train_df.Cover_Type]).T\n",
        "    wilder.columns = [\"Wilder_Type\", \"Cover_Type\"]\n",
        "    wilder[\"one\"] = 1\n",
        "    piv = wilder.pivot_table(values=\"one\",\n",
        "                             index=\"Wilder_Type\",\n",
        "                             columns=\"Cover_Type\",\n",
        "                             aggfunc=\"sum\").fillna(0)\n",
        "    \n",
        "    tmp = pd.DataFrame(piv.apply(categorical_post_mean, axis=1).tolist()).reset_index()\n",
        "    tmp[\"index\"] = piv.sum(axis=1).index\n",
        "    tmp.columns = [\"Wilder_Type\"] + [\"Wilder_prob_ctype_{}\".format(i) for i in range(1, 8)]\n",
        "    tmp[\"Wilder_Type_count\"] = piv.sum(axis=1).values\n",
        "    \n",
        "    total_df[\"Wilder_Type\"] = (total_df.filter(regex=\"Wilder\") * np.arange(1, 5)).sum(axis=1)\n",
        "    total_df = total_df.merge(tmp, on=\"Wilder_Type\", how=\"left\")\n",
        "    \n",
        "    for i in range(7):\n",
        "        total_df.loc[:, \"Wilder_prob_ctype_{}\".format(i+1)] = total_df.loc[:, \"Wilder_prob_ctype_{}\".format(i+1)].fillna(type_ratio[i])\n",
        "    total_df.loc[:, \"Wilder_Type_count\"] = total_df.loc[:, \"Wilder_Type_count\"].fillna(0)\n",
        "    print(\"Wilder_type\", (datetime.now() - start).seconds)\n",
        "    \n",
        "    \n",
        "    # Soil type\n",
        "    soil = pd.DataFrame([(train_df.iloc[:, -41:-1] * np.arange(1, 41)).sum(axis=1),\n",
        "                          train_df.Cover_Type]).T\n",
        "    soil.columns = [\"Soil_Type\", \"Cover_Type\"]\n",
        "    soil[\"one\"] = 1\n",
        "    piv = soil.pivot_table(values=\"one\",\n",
        "                           index=\"Soil_Type\",\n",
        "                           columns=\"Cover_Type\",\n",
        "                           aggfunc=\"sum\").fillna(0)\n",
        "    \n",
        "    tmp = pd.DataFrame(piv.apply(categorical_post_mean, axis=1).tolist()).reset_index()\n",
        "    tmp[\"index\"] = piv.sum(axis=1).index\n",
        "    tmp.columns = [\"Soil_Type\"] + [\"Soil_prob_ctype_{}\".format(i) for i in range(1, 8)]\n",
        "    tmp[\"Soil_Type_count\"] = piv.sum(axis=1).values\n",
        "    \n",
        "    total_df[\"Soil_Type\"] = (total_df.filter(regex=\"Soil\") * np.arange(1, 41)).sum(axis=1)\n",
        "    total_df = total_df.merge(tmp, on=\"Soil_Type\", how=\"left\")\n",
        "    \n",
        "    for i in range(7):\n",
        "        total_df.loc[:, \"Soil_prob_ctype_{}\".format(i+1)] = total_df.loc[:, \"Soil_prob_ctype_{}\".format(i+1)].fillna(type_ratio[i])\n",
        "    total_df.loc[:, \"Soil_Type_count\"] = total_df.loc[:, \"Soil_Type_count\"].fillna(0)\n",
        "    print(\"Soil_type\", (datetime.now() - start).seconds)\n",
        "    \n",
        "    icol = total_df.select_dtypes(np.int64).columns\n",
        "    fcol = total_df.select_dtypes(np.float64).columns\n",
        "    total_df.loc[:, icol] = total_df.loc[:, icol].astype(np.int32)\n",
        "    total_df.loc[:, fcol] = total_df.loc[:, fcol].astype(np.float32)\n",
        "    return total_df\n",
        "\n",
        "total_df = main(train_df, test_df)\n",
        "one_col = total_df.filter(regex=\"(Type\\d+)|(Area\\d+)\").columns\n",
        "total_df = total_df.drop(one_col, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is a script for feature engineering for the Forest Cover Type Prediction problem, where the goal is to predict the type of tree cover in a forest given various environmental features.\n",
        "\n",
        "The script starts by reading in three CSV files: \"train.csv\", \"test.csv\", and \"sample_submission.csv\". The training and test CSV files contain the input features, while the sample submission CSV file contains the expected format for the predicted output.\n",
        "\n",
        "Next, the script defines a function called \"main\" that takes two arguments: \"train_df\" and \"test_df\", which are the dataframes read in from the CSV files. Within the \"main\" function, several new features are generated based on the existing features in the dataframes. The new features are:\n",
        "\n",
        "- Aspect_Sin and Aspect_Cos: The sine and cosine of the aspect angle in degrees.\n",
        "- Hillshade features: Various combinations of the three hillshade features (Hillshade_9am, Hillshade_Noon, and Hillshade_3pm) are created, including their sum, difference, ratio, and absolute difference. Additionally, new features are created that are the mean, standard deviation, minimum, and maximum of the hillshade features.\n",
        "- Degree_to_Hydrology: The vertical distance to water divided by the horizontal distance to water.\n",
        "- Holizontal features: Similar to the hillshade features, various combinations of the three horizontal distance features (Horizontal_Distance_To_Hydrology, Horizontal_Distance_To_Roadways, and Horizontal_Distance_To_Fire_Points) are created.\n",
        "- Wilder_Type features: A new feature is created by summing up the product of the four wilder features (Wilderness_Area1-4) and the integers 1-4, respectively. This new feature is then used to create a pivot table that shows the counts of each wilder type for each cover type. A function is defined that takes in the count of each wilder type and returns a probability distribution for the cover type based on the public leaderboard ratio. The function is applied to each wilder type in the pivot table, and the resulting probabilities are added as new features to the total dataframe.\n",
        "- Soil_Type features: Similar to the Wilder_Type features, a new feature is created by summing up the product of the 40 soil type features (Soil_Type1-40) and the integers 1-40, respectively. This new feature is then used to create a pivot table that shows the counts of each soil type for each cover type. A function is defined that takes in the count of each soil type and returns a probability distribution for the cover type based on the public leaderboard ratio. The function is applied to each soil type in the pivot table, and the resulting probabilities are added as new features to the total dataframe.\n",
        "\n",
        "The script also prints out the time it takes to generate each set of features. The output of the function is the total dataframe that includes all of the new features."
      ],
      "metadata": {
        "id": "nSvmwixCO0sa"
      }
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-09-25T02:44:12.546743Z",
          "start_time": "2018-09-25T02:44:12.435041Z"
        },
        "trusted": false,
        "_uuid": "e4fc31a4e0bf23728d886147a3430ff10de919b7",
        "id": "naQxHvLXNg6W"
      },
      "cell_type": "code",
      "source": [
        "y = train_df[\"Cover_Type\"].values\n",
        "X = total_df[total_df[\"Id\"] <= 15120].drop(\"Id\", axis=1)\n",
        "X_test = total_df[total_df[\"Id\"] > 15120].drop(\"Id\", axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-09-25T02:46:06.547203Z",
          "start_time": "2018-09-25T02:46:06.154254Z"
        },
        "trusted": false,
        "_uuid": "0504ef9435ceaba8b37bfbde45ec416ac7e7ff34",
        "id": "nqpmdDIFNg6W"
      },
      "cell_type": "code",
      "source": [
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "02527a3340e8079cdb7a7f978d06ffe36feacba4",
        "id": "mfZ8J9AQNg6W"
      },
      "cell_type": "markdown",
      "source": [
        "### KNN features and Decision tree feature"
      ]
    },
    {
      "metadata": {
        "_uuid": "4db26e4628d5a1933c745629c2f1cd1a38499708",
        "id": "eDGdAMRsNg6X"
      },
      "cell_type": "markdown",
      "source": [
        "For the variable created up to the above, the decision tree and the k-nearest neighbor method are applied after narrowing down the number of variables and adding the prediction probability as the feature amount. \n",
        "\n",
        "I decided the combination of variables to be used last and the setting of parameters based on Multi-class logarithmic loss while considering diversity."
      ]
    },
    {
      "metadata": {
        "_uuid": "5b664180bc7f55a5f05226caf1956bb50b3a5f98",
        "id": "wmzo7MVsNg6X"
      },
      "cell_type": "markdown",
      "source": [
        "#### KNN_feature"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-09-25T02:58:54.238533Z",
          "start_time": "2018-09-25T02:58:54.172710Z"
        },
        "_uuid": "eaf1fe074875abe7c8d997aa2c3b3355825f7d27",
        "trusted": false,
        "id": "3Ynz1HGINg6X"
      },
      "cell_type": "code",
      "source": [
        "all_set =  [['Elevation', 500],\n",
        "            ['Horizontal_Distance_To_Roadways', 500],\n",
        "            ['Horizontal_Distance_To_Fire_Points', 500],\n",
        "            ['Horizontal_Distance_To_Hydrology', 500],\n",
        "            ['Hillshade_9am', 500],\n",
        "            ['Aspect', 500],\n",
        "            ['Hillshade_3pm', 500],\n",
        "            ['Slope', 500],\n",
        "            ['Hillshade_Noon', 500],\n",
        "            ['Vertical_Distance_To_Hydrology', 500],\n",
        "            ['Elevation_PLUS_Vertical_Distance_To_Hydrology', 200],\n",
        "            ['Elevation_PLUS_Hillshade_9am_add_Hillshade_Noon', 200],\n",
        "            ['Elevation_PLUS_Aspect', 200],\n",
        "            ['Elevation_PLUS_Hillshade_Noon_dif_Hillshade_3pm', 200],\n",
        "            ['Elevation_PLUS_Hillshade_Noon_abs_Hillshade_3pm', 200],\n",
        "            ['Elevation_PLUS_Hillshade_9am', 200],\n",
        "            ['Elevation_PLUS_Horizontal_Distance_To_Hydrology', 200],\n",
        "            ['Elevation_PLUS_Horizontal_Distance_To_Roadways', 100],\n",
        "            ['Elevation_PLUS_Vertical_Distance_To_Hydrology', 200],\n",
        "            ['Wilder_Type_PLUS_Elevation', 500],\n",
        "            ['Wilder_Type_PLUS_Hillshade_Noon_div_Hillshade_3pm', 500],\n",
        "            ['Wilder_Type_PLUS_Degree_to_Hydrology', 200],\n",
        "            ['Wilder_Type_PLUS_Hillshade_9am_div_Hillshade_3pm', 500],\n",
        "            ['Wilder_Type_PLUS_Aspect_Cos', 500],\n",
        "            ['Hillshade_9am_dif_Hillshade_Noon_PLUS_Hillshade_Noon_dif_Hillshade_3pm', 200],\n",
        "            ['Hillshade_Noon_PLUS_Hillshade_3pm', 200],\n",
        "            ['Hillshade_Noon_add_Hillshade_3pm_PLUS_Hillshade_Noon_dif_Hillshade_3pm', 200]]\n",
        "\n",
        "\n",
        "def simple_feature_scores2(clf, cols, test=False, **params):\n",
        "    scores = []\n",
        "    bscores = []\n",
        "    lscores = []\n",
        "    \n",
        "    X_preds = np.zeros((len(y), 7))\n",
        "    scl = StandardScaler().fit(X.loc[:, cols])\n",
        "    \n",
        "    for train, val in StratifiedKFold(n_splits=10, shuffle=True, random_state=2018).split(X, y):\n",
        "        X_train = scl.transform(X.loc[train, cols])\n",
        "        X_val = scl.transform(X.loc[val, cols])\n",
        "        y_train = y[train]\n",
        "        y_val = y[val]\n",
        "        C = clf(**params) \n",
        "\n",
        "        C.fit(X_train, y_train)\n",
        "        X_preds[val] = C.predict_proba(X_val)\n",
        "        #scores.append(accuracy_score(y_val, C.predict(X_val)))\n",
        "        #bscores.append(balanced_accuracy_score(y_val, C.predict(X_val)))\n",
        "        #lscores.append(log_loss(y_val, C.predict_proba(X_val), labels=list(range(1, 8))))\n",
        "    \n",
        "    if test:\n",
        "        X_test_select = scl.transform(X_test.loc[:, cols])\n",
        "        C = clf(**params)\n",
        "        C.fit(scl.transform(X.loc[:, cols]), y)\n",
        "        X_test_preds = C.predict_proba(X_test_select)\n",
        "    else:\n",
        "        X_test_preds = None\n",
        "    return scores, bscores, lscores, X_preds, X_test_preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a Python function called `simple_feature_scores2` that takes in some parameters and returns several values. \n",
        "\n",
        "The function uses a machine learning classifier specified by the parameter `clf` to train on a training set, validate on a validation set, and predict on a test set. The training and validation sets are created using the StratifiedKFold cross-validation method, and the features used for training and prediction are specified by the parameter `cols`.\n",
        "\n",
        "The function returns four lists: `scores`, `bscores`, `lscores`, `X_preds`, and `X_test_preds`.\n",
        "\n",
        "The `scores` list contains the accuracy scores for each fold of the cross-validation process.\n",
        "\n",
        "The `bscores` list contains the balanced accuracy scores for each fold of the cross-validation process.\n",
        "\n",
        "The `lscores` list contains the log loss scores for each fold of the cross-validation process.\n",
        "\n",
        "The `X_preds` array contains the predicted probabilities for each sample in the training set, for each class.\n",
        "\n",
        "The `X_test_preds` array contains the predicted probabilities for each sample in the test set, for each class. If `test=True` is passed as an argument, the function will return this array, otherwise it will return `None`."
      ],
      "metadata": {
        "id": "OQy3nmVXRgCH"
      }
    },
    {
      "metadata": {
        "_uuid": "d5cc637ad06e0e1d3fe5924b5dbf15f1ee3d2a38",
        "trusted": false,
        "id": "u2UaxBTnNg6Y"
      },
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "import gc\n",
        "from multiprocessing import Pool\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "preds = []\n",
        "test_preds = []\n",
        "for colname, neighbor in tqdm(all_set):\n",
        "    gc.collect()\n",
        "    #print(colname, depth)\n",
        "    ts, tbs, ls, pred, test_pred = simple_feature_scores2(KNeighborsClassifier,\n",
        "                                                          colname.split(\"_PLUS_\"),\n",
        "                                                          test=True,\n",
        "                                                          n_neighbors=neighbor)\n",
        "    preds.append(pred)\n",
        "    test_preds.append(test_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems like you're trying to calculate feature scores for all the features in `all_set` using the `simple_feature_scores2` function. The function takes a classifier (`clf`), a list of column names (`cols`), and some other arguments (`**params`) as inputs, and returns some scores and predictions.\n",
        "\n",
        "You are using the `KNeighborsClassifier` classifier with varying numbers of neighbors (`n_neighbors=neighbor`) for each column name in `all_set`. You are then collecting the predictions for each fold (`pred`) and test set (`test_pred`) into `preds` and `test_preds`, respectively.\n",
        "\n",
        "This code is using the `multiprocessing.Pool` function to parallelize the computation of feature scores across all the columns in `all_set`. By doing so, it can speed up the computation significantly. \n",
        "\n",
        "However, note that you need to define a function that will be called by `Pool.map()` to perform the computation in parallel. Currently, the code you provided is not defining such a function."
      ],
      "metadata": {
        "id": "85bvrvvzRqPO"
      }
    },
    {
      "metadata": {
        "_uuid": "1c301a98315266944aae3498c863a9005623ec87",
        "trusted": false,
        "id": "hfLpLsBnNg6Y"
      },
      "cell_type": "code",
      "source": [
        "cols = list(chain.from_iterable([[col[0] + \"_KNN_{}\".format(i) for i in range(1, 8)] for col in all_set]))\n",
        "knn_train_df = pd.DataFrame(np.hstack(preds)).astype(np.float32)\n",
        "knn_train_df.columns = cols\n",
        "knn_test_df = pd.DataFrame(np.hstack(test_preds)).astype(np.float32)\n",
        "knn_test_df.columns = cols\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "cc362ea934b2b07aaa900a3f1510118aecd3eecc",
        "id": "xONOCPRENg6Y"
      },
      "cell_type": "markdown",
      "source": [
        "#### DT_features"
      ]
    },
    {
      "metadata": {
        "_uuid": "00347f67de008070fcdc4caf7d3e8bace8ba2dbd",
        "trusted": false,
        "id": "PFnXynsNNg6Z"
      },
      "cell_type": "code",
      "source": [
        "all_set = [['Elevation', 4],\n",
        "           ['Horizontal_Distance_To_Roadways', 4],\n",
        "           ['Horizontal_Distance_To_Fire_Points', 3],\n",
        "           ['Horizontal_Distance_To_Hydrology', 4],\n",
        "           ['Hillshade_9am', 3],\n",
        "           ['Vertical_Distance_To_Hydrology', 3],\n",
        "           ['Slope', 4],\n",
        "           ['Aspect', 4],\n",
        "           ['Hillshade_3pm', 3],\n",
        "           ['Hillshade_Noon', 3],\n",
        "           ['Degree_to_Hydrology', 3],\n",
        "           ['Hillshade_Noon_dif_Hillshade_3pm', 3],\n",
        "           ['Hillshade_Noon_abs_Hillshade_3pm', 3],\n",
        "           ['Elevation_PLUS_Hillshade_9am_add_Hillshade_Noon', 5],\n",
        "           ['Elevation_PLUS_Hillshade_max', 5],\n",
        "           ['Elevation_PLUS_Horizontal_Distance_To_Hydrology', 5],\n",
        "           ['Aspect_Sin_PLUS_Aspect_Cos_PLUS_Elevation', 5],\n",
        "           ['Elevation_PLUS_Horizontal_Distance_To_Fire_Points', 5],\n",
        "           ['Wilder_Type_PLUS_Elevation', 5],\n",
        "           ['Elevation_PLUS_Hillshade_9am', 5],\n",
        "           ['Elevation_PLUS_Degree_to_Hydrology', 5],\n",
        "           ['Wilder_Type_PLUS_Horizontal_Distance_To_Roadways', 5],\n",
        "           ['Wilder_Type_PLUS_Hillshade_9am_add_Hillshade_Noon', 4],\n",
        "           ['Wilder_Type_PLUS_Horizontal_Distance_To_Hydrology', 5],\n",
        "           ['Wilder_Type_PLUS_Hillshade_Noon_abs_Hillshade_3pm', 4],\n",
        "           ['Hillshade_9am_add_Hillshade_Noon_PLUS_Hillshade_std', 4],\n",
        "           ['Hillshade_9am_PLUS_Hillshade_9am_add_Hillshade_Noon', 4],\n",
        "           ['Hillshade_9am_add_Hillshade_Noon_PLUS_Hillshade_Noon_add_Hillshade_3pm', 5]]\n",
        "\n",
        "def simple_feature_scores(clf, cols, test=False, **params):\n",
        "    scores = []\n",
        "    bscores = []\n",
        "    lscores = []\n",
        "    \n",
        "    X_preds = np.zeros((len(y), 7))\n",
        "    \n",
        "    \n",
        "    for train, val in StratifiedKFold(n_splits=10, shuffle=True, random_state=2018).split(X, y):\n",
        "        X_train = X.loc[train, cols]\n",
        "        X_val = X.loc[val, cols]\n",
        "        y_train = y[train]\n",
        "        y_val = y[val]\n",
        "        C = clf(**params) \n",
        "\n",
        "        C.fit(X_train, y_train)\n",
        "        X_preds[val] = C.predict_proba(X_val)\n",
        "        #scores.append(accuracy_score(y_val, C.predict(X_val)))\n",
        "        #bscores.append(balanced_accuracy_score(y_val, C.predict(X_val)))\n",
        "        #lscores.append(log_loss(y_val, C.predict_proba(X_val), labels=list(range(1, 8))))\n",
        "    \n",
        "    if test:\n",
        "        X_test_select = X_test.loc[:, cols]\n",
        "        C = clf(**params)\n",
        "        C.fit(X.loc[:, cols], y)\n",
        "        X_test_preds = C.predict_proba(X_test_select)\n",
        "    else:\n",
        "        X_test_preds = None\n",
        "    return scores, bscores, lscores, X_preds, X_test_preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code is defining a function called `simple_feature_scores` that trains a given classifier on a set of features and returns several scores and predictions. \n",
        "\n",
        "The function takes the following parameters:\n",
        "- `clf`: the classifier to use.\n",
        "- `cols`: a list of feature names to use for training.\n",
        "- `test`: a boolean flag indicating whether to use the function for training or for prediction on a test set.\n",
        "- `**params`: additional keyword arguments that are passed to the classifier constructor.\n",
        "\n",
        "The function first initializes empty lists for `scores`, `bscores`, and `lscores` that will hold different metrics of model performance. It then creates an array `X_preds` of zeros of shape `(len(y), 7)` where `y` is the target variable. \n",
        "\n",
        "Next, the function uses `StratifiedKFold` to split the data into training and validation sets and trains the given classifier on the training set. The classifier is then used to make predictions on the validation set and the predictions are appended to the `X_preds` array. \n",
        "\n",
        "Finally, if the `test` flag is `True`, the function trains the classifier on the entire dataset and makes predictions on the test set. Otherwise, it returns `None` for `X_test_preds`.\n",
        "\n",
        "The function returns the following values:\n",
        "- `scores`: a list of accuracy scores on the validation sets for each fold.\n",
        "- `bscores`: a list of balanced accuracy scores on the validation sets for each fold.\n",
        "- `lscores`: a list of log loss scores on the validation sets for each fold.\n",
        "- `X_preds`: an array of shape `(len(y), 7)` containing the predicted probabilities on the validation sets.\n",
        "- `X_test_preds`: an array of shape `(len(X_test), 7)` containing the predicted probabilities on the test set."
      ],
      "metadata": {
        "id": "5oNKbz5MR7E8"
      }
    },
    {
      "metadata": {
        "_uuid": "882ab485d7b1be411b67d0d244cee7e9e078d226",
        "trusted": false,
        "id": "g8O-59GvNg6Z"
      },
      "cell_type": "code",
      "source": [
        "preds = []\n",
        "test_preds = []\n",
        "for colname, depth in tqdm(all_set):\n",
        "    #print(colname, depth)\n",
        "    ts, tbs, ls, pred, test_pred = simple_feature_scores(DecisionTreeClassifier,\n",
        "                                                         colname.split(\"_PLUS_\"),\n",
        "                                                         test=True,\n",
        "                                                         max_depth=depth)\n",
        "    preds.append(pred)\n",
        "    test_preds.append(test_pred)\n",
        "\n",
        "cols = list(chain.from_iterable([[col[0] + \"_DT_{}\".format(i) for i in range(1, 8)] for col in all_set]))\n",
        "dt_train_df = pd.DataFrame(np.hstack(preds)).astype(np.float32)\n",
        "dt_train_df.columns = cols\n",
        "\n",
        "dt_test_df = pd.DataFrame(np.hstack(test_preds)).astype(np.float32)\n",
        "dt_test_df.columns = cols"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "650ae398572fa740879dc785a5b461317086ef8c",
        "trusted": false,
        "id": "Sm2o3dWnNg6s"
      },
      "cell_type": "code",
      "source": [
        "# target encoding features(1.2.3)\n",
        "te_train_df = total_df.filter(regex=\"ctype\").iloc[:len(train_df)]\n",
        "te_test_df = total_df.filter(regex=\"ctype\").iloc[len(train_df):]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c4c32b082b39c89180db5b547ecf2b0571aaf861",
        "trusted": false,
        "id": "R92uaVP2Ng6t"
      },
      "cell_type": "code",
      "source": [
        "train_level2 = train_df[[\"Id\"]]\n",
        "test_level2 = test_df[[\"Id\"]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f349e101f81a8f3d4af953d227943254a760413e",
        "id": "cwPrQV43Ng6t"
      },
      "cell_type": "markdown",
      "source": [
        "## modeling"
      ]
    },
    {
      "metadata": {
        "_uuid": "095ef00c2099aba8fd04b9e9d1694249d2feb2c5",
        "id": "tKn4qeNqNg6t"
      },
      "cell_type": "markdown",
      "source": [
        "I have created 6 models\n",
        "\n",
        "without KNN&DT features\n",
        "* Random Forest Classifier\n",
        "* PCA & K-nearest Neighbors Classifier\n",
        "* LightGBM\n",
        "\n",
        "with KNN & DT features\n",
        "* Random Forest Classifier\n",
        "* Logistic Regression\n",
        "* LightGBM\n",
        "\n",
        "Using these learning machines, data for stacking was created using 10-fold cross validation."
      ]
    },
    {
      "metadata": {
        "_uuid": "f0d1dd30a7e2af14a04f8250b92cb14eda93de18",
        "id": "Sh7bKyRBNg6u"
      },
      "cell_type": "markdown",
      "source": [
        "### without KNN&DT feature"
      ]
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "ca42a3cd6f0f284ed94472a66e0610801e909552",
        "id": "YMTLxDkzNg6u"
      },
      "cell_type": "code",
      "source": [
        "y = train_df[\"Cover_Type\"].values\n",
        "X = total_df[total_df[\"Id\"] <= 15120].drop(\"Id\", axis=1)\n",
        "X_test = total_df[total_df[\"Id\"] > 15120].drop(\"Id\", axis=1)\n",
        "type_ratio = np.array([0.37053, 0.49681, 0.05936, 0.00103, 0.01295, 0.02687, 0.03242])\n",
        "class_weight = {k: v for k, v in enumerate(type_ratio, start=1)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "efd524163b32998c065c279e8cf10cd48b9c2142",
        "id": "k8vcfp0iNg6u"
      },
      "cell_type": "markdown",
      "source": [
        "#### Random forest classifier"
      ]
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "719e2a31d0667c8dfd2b332349a8367bbb57691c",
        "id": "Ia5x6KuvNg6v"
      },
      "cell_type": "code",
      "source": [
        "RFC1_col = [\"RFC1_{}_proba\".format(i) for i in range(1, 8)]\n",
        "for col in RFC1_col:\n",
        "    train_level2.loc[:, col] = 0\n",
        "    test_level2.loc[:, col] = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "5bf8085715c17f5117f867d2df9cab077b167714",
        "id": "hm0f5JzwNg6v"
      },
      "cell_type": "code",
      "source": [
        "rfc = RandomForestClassifier(n_estimators=150,\n",
        "                             max_depth=12,\n",
        "                             class_weight=class_weight,\n",
        "                             n_jobs=-1)\n",
        "\n",
        "confusion = np.zeros((7, 7))\n",
        "scores = []\n",
        "for train, val in tqdm(StratifiedKFold(n_splits=10, random_state=2434, shuffle=True).split(X, y)):\n",
        "    X_train = X.iloc[train, :]\n",
        "    X_val = X.iloc[val, :]\n",
        "\n",
        "    y_train = y[train]\n",
        "    y_val = y[val]\n",
        "    rfc.fit(X_train, y_train)\n",
        "    y_val_pred = rfc.predict(X_val)\n",
        "    y_val_proba = rfc.predict_proba(X_val)\n",
        "    \n",
        "    confusion += confusion_matrix(y_val, y_val_pred)    \n",
        "    train_level2.loc[val, RFC1_col] = y_val_proba\n",
        "    scores.append(balanced_accuracy_score(y_val, y_val_pred))\n",
        "\n",
        "rfc.fit(X, y)\n",
        "test_level2.loc[:, RFC1_col] = rfc.predict_proba(X_test)\n",
        "#smpsb.loc[:, \"Cover_Type\"] = rfc.predict(X_test)\n",
        "#smpsb.to_csv(\"RFC1.csv\", index=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "040b3893919ef650c0be96b562393fdff12ed226",
        "id": "jg7uTJxBNg6v"
      },
      "cell_type": "code",
      "source": [
        "print(np.mean(scores))\n",
        "sns.heatmap(confusion, cmap=\"Blues\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0bbae34ea9b2bf5a479c0e9aae0d7a13dcafd168",
        "id": "VNGnLpfLNg6w"
      },
      "cell_type": "markdown",
      "source": [
        "#### PCA & KNN"
      ]
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "524d158a396206814880ee7d682406dd8eb2aa37",
        "id": "XYOLmISVNg6w"
      },
      "cell_type": "code",
      "source": [
        "KNN1_col = [\"KNN1_{}_proba\".format(i) for i in range(1, 8)]\n",
        "for col in KNN1_col:\n",
        "    train_level2.loc[:, col] = 0\n",
        "    test_level2.loc[:, col] = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "65537269007f3bee16e8079db738e065e704e2ef",
        "id": "3a7ZcB28Ng6w"
      },
      "cell_type": "code",
      "source": [
        "cat_col = X.filter(regex=\"Soil_Type|Wilderness\").columns.tolist()[:-1] + [\"Wilder_Type\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "32e8c05574e03cc14beed390ac5b1c520163c126",
        "id": "3teX1wkTNg6w"
      },
      "cell_type": "code",
      "source": [
        "knn = KNeighborsClassifier(n_neighbors=2, n_jobs=-1)\n",
        "\n",
        "scl = StandardScaler().fit(X_test.drop(cat_col, axis=1))\n",
        "X_scl = scl.transform(X.drop(cat_col, axis=1))\n",
        "X_test_scl = scl.transform(X_test.drop(cat_col, axis=1))\n",
        "pca = PCA(n_components=23).fit(X_test_scl)\n",
        "X_pca = pca.transform(X_scl)\n",
        "X_test_pca = pca.transform(X_test_scl)\n",
        "\n",
        "confusion = np.zeros((7, 7))\n",
        "scores = []\n",
        "for train, val in tqdm(StratifiedKFold(n_splits=10, random_state=2434, shuffle=True).split(X, y)):\n",
        "    X_train = X_pca[train]\n",
        "    X_val = X_pca[val]\n",
        "\n",
        "    y_train = y[train]\n",
        "    y_val = y[val]\n",
        "    knn.fit(X_train, y_train)\n",
        "    y_val_pred = knn.predict(X_val)\n",
        "    y_val_proba = knn.predict_proba(X_val)\n",
        "    \n",
        "    confusion += confusion_matrix(y_val, y_val_pred)    \n",
        "    train_level2.loc[val, KNN1_col] = y_val_proba\n",
        "    scores.append(balanced_accuracy_score(y_val, y_val_pred))\n",
        "\n",
        "knn.fit(X_pca, y)\n",
        "test_level2.loc[:, KNN1_col] = knn.predict_proba(X_test_pca)\n",
        "#smpsb.loc[:, \"Cover_Type\"] = knn.predict(X_test_pca)\n",
        "#smpsb.to_csv(\"KNN1.csv\", index=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code appears to be implementing a k-nearest neighbors (KNN) classifier using PCA for feature selection. Here are some details on what the code is doing:\n",
        "\n",
        "- A `KNeighborsClassifier` object is created with 2 neighbors and using all available CPUs (`n_jobs=-1`).\n",
        "- A `StandardScaler` object is created and fitted on `X_test` with categorical columns dropped. The scaler is then used to transform both `X` and `X_test`, dropping categorical columns from both.\n",
        "- A `PCA` object is created and fitted on the transformed `X_test` data with 23 components.\n",
        "- The `X` and `X_test` data are then transformed using the fitted `StandardScaler` and `PCA` objects.\n",
        "- A confusion matrix is initialized with 7 rows and 7 columns (7 classes).\n",
        "- A `StratifiedKFold` object is created with 10 splits, random state 2434, and shuffling.\n",
        "- The `KNeighborsClassifier` object is fit on each split's training data (transformed with `StandardScaler` and `PCA`), and then used to predict class probabilities on the validation data. The predicted probabilities are stored in the `train_level2` dataframe in the `KNN1_col` column.\n",
        "- The confusion matrix is updated using the predicted and true labels for the validation data.\n",
        "- The balanced accuracy score is calculated for each split and stored in the `scores` list.\n",
        "- After all splits, the `KNeighborsClassifier` object is fit on the full transformed training data, and then used to predict class probabilities on the transformed test data. The predicted probabilities are stored in the `test_level2` dataframe in the `KNN1_col` column.\n",
        "\n",
        "Note that this code is using only a single value of `n_neighbors` (2), and that it is using class probabilities instead of predicted classes for evaluation."
      ],
      "metadata": {
        "id": "UjcyCGZISHUl"
      }
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "1e1987f27e1b1a580c146796a16b56921ee16ce6",
        "id": "edi3YgLWNg6x"
      },
      "cell_type": "code",
      "source": [
        "print(np.mean(scores))\n",
        "sns.heatmap(confusion, cmap=\"Blues\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6ef7c413eab43090eaa2ac07a6bb3b46c2e25ed3",
        "id": "G1DkAv1QNg6x"
      },
      "cell_type": "markdown",
      "source": [
        "#### LightGBM"
      ]
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "bd9a9af0ce09640f0b90e349d1d69539651fd873",
        "id": "3INKA_dVNg6x"
      },
      "cell_type": "code",
      "source": [
        "LGBM1_col = [\"LGBM1_{}_proba\".format(i) for i in range(1, 8)]\n",
        "for col in LGBM1_col:\n",
        "    train_level2.loc[:, col] = 0\n",
        "    test_level2.loc[:, col] = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "c35f7a3011697bbf95e34193c1f26c1a755c9448",
        "id": "GUqeVb2oNg6y"
      },
      "cell_type": "code",
      "source": [
        "cat_col = X.filter(regex=\"Soil_Type|Wilderness\").columns.tolist()[:-1] + [\"Wilder_Type\"]\n",
        "categorical_feature = [29, 38]\n",
        "lgbm_col = X.drop(cat_col[:-2], axis=1).columns.tolist()\n",
        "class_weight_lgbm = {i: v for i, v in enumerate(type_ratio)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "2134226ea0195d71d61f10d53b00a325bb23dbee",
        "id": "HnTrRz6NNg6y"
      },
      "cell_type": "code",
      "source": [
        "gbm = lgb.LGBMClassifier(n_estimators=15,\n",
        "                         num_class=7,\n",
        "                         learning_rate=0.1,\n",
        "                         bagging_fraction=0.6,\n",
        "                         num_boost_round=370,\n",
        "                         max_depth=8,\n",
        "                         max_cat_to_onehot=40,\n",
        "                         class_weight=class_weight_lgbm,\n",
        "                         device=\"cpu\",\n",
        "                         n_jobs=4,\n",
        "                         silent=-1,\n",
        "                         verbose=-1)\n",
        "\n",
        "confusion = np.zeros((7, 7))\n",
        "scores = []\n",
        "for train, val in tqdm(StratifiedKFold(n_splits=10, random_state=2434, shuffle=True).split(X, y)):\n",
        "    X_train = X.loc[train, lgbm_col]\n",
        "    X_val = X.loc[val, lgbm_col]\n",
        "\n",
        "    y_train = y[train]\n",
        "    y_val = y[val]\n",
        "    gbm.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)],\n",
        "            verbose=50, categorical_feature=categorical_feature)\n",
        "\n",
        "    y_val_pred = gbm.predict(X_val)\n",
        "    y_val_proba = gbm.predict_proba(X_val)\n",
        "    \n",
        "    scores.append(balanced_accuracy_score(y_val, y_val_pred))\n",
        "    confusion += confusion_matrix(y_val, y_val_pred)\n",
        "    train_level2.loc[val, LGBM1_col] = y_val_proba\n",
        "\n",
        "\n",
        "X_all = X.loc[:, lgbm_col]\n",
        "X_test_lgbm = X_test.loc[:, lgbm_col]\n",
        "gbm.fit(X_all, y, verbose=50, categorical_feature=categorical_feature)\n",
        "test_level2.loc[:, LGBM1_col] = gbm.predict_proba(X_test_lgbm)\n",
        "#smpsb[\"Cover_Type\"] = gbm.predict(X_test_lgbm)\n",
        "#smpsb.to_csv(\"LGBM1.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It looks like the code is implementing a two-level stacked ensemble model, where the outputs of several base models are used as inputs to a higher-level model to make the final predictions. \n",
        "\n",
        "The base models are a k-Nearest Neighbors (KNN) classifier and a LightGBM classifier. The KNN classifier is trained on a PCA-transformed dataset, while the LightGBM classifier is trained on the original dataset. \n",
        "\n",
        "The predictions of the base models are saved in a DataFrame called `train_level2`, and are then used as inputs to the higher-level model. In this case, the higher-level model is not explicitly defined in the code, but I assume it is some kind of weighted average or logistic regression model that takes the predictions of the base models as inputs and produces the final predictions. \n",
        "\n",
        "The final predictions are saved in a DataFrame called `test_level2`. These predictions can be used to submit to a competition or evaluate the performance of the ensemble model. \n",
        "\n",
        "Overall, this code is a good example of how to implement a stacked ensemble model in Python using scikit-learn, LightGBM, and pandas."
      ],
      "metadata": {
        "id": "s4eXdXf8SWWJ"
      }
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "a97ac783c210d78101df455ade63265a17379134",
        "id": "UIwqWEG2Ng6z"
      },
      "cell_type": "code",
      "source": [
        "print(np.mean(scores))\n",
        "sns.heatmap(confusion, cmap=\"Blues\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "aa06d3aff18a8f46592c976296539f14b380f148",
        "id": "RMhQPAfKNg6z"
      },
      "cell_type": "markdown",
      "source": [
        "### with KNN & DT features"
      ]
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "25308199c65d017343ea96fb02f668e6cb426dd8",
        "id": "-AHs0V0xNg6z"
      },
      "cell_type": "code",
      "source": [
        "X_p = pd.concat([knn_train_df, dt_train_df, te_train_df], axis=1).astype(np.float32)\n",
        "X_test_p = pd.concat([knn_test_df, dt_test_df, te_test_df.reset_index(drop=True)], axis=1).astype(np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e4e05bc017ab0deaf36bec4d86840b5f2b12366c",
        "id": "yhsdtt0uNg6z"
      },
      "cell_type": "markdown",
      "source": [
        "#### RandomForestClassifier"
      ]
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "e5bfffb307a40b0c67c31d53a8a08033e5b91381",
        "id": "RSn6KfotNg60"
      },
      "cell_type": "code",
      "source": [
        "KNNDT_RF_col = [\"KNNDT_RF_{}_proba\".format(i) for i in range(1, 8)]\n",
        "for col in KNNDT_RF_col:\n",
        "    train_level2.loc[:, col] = 0\n",
        "    test_level2.loc[:, col] = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "b624a15e4a0c11612205ead52e2c8711951ca31b",
        "id": "p8iqHdqGNg60"
      },
      "cell_type": "code",
      "source": [
        "rfc = RandomForestClassifier(n_jobs=-1,\n",
        "                             n_estimators=200,\n",
        "                             max_depth=None,\n",
        "                             max_features=.7,\n",
        "                             max_leaf_nodes=220,\n",
        "                             class_weight=class_weight)\n",
        "\n",
        "confusion = np.zeros((7, 7))\n",
        "scores = []\n",
        "for train, val in tqdm(StratifiedKFold(n_splits=10, shuffle=True, random_state=2434).split(X_p, y)):\n",
        "    X_train = X_p.iloc[train, :]\n",
        "    y_train = y[train]\n",
        "    X_val = X_p.iloc[val, :]\n",
        "    y_val = y[val]\n",
        "    rfc.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = rfc.predict(X_val)\n",
        "    scores.append(balanced_accuracy_score(y_val, y_pred))\n",
        "    confusion += confusion_matrix(y_val, y_pred)\n",
        "    train_level2.loc[val, KNNDT_RF_col] = rfc.predict_proba(X_val)\n",
        "\n",
        "rfc.fit(X_p, y)\n",
        "test_level2.loc[:, KNNDT_RF_col] = rfc.predict_proba(X_test_p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "bcd8c782f669a2ef57cf14b0ac1d3f5079d3dde0",
        "id": "uwpgfyDONg60"
      },
      "cell_type": "code",
      "source": [
        "print(np.mean(scores))\n",
        "sns.heatmap(confusion, cmap=\"Blues\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c283aadde8a935479ca9b3efa466d14e3abd4050",
        "id": "vpXoooP1Ng61"
      },
      "cell_type": "markdown",
      "source": [
        "#### Logistic Regression"
      ]
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "15db3d3366a7a6f6fe9bf02aec83782b748cb2eb",
        "id": "racvemUrNg61"
      },
      "cell_type": "code",
      "source": [
        "KNNDT_LR_col = [\"KNNDT_LR_{}_proba\".format(i) for i in range(1, 8)]\n",
        "for col in KNNDT_LR_col:\n",
        "    train_level2.loc[:, col] = 0\n",
        "    test_level2.loc[:, col] = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "72ad8f5bb755924ea308a365f701ad86b1f09597",
        "id": "lY1At6ZzNg61"
      },
      "cell_type": "code",
      "source": [
        "confusion = np.zeros((7, 7))\n",
        "scores = []\n",
        "for train, val in tqdm(StratifiedKFold(n_splits=10, shuffle=True, random_state=2434).split(X, y)):\n",
        "    X_train = X_p.iloc[train, :]\n",
        "    y_train = y[train]\n",
        "    X_val = X_p.iloc[val, :]\n",
        "    y_val = y[val]\n",
        "    lr = LogisticRegression(n_jobs=-1, multi_class=\"multinomial\", C=10**9, solver=\"saga\", class_weight=class_weight)\n",
        "    lr.fit(X_train, y_train)\n",
        "    y_val_pred = lr.predict(X_val)\n",
        "    train_level2.loc[val, KNNDT_LR_col] = lr.predict_proba(X_val)\n",
        "    scores.append(balanced_accuracy_score(y_val, y_val_pred))\n",
        "    confusion += confusion_matrix(y_val, y_val_pred)\n",
        "\n",
        "lr.fit(X_p, y)\n",
        "test_level2.loc[:, KNNDT_LR_col] = lr.predict_proba(X_test_p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "8aeac651853a98927d9d3a5c255e83d5d8dfc37f",
        "id": "UOcydrfbNg62"
      },
      "cell_type": "code",
      "source": [
        "print(np.mean(scores))\n",
        "sns.heatmap(confusion, cmap=\"Blues\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "91077b39bcbb10180ef2a0424b19c1791db9b776",
        "id": "v8GvWkb7Ng62"
      },
      "cell_type": "markdown",
      "source": [
        "#### LightGBM"
      ]
    },
    {
      "metadata": {
        "_uuid": "d09a32de613b485bef804a563d082e3dbe1ff787",
        "trusted": false,
        "id": "qm_iYB6pNg62"
      },
      "cell_type": "code",
      "source": [
        "KNNDT_LGB_col = [\"KNNDT_LGB_{}_proba\".format(i) for i in range(1, 8)]\n",
        "for col in KNNDT_LGB_col:\n",
        "    train_level2.loc[:, col] = 0\n",
        "    test_level2.loc[:, col] = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "bb284b9b2d18193fab68f211ea66f7994696d200",
        "trusted": false,
        "id": "E54_hfmDNg63"
      },
      "cell_type": "code",
      "source": [
        "X = total_df[total_df[\"Id\"] <= 15120].drop(\"Id\", axis=1)\n",
        "X_test = total_df[total_df[\"Id\"] > 15120].drop(\"Id\", axis=1).reset_index(drop=True)\n",
        "\n",
        "X_d = pd.concat([X.drop(total_df.filter(regex=\"Type\\d+\").columns, axis=1),\n",
        "                 knn_train_df,\n",
        "                 dt_train_df], axis=1)\n",
        "\n",
        "X_test_d = pd.concat([X_test.drop(total_df.filter(regex=\"Type\\d+\").columns, axis=1),\n",
        "                 knn_test_df,\n",
        "                 dt_test_df], axis=1)\n",
        "\n",
        "fcol = X_d.select_dtypes(np.float64).columns\n",
        "X_d.loc[:, fcol] = X_d.loc[:, fcol].astype(np.float32)\n",
        "X_d = X_d.values.astype(np.float32)\n",
        "X_test_d.loc[:, fcol] = X_test_d.loc[:, fcol].astype(np.float32)\n",
        "X_test_d = X_test_d.values.astype(np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "742567953206af0aeae3a560941b9a5687737514",
        "trusted": false,
        "id": "qN9F5uG6Ng63"
      },
      "cell_type": "code",
      "source": [
        "class_weight_lgbm = {i: v for i, v in enumerate(type_ratio)}\n",
        "\n",
        "gbm = lgb.LGBMClassifier(n_estimators=300,\n",
        "                         num_class=8,\n",
        "                         num_leaves=32,\n",
        "                         feature_fraction=0.3,\n",
        "                         min_child_samples=20,\n",
        "                         learning_rate=0.05,\n",
        "                         num_boost_round=430,\n",
        "                         max_depth=-1,                         \n",
        "                         class_weight=class_weight_lgbm,\n",
        "                         device=\"cpu\",\n",
        "                         n_jobs=4,\n",
        "                         silent=-1,\n",
        "                         verbose=-1)\n",
        "\n",
        "confusion = np.zeros((7, 7))\n",
        "scores = []\n",
        "for train, val in tqdm(StratifiedKFold(n_splits=10, shuffle=True, random_state=2434).split(X_p, y)):\n",
        "    X_train = X_d[train]\n",
        "    X_val = X_d[val]\n",
        "\n",
        "    y_train = y[train]\n",
        "    y_val = y[val]\n",
        "    gbm.fit(X_train, y_train, categorical_feature=[33, 42])\n",
        "\n",
        "    y_pred = gbm.predict(X_val)\n",
        "    scores.append(balanced_accuracy_score(y_val, y_pred))\n",
        "    confusion += confusion_matrix(y_val, y_pred)\n",
        "    train_level2.loc[val, KNNDT_LGB_col] = gbm.predict_proba(X_val)\n",
        "    \n",
        "gbm.fit(X_d, y, categorical_feature=[33, 42])\n",
        "test_level2.loc[:, KNNDT_LGB_col] = gbm.predict_proba(X_test_d)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "training a LightGBM classifier (LGBMClassifier) using the X_d input data and y labels with a stratified K-fold cross-validation. The categorical_feature parameter is set to [33, 42], which indicates that columns 33 and 42 are categorical features. The resulting predicted probabilities for each class on the validation data are stored in the train_level2 dataframe, and the predicted probabilities for each class on the test data are stored in the test_level2 dataframe.\n",
        "\n",
        "The class_weight_lgbm dictionary is defined as the class weights to be used in the LightGBM classifier. It is a dictionary with the class index as keys and the corresponding class weight as values.\n",
        "\n",
        "The LightGBM classifier is initialized with various hyperparameters, such as the number of estimators, the number of leaves, the learning rate, and so on. The num_boost_round parameter specifies the number of boosting rounds to perform, while the max_depth parameter is set to -1, which means no limit on the depth of each tree.\n",
        "\n",
        "During the training process, the fit method is called on the LightGBM classifier with the training data and labels, as well as the categorical_feature parameter. After training, the predict_proba method is called on the trained classifier to generate predicted probabilities for each class on the validation and test data. These probabilities are stored in the train_level2 and test_level2 dataframes, respectively. Finally, the resulting predictions on the test data are stored in the smpsb dataframe."
      ],
      "metadata": {
        "id": "08O-h2FuS7NN"
      }
    },
    {
      "metadata": {
        "_uuid": "3b496508a7b0b892ed6e33011e2c9d1a922fef50",
        "trusted": false,
        "id": "vaTKtyoZNg63"
      },
      "cell_type": "code",
      "source": [
        "print(np.mean(scores))\n",
        "sns.heatmap(confusion, cmap=\"Blues\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "acfd60853a6f5c166f59a29e1bc66a345ba4126e",
        "id": "t0SdUMfINg64"
      },
      "cell_type": "markdown",
      "source": [
        "# ykskks's kernel"
      ]
    },
    {
      "metadata": {
        "_uuid": "6c91ea9786eebe34c0dc9e772139dd12cfb81cc9",
        "trusted": false,
        "id": "diHNXqk1Ng64"
      },
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "\n",
        "import os\n",
        "print(os.listdir(\"../input\"))\n",
        "\n",
        "#import warnings\n",
        "#warnings.filterwarnings('ignore')\n",
        "\n",
        "# Any results you write to the current directory are saved as output.\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.svm import SVC\n",
        "import lightgbm as lgb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e48eb826e4756b0dde940764177f5ada1169ba97",
        "trusted": false,
        "id": "1ArgIQwpNg64"
      },
      "cell_type": "code",
      "source": [
        "train=pd.read_csv('../input/train.csv')\n",
        "test=pd.read_csv('../input/test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e1a71c69e138722e7bd744a940ee5acf811286a4",
        "trusted": false,
        "id": "ns9sO_JcNg65"
      },
      "cell_type": "code",
      "source": [
        "#drop columns that have the same value in every row\n",
        "train.drop(['Soil_Type7', 'Soil_Type15'], axis=1, inplace=True)\n",
        "test.drop(['Soil_Type7', 'Soil_Type15'], axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "cfecd1b31f2b47ab71661ffda9a1632c72183ab3",
        "id": "QA812Hr2Ng65"
      },
      "cell_type": "markdown",
      "source": [
        "The feature enginnering ideas I used here are based on [Lathwal's amazing kernel ](https://www.kaggle.com/codename007/forest-cover-type-eda-baseline-model).\n",
        "\n",
        "I removed 'slope_hyd' feature from the original one beacause it did'nt seem to be that useful for prediction.\n"
      ]
    },
    {
      "metadata": {
        "_uuid": "90502edbe566c397c1b998e62d4d72c4e82cf915",
        "trusted": false,
        "id": "guDTRNkWNg65"
      },
      "cell_type": "code",
      "source": [
        "train['HF1'] = train['Horizontal_Distance_To_Hydrology']+train['Horizontal_Distance_To_Fire_Points']\n",
        "train['HF2'] = abs(train['Horizontal_Distance_To_Hydrology']-train['Horizontal_Distance_To_Fire_Points'])\n",
        "train['HR1'] = abs(train['Horizontal_Distance_To_Hydrology']+train['Horizontal_Distance_To_Roadways'])\n",
        "train['HR2'] = abs(train['Horizontal_Distance_To_Hydrology']-train['Horizontal_Distance_To_Roadways'])\n",
        "train['FR1'] = abs(train['Horizontal_Distance_To_Fire_Points']+train['Horizontal_Distance_To_Roadways'])\n",
        "train['FR2'] = abs(train['Horizontal_Distance_To_Fire_Points']-train['Horizontal_Distance_To_Roadways'])\n",
        "train['ele_vert'] = train.Elevation-train.Vertical_Distance_To_Hydrology\n",
        "train['Mean_Amenities']=(train.Horizontal_Distance_To_Fire_Points + train.Horizontal_Distance_To_Hydrology + train.Horizontal_Distance_To_Roadways) / 3  \n",
        "train['Mean_Fire_Hyd']=(train.Horizontal_Distance_To_Fire_Points + train.Horizontal_Distance_To_Hydrology) / 2 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7cea9c1dfad87d4d961fbfed2c124824f485c185",
        "trusted": false,
        "id": "D0Xv6-nLNg66"
      },
      "cell_type": "code",
      "source": [
        "test['HF1'] = test['Horizontal_Distance_To_Hydrology']+test['Horizontal_Distance_To_Fire_Points']\n",
        "test['HF2'] = abs(test['Horizontal_Distance_To_Hydrology']-test['Horizontal_Distance_To_Fire_Points'])\n",
        "test['HR1'] = abs(test['Horizontal_Distance_To_Hydrology']+test['Horizontal_Distance_To_Roadways'])\n",
        "test['HR2'] = abs(test['Horizontal_Distance_To_Hydrology']-test['Horizontal_Distance_To_Roadways'])\n",
        "test['FR1'] = abs(test['Horizontal_Distance_To_Fire_Points']+test['Horizontal_Distance_To_Roadways'])\n",
        "test['FR2'] = abs(test['Horizontal_Distance_To_Fire_Points']-test['Horizontal_Distance_To_Roadways'])\n",
        "test['ele_vert'] = test.Elevation-test.Vertical_Distance_To_Hydrology \n",
        "test['Mean_Amenities']=(test.Horizontal_Distance_To_Fire_Points + test.Horizontal_Distance_To_Hydrology + test.Horizontal_Distance_To_Roadways) / 3  \n",
        "test['Mean_Fire_Hyd']=(test.Horizontal_Distance_To_Fire_Points + test.Horizontal_Distance_To_Hydrology) / 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "483b4ae89f0770f861bea38823276331e0f692c6",
        "trusted": false,
        "id": "0miOlaLZNg66"
      },
      "cell_type": "code",
      "source": [
        "#Id for later use\n",
        "Id_train=train['Id']\n",
        "Id_test=test['Id']\n",
        "\n",
        "train.drop('Id', axis=1, inplace=True)\n",
        "test.drop('Id', axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "fa8c2be8de8ddbbc41de1a52b558dfa9d6c44fc7",
        "trusted": false,
        "id": "OuyfG-6JNg66"
      },
      "cell_type": "code",
      "source": [
        "x_train=train.drop('Cover_Type', axis=1)\n",
        "y_train=train['Cover_Type']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8cb0bc06ea06f7cd01023d76c17a18e81df0d61a",
        "id": "QrD3e4xANg67"
      },
      "cell_type": "markdown",
      "source": [
        "## randomforest"
      ]
    },
    {
      "metadata": {
        "_uuid": "1ebfc5f7a08fde1f4746234c72511194af5d265f",
        "trusted": false,
        "id": "45M6ORt1Ng67"
      },
      "cell_type": "code",
      "source": [
        "#prepare df to store pred proba\n",
        "x_train_L2=pd.DataFrame(Id_train)\n",
        "x_test_L2=pd.DataFrame(Id_test)\n",
        "rf_cul=['rf'+str(i+1) for i in range(7)]\n",
        "\n",
        "#prepare cols to store pred proba\n",
        "for i in rf_cul:\n",
        "    x_train_L2.loc[:, i]=0\n",
        "    x_test_L2.loc[:, i]=0\n",
        "\n",
        "rf=RandomForestClassifier(max_depth=None, max_features=20,n_estimators=500, random_state=1)\n",
        "\n",
        "#StratifiedKfold to avoid leakage\n",
        "for train_index, val_index in tqdm(StratifiedKFold(n_splits=10, shuffle=True, random_state=1).split(x_train, y_train)):\n",
        "    x_train_L1=x_train.iloc[train_index, :]\n",
        "    y_train_L1=y_train.iloc[train_index]\n",
        "    x_val_L1=x_train.iloc[val_index, :]\n",
        "    y_val_L1=y_train.iloc[val_index]\n",
        "\n",
        "    rf.fit(x_train_L1, y_train_L1)\n",
        "    y_val_proba=rf.predict_proba(x_val_L1)\n",
        "    x_train_L2.loc[val_index, rf_cul]=y_val_proba\n",
        "\n",
        "rf.fit(x_train, y_train)\n",
        "x_test_L2.loc[:, rf_cul]=rf.predict_proba(test)\n",
        "\n",
        "#prepare df for submission\n",
        "#submit_df=pd.DataFrame(rf.predict(test))\n",
        "#submit_df.columns=['Cover_Type']\n",
        "#submit_df['Id']=Id_test\n",
        "#submit_df=submit_df.loc[:, ['Id', 'Cover_Type']]\n",
        "#submit_df.to_csv('rf.csv', index=False)\n",
        "\n",
        "#0.75604"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "293cb1e3271a078108834aed9118fc18ff31672a",
        "id": "8cq25pkUNg67"
      },
      "cell_type": "markdown",
      "source": [
        "## LightGBM"
      ]
    },
    {
      "metadata": {
        "_uuid": "ceef4c46d42a9701b28ea63fe8dd4bf8fa64cbf1",
        "trusted": false,
        "id": "x7nlMdliNg67"
      },
      "cell_type": "code",
      "source": [
        "#prepare df to store pred proba\n",
        "#x_train_L2=pd.DataFrame(Id_train)\n",
        "#x_test_L2=pd.DataFrame(Id_test)\n",
        "lgbm_cul=['lgbm'+str(i+1) for i in range(7)]\n",
        "\n",
        "#prepare cols to store pred proba\n",
        "for i in lgbm_cul:\n",
        "    x_train_L2.loc[:, i]=0\n",
        "    x_test_L2.loc[:, i]=0\n",
        "\n",
        "lgbm=lgb.LGBMClassifier(learning_rate=0.3, max_depth=-1, min_child_samples=20, n_estimators=300, num_leaves=200, random_state=1, n_jobs=4)\n",
        "\n",
        "#StratifiedKfold to avoid leakage\n",
        "for train_index, val_index in tqdm(StratifiedKFold(n_splits=10, shuffle=True, random_state=1).split(x_train, y_train)):\n",
        "    x_train_L1=x_train.iloc[train_index, :]\n",
        "    y_train_L1=y_train.iloc[train_index]\n",
        "    x_val_L1=x_train.iloc[val_index, :]\n",
        "    y_val_L1=y_train.iloc[val_index]\n",
        "\n",
        "    lgbm.fit(x_train_L1, y_train_L1)\n",
        "    y_val_proba=lgbm.predict_proba(x_val_L1)\n",
        "    x_train_L2.loc[val_index, lgbm_cul]=y_val_proba\n",
        "\n",
        "lgbm.fit(x_train, y_train)\n",
        "x_test_L2.loc[:, lgbm_cul]=lgbm.predict_proba(test)\n",
        "\n",
        "#prepare df for submission\n",
        "#submit_df=pd.DataFrame(lgbm.predict(test))\n",
        "#submit_df.columns=['Cover_Type']\n",
        "#submit_df['Id']=Id_test\n",
        "#submit_df=submit_df.loc[:, ['Id', 'Cover_Type']]\n",
        "#submit_df.to_csv('lgbm.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f6cfa93e9d087406d2331e830e63ef877312893a",
        "id": "zvD44eApNg68"
      },
      "cell_type": "markdown",
      "source": [
        "## LR"
      ]
    },
    {
      "metadata": {
        "_uuid": "f9ce60a286c0998db755fa87c58c14a715c4524d",
        "trusted": false,
        "id": "SO3zq-c4Ng68"
      },
      "cell_type": "code",
      "source": [
        "lr_cul=['lr'+str(i+1) for i in range(7)]\n",
        "\n",
        "#prepare cols to store pred proba\n",
        "for i in lr_cul:\n",
        "    x_train_L2.loc[:, i]=0\n",
        "    x_test_L2.loc[:, i]=0\n",
        "    \n",
        "pca=PCA(n_components=40)\n",
        "x_train_pca=pd.DataFrame(pca.fit_transform(x_train))\n",
        "test_pca=pd.DataFrame(pca.transform(test))\n",
        "\n",
        "pipeline=Pipeline([('scaler', StandardScaler()), ('lr', LogisticRegression(C=10, solver='newton-cg', multi_class='multinomial',max_iter=500))])\n",
        "\n",
        "#StratifiedKfold to avoid leakage\n",
        "for train_index, val_index in tqdm(StratifiedKFold(n_splits=10, shuffle=True, random_state=1).split(x_train_pca, y_train)):\n",
        "    x_train_L1=x_train_pca.iloc[train_index, :]\n",
        "    y_train_L1=y_train.iloc[train_index]\n",
        "    x_val_L1=x_train_pca.iloc[val_index, :]\n",
        "    y_val_L1=y_train.iloc[val_index]\n",
        "\n",
        "    pipeline.fit(x_train_L1, y_train_L1)\n",
        "    y_val_proba=pipeline.predict_proba(x_val_L1)\n",
        "    x_train_L2.loc[val_index, lr_cul]=y_val_proba\n",
        "\n",
        "pipeline.fit(x_train_pca, y_train)\n",
        "x_test_L2.loc[:, lr_cul]=pipeline.predict_proba(test_pca)\n",
        "\n",
        "#prepare df for submission\n",
        "#submit_df=pd.DataFrame(pipeline.predict(test_pca))\n",
        "#submit_df.columns=['Cover_Type']\n",
        "#submit_df['Id']=Id_test\n",
        "#submit_df=submit_df.loc[:, ['Id', 'Cover_Type']]\n",
        "#submit_df.to_csv('lr.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "you are using Logistic Regression to make predictions on your data. You also use PCA to reduce the dimensionality of the data before applying the logistic regression model. You then use stratified K-fold cross-validation to avoid data leakage while training the model. Finally, you use the trained model to make predictions on the test data and store the predictions in a separate data frame. "
      ],
      "metadata": {
        "id": "wvvHcxNtTO4d"
      }
    },
    {
      "metadata": {
        "_uuid": "39561fd3170fdc43962d89ec5d552d188e181134",
        "id": "H8AY2f9HNg69"
      },
      "cell_type": "markdown",
      "source": [
        "## SVM"
      ]
    },
    {
      "metadata": {
        "_uuid": "cfa2a2e75d833bff82cd51fb73ee38ddba7e3205",
        "trusted": false,
        "id": "3f1tzvWZNg69"
      },
      "cell_type": "code",
      "source": [
        "svm_cul=['svm'+str(i+1) for i in range(7)]\n",
        "\n",
        "#prepare cols to store pred proba\n",
        "for i in svm_cul:\n",
        "    x_train_L2.loc[:, i]=0\n",
        "    x_test_L2.loc[:, i]=0\n",
        "    \n",
        "#pca=PCA(n_components=40)\n",
        "#x_train_pca=pca.fit_transform(x_train)\n",
        "#test_pca=pca.transform(test)\n",
        "\n",
        "pipeline=Pipeline([('scaler', StandardScaler()), ('svm', SVC(C=10, gamma=0.1, probability=True))])\n",
        "\n",
        "\n",
        "#StratifiedKfold to avoid leakage\n",
        "for train_index, val_index in tqdm(StratifiedKFold(n_splits=10, shuffle=True, random_state=1).split(x_train_pca, y_train)):\n",
        "    x_train_L1=x_train_pca.iloc[train_index, :]\n",
        "    y_train_L1=y_train.iloc[train_index]\n",
        "    x_val_L1=x_train_pca.iloc[val_index, :]\n",
        "    y_val_L1=y_train.iloc[val_index]\n",
        "\n",
        "    pipeline.fit(x_train_L1, y_train_L1)\n",
        "    y_val_proba=pipeline.predict_proba(x_val_L1)\n",
        "    x_train_L2.loc[val_index, svm_cul]=y_val_proba\n",
        "\n",
        "pipeline.fit(x_train_pca, y_train)\n",
        "x_test_L2.loc[:, svm_cul]=pipeline.predict_proba(test_pca)\n",
        "\n",
        "#prepare df for submission\n",
        "#submit_df=pd.DataFrame(pipeline.predict(test_pca))\n",
        "#submit_df.columns=['Cover_Type']\n",
        "#submit_df['Id']=Id_test\n",
        "#submit_df=submit_df.loc[:, ['Id', 'Cover_Type']]\n",
        "#submit_df.to_csv('svm.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "you are using a Support Vector Machine (SVM) classifier to predict the Cover_Type of the forest cover dataset. You have also used a 10-fold stratified cross-validation to train the model and stored the predicted probabilities of each fold in separate columns of a new DataFrame, x_train_L2, and used the same model to predict the probabilities of the test set and stored them in the x_test_L2 DataFrame. Finally, you have saved the predictions in separate CSV files for the Logistic Regression (LR) and SVM models."
      ],
      "metadata": {
        "id": "ViqRNlUuTV6d"
      }
    },
    {
      "metadata": {
        "_uuid": "dc4dd658ece053364a668653ab3c35a5e48f83b6",
        "id": "puPwluynNg69"
      },
      "cell_type": "markdown",
      "source": [
        "# stacking"
      ]
    },
    {
      "metadata": {
        "_uuid": "d42deaa383b95ea9fadc4e05bbb908fc702d7caa",
        "id": "uKD3XIt9Ng6-"
      },
      "cell_type": "markdown",
      "source": [
        "## Level1 summary"
      ]
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "310a00ca38bfc3d2149b14620203dd8b83cad06d",
        "id": "qlobhb5oNg6-"
      },
      "cell_type": "code",
      "source": [
        "# concatenate two data\n",
        "train_L2 = pd.concat([x_train_L2.iloc[:, 1:].reset_index(drop=True), train_level2.iloc[:, 1:].reset_index(drop=True)], axis=1)\n",
        "test_L2 = pd.concat([x_test_L2.iloc[:, 1:].reset_index(drop=True), test_level2.iloc[:, 1:].reset_index(drop=True)], axis=1)\n",
        "train_L2.to_csv(\"Wtrain_L2.csv\", index=False)\n",
        "test_L2.to_csv(\"Wtest_L2.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "28b75db2a27aa91fc6da91cd65b32fb614b5be3e",
        "id": "K06ybPVbNg6-"
      },
      "cell_type": "code",
      "source": [
        "# each models score\n",
        "\n",
        "y = pd.read_csv(\"../input/train.csv\")[\"Cover_Type\"].values\n",
        "model_scores = {}\n",
        "text = []\n",
        "\n",
        "for i in range(10):\n",
        "    y_pred = np.argmax(train_L2.iloc[:, 7*i:7*(i+1)].values, axis=1) + 1\n",
        "    score = balanced_accuracy_score(y, y_pred)\n",
        "    model_scores[cols[i*7]] = score\n",
        "    text.append(\"{}\\t{:<.5}\".format(train_L2.columns[i*7], score))\n",
        "\n",
        "print(*text[::-1], sep=\"\\n\")\n",
        "pd.Series(model_scores).plot(kind=\"barh\")\n",
        "plt.savefig(\"model_summary.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c3fd4028181ba8d2e302bbf57cd203e004fef5e7",
        "id": "7jBu_AC-Ng6_"
      },
      "cell_type": "markdown",
      "source": [
        "## stacking with Logistic Regression"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-09-25T04:55:54.024497Z",
          "start_time": "2018-09-25T04:55:54.021506Z"
        },
        "_uuid": "a46e27fd2856f948dbb7bf9b0767e1f53b129b51",
        "id": "bFYVWKvWNg6_"
      },
      "cell_type": "markdown",
      "source": [
        "### nadare's simple stacking"
      ]
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "b4093457aacc4a3ac1589b20e256435e7dc8bebc",
        "id": "05eq0vAjNg6_"
      },
      "cell_type": "code",
      "source": [
        "score = []\n",
        "for train, val in tqdm(StratifiedKFold(n_splits=10, random_state=2434, shuffle=True).split(X, y)):\n",
        "    X_train = train_level2.iloc[train, 1:]\n",
        "    X_val = train_level2.iloc[val, 1:]\n",
        "    y_train = y[train]\n",
        "    y_val = y[val]\n",
        "    lr = LogisticRegression(n_jobs=1, class_weight=class_weight)\n",
        "    lr.fit(X_train, y_train)\n",
        "    y_pred = lr.predict(X_val)\n",
        "    score.append(balanced_accuracy_score(y_val, y_pred))\n",
        "    #print(score[-1])\n",
        "print(np.mean(score))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4a073df808a00458e3a6bdfbe74fdc68bcd1649b",
        "id": "yz16sqy0Ng6_"
      },
      "cell_type": "markdown",
      "source": [
        "### ykskks's simple stacking"
      ]
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "f8823485458b15b4a030f0b6cb3e74597005139a",
        "id": "zmHReY-MNg7A"
      },
      "cell_type": "code",
      "source": [
        "score = []\n",
        "for train, val in tqdm(StratifiedKFold(n_splits=10, random_state=2434, shuffle=True).split(X, y)):\n",
        "    X_train = x_train_L2.iloc[train, 1:]\n",
        "    X_val = x_train_L2.iloc[val, 1:]\n",
        "    y_train = y[train]\n",
        "    y_val = y[val]\n",
        "    lr = LogisticRegression(n_jobs=1, class_weight=class_weight)\n",
        "    lr.fit(X_train, y_train)\n",
        "    y_pred = lr.predict(X_val)\n",
        "    score.append(balanced_accuracy_score(y_val, y_pred))\n",
        "print(np.mean(score))\n",
        "\n",
        "lr = LogisticRegression(n_jobs=1, class_weight=class_weight)\n",
        "lr.fit(x_train_L2, y)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we are training a meta-model using the outputs (predictions) of several base models as features. Then, you are using stratified k-fold cross-validation to evaluate the performance of the meta-model and finally training the meta-model on the entire training set. Is that correct?\n",
        "\n",
        "If so, then it looks like a good approach to ensemble learning. Using multiple base models and blending their outputs can often lead to better performance than using a single model. And evaluating the performance using cross-validation helps to avoid overfitting to the training set."
      ],
      "metadata": {
        "id": "dyqoFE6gTiye"
      }
    },
    {
      "metadata": {
        "_uuid": "bdfd0730d524342031c365cd24a5934c94d199ae",
        "id": "u6htsURjNg7A"
      },
      "cell_type": "markdown",
      "source": [
        "### double simple stacking"
      ]
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "b9520a86d6a7b24591596457b6530328baa9c3b2",
        "id": "BXVrhRBYNg7A"
      },
      "cell_type": "code",
      "source": [
        "score = []\n",
        "for train, val in tqdm(StratifiedKFold(n_splits=10, random_state=2434, shuffle=True).split(X, y)):\n",
        "    X_train = train_L2.iloc[train, 1:]\n",
        "    X_val = train_L2.iloc[val, 1:]\n",
        "    y_train = y[train]\n",
        "    y_val = y[val]\n",
        "    lr = LogisticRegression(n_jobs=1, class_weight=class_weight)\n",
        "    lr.fit(X_train, y_train)\n",
        "    y_pred = lr.predict(X_val)\n",
        "    score.append(balanced_accuracy_score(y_val, y_pred))\n",
        "print(np.mean(score))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "a6116b70210f4b0be69389d95af93e3a9172dd8e",
        "id": "YCSnY10YNg7B"
      },
      "cell_type": "code",
      "source": [
        "# this is 0.83266 on public LB\n",
        "\"\"\"\n",
        "smpsb = pd.read_csv(\"../input/sample_submission.csv\")\n",
        "lr = LogisticRegression(n_jobs=1, class_weight=class_weight)\n",
        "lr.fit(train_L2, y)\n",
        "smpsb[\"Cover_Type\"] = lr.predict(test_L2)\n",
        "smpsb.to_csv(\"W_ensemble_LR.csv\", index=False)\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9b2b9381ffef1bc1b8c25758ad17723a379b879e",
        "id": "fywpjnlVNg7B"
      },
      "cell_type": "markdown",
      "source": [
        "## stacking with LightGBM"
      ]
    },
    {
      "metadata": {
        "_uuid": "a72184de5e684ade61bb85d60c137f6994bab595",
        "trusted": false,
        "id": "a5UJGOc4Ng7B"
      },
      "cell_type": "code",
      "source": [
        "wtrain = train_L2.values.astype(np.float32)\n",
        "wtest = test_L2.values.astype(np.float32)\n",
        "y = pd.read_csv(\"../input/train.csv\")[\"Cover_Type\"].values\n",
        "smpsb = pd.read_csv(\"../input/sample_submission.csv\")\n",
        "cols = train_L2.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "042ee7b70f3ba6e537f5957582214d6dc2241254",
        "trusted": false,
        "id": "fxRYyaPwNg7B"
      },
      "cell_type": "code",
      "source": [
        "# this is our final submission which is 0.84806 on Public LB\n",
        "gbm = lgb.LGBMClassifier(n_estimators=300,\n",
        "                         num_class=8,\n",
        "                         num_leaves=25,\n",
        "                         learning_rate=5,\n",
        "                         min_child_samples=20,\n",
        "                         bagging_fraction=.3,\n",
        "                         bagging_freq=1,\n",
        "                         reg_lambda = 10**4.5,\n",
        "                         reg_alpha = 1,\n",
        "                         feature_fraction=.2,\n",
        "                         num_boost_round=4000,\n",
        "                         max_depth=-1,\n",
        "                         class_weight=class_weight_lgbm,\n",
        "                         device=\"cpu\",\n",
        "                         n_jobs=4,\n",
        "                         silent=-1,\n",
        "                         verbose=-1)\n",
        "\n",
        "gbm.fit(wtrain, y, verbose=-1)\n",
        "smpsb[\"Cover_Type\"] = gbm.predict(wtest)\n",
        "smpsb.to_csv(\"final_submission.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6b36ef78a85b570c684b9301a5deb79920f7caa9",
        "trusted": false,
        "id": "8nRB3-geNg7C"
      },
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6, 12))\n",
        "plt.barh(cols, gbm.feature_importances_)\n",
        "plt.savefig(\"feature_importances.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "351fb3443797feddd01aa91cb8833852754e0bba",
        "trusted": false,
        "id": "ekFF-qqzNg7C"
      },
      "cell_type": "code",
      "source": [
        "# bagging with k-fold\n",
        "scores = []\n",
        "gbm = lgb.LGBMClassifier(n_estimators=300,\n",
        "                         num_class=8,\n",
        "                         num_leaves=25,\n",
        "                         learning_rate=5,\n",
        "                         min_child_samples=20,\n",
        "                         bagging_fraction=.3,\n",
        "                         bagging_freq=1,\n",
        "                         reg_lambda = 10**4.5,\n",
        "                         reg_alpha = 1,\n",
        "                         feature_fraction=.2,\n",
        "                         num_boost_round=8000,\n",
        "                         max_depth=-1,\n",
        "                         class_weight=class_weight_lgbm,\n",
        "                         device=\"cpu\",\n",
        "                         n_jobs=-1,\n",
        "                         silent=-1,\n",
        "                         verbose=-1)\n",
        "\n",
        "proba = np.zeros((wtest.shape[0], 7))\n",
        "for train, val in tqdm(StratifiedKFold(n_splits=5, shuffle=True, random_state=2434).split(wtrain, y)):\n",
        "    X_train = wtrain[train]\n",
        "    X_val = wtrain[val]\n",
        "    y_train = y[train]\n",
        "    y_val = y[val]\n",
        "    gbm.fit(X_train, y_train, verbose=-1, \n",
        "            eval_set=[(X_train, y_train), (X_val, y_val)], early_stopping_rounds=20)\n",
        "    proba += gbm.predict_proba(wtest) / 10\n",
        "    y_pred = gbm.predict(X_val)\n",
        "    scores.append(balanced_accuracy_score(y_val, y_pred))\n",
        "\n",
        "print(np.mean(scores))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code seems to be using LightGBM for classification using gradient boosting. It uses bagging with k-fold cross-validation to evaluate the model's performance. The `lgb.LGBMClassifier` class is used to define the LightGBM classifier with various hyperparameters like the number of estimators, learning rate, number of leaves, minimum number of samples in a leaf, bagging fraction, regularization parameters, and so on. The `fit` method of the classifier is called inside the k-fold loop to train the model on each fold's training data and evaluate it on the validation data. The `predict_proba` method of the classifier is called on the test set after training on all folds to generate class probabilities, and the probabilities are averaged across all folds to obtain the final predictions. The balanced accuracy score is calculated on each fold's validation data, and the average score is printed at the end."
      ],
      "metadata": {
        "id": "5rbSyDH4Tsd5"
      }
    },
    {
      "metadata": {
        "_uuid": "9d36a5aa6bcd558bd50f184ec4f93c7973e0ba9a",
        "trusted": false,
        "id": "QF4gZF_BNg7D"
      },
      "cell_type": "code",
      "source": [
        "smpsb[\"Cover_Type\"] = np.argmax(proba, axis=1) + 1\n",
        "smpsb.to_csv(\"final_submission_bagging.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}