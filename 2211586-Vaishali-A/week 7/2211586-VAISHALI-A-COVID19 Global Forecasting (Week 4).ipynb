{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/covid19-us-county-jhu-data-demographics/us_county.prj\n",
      "/kaggle/input/covid19-us-county-jhu-data-demographics/us_county.dbf\n",
      "/kaggle/input/covid19-us-county-jhu-data-demographics/covid_us_county.csv\n",
      "/kaggle/input/covid19-us-county-jhu-data-demographics/us_county.shp\n",
      "/kaggle/input/covid19-us-county-jhu-data-demographics/us_county.shx\n",
      "/kaggle/input/covid19-us-county-jhu-data-demographics/us_county.csv\n",
      "/kaggle/input/model-dirv5/modelconfirmedc3\n",
      "/kaggle/input/model-dirv5/modelfatal9\n",
      "/kaggle/input/model-dirv5/modelfatalc29\n",
      "/kaggle/input/model-dirv5/modelfatalc23\n",
      "/kaggle/input/model-dirv5/modelfatal2\n",
      "/kaggle/input/model-dirv5/modelfatalc21\n",
      "/kaggle/input/model-dirv5/modelfatalc0\n",
      "/kaggle/input/model-dirv5/modelfatal23\n",
      "/kaggle/input/model-dirv5/modelconfirmed15\n",
      "/kaggle/input/model-dirv5/modelconfirmed3\n",
      "/kaggle/input/model-dirv5/modelconfirmedc11\n",
      "/kaggle/input/model-dirv5/modelconfirmed2\n",
      "/kaggle/input/model-dirv5/modelconfirmed27\n",
      "/kaggle/input/model-dirv5/modelfatal24\n",
      "/kaggle/input/model-dirv5/modelfatal7\n",
      "/kaggle/input/model-dirv5/modelconfirmed14\n",
      "/kaggle/input/model-dirv5/modelfatal20\n",
      "/kaggle/input/model-dirv5/modelconfirmed11\n",
      "/kaggle/input/model-dirv5/modelfatalc1\n",
      "/kaggle/input/model-dirv5/modelfatal26\n",
      "/kaggle/input/model-dirv5/modelconfirmed4\n",
      "/kaggle/input/model-dirv5/modelconfirmed5\n",
      "/kaggle/input/model-dirv5/modelconfirmedc13\n",
      "/kaggle/input/model-dirv5/modelconfirmedc4\n",
      "/kaggle/input/model-dirv5/modelconfirmedc5\n",
      "/kaggle/input/model-dirv5/modelfatalc25\n",
      "/kaggle/input/model-dirv5/modelfatalc6\n",
      "/kaggle/input/model-dirv5/modelfatalc15\n",
      "/kaggle/input/model-dirv5/modelconfirmedc15\n",
      "/kaggle/input/model-dirv5/modelfatalc10\n",
      "/kaggle/input/model-dirv5/modelconfirmed21\n",
      "/kaggle/input/model-dirv5/modelconfirmedc26\n",
      "/kaggle/input/model-dirv5/modelconfirmedc1\n",
      "/kaggle/input/model-dirv5/modelconfirmed8\n",
      "/kaggle/input/model-dirv5/modelfatal29\n",
      "/kaggle/input/model-dirv5/modelfatalc28\n",
      "/kaggle/input/model-dirv5/modelconfirmedc10\n",
      "/kaggle/input/model-dirv5/modelfatal19\n",
      "/kaggle/input/model-dirv5/modelfatal18\n",
      "/kaggle/input/model-dirv5/modelconfirmedc23\n",
      "/kaggle/input/model-dirv5/modelconfirmed10\n",
      "/kaggle/input/model-dirv5/modelfatalc5\n",
      "/kaggle/input/model-dirv5/modelfatal25\n",
      "/kaggle/input/model-dirv5/modelfatalc13\n",
      "/kaggle/input/model-dirv5/modelconfirmedc8\n",
      "/kaggle/input/model-dirv5/modelfatal10\n",
      "/kaggle/input/model-dirv5/modelfatalc4\n",
      "/kaggle/input/model-dirv5/modelconfirmedc20\n",
      "/kaggle/input/model-dirv5/modelconfirmedc24\n",
      "/kaggle/input/model-dirv5/modelfatalc14\n",
      "/kaggle/input/model-dirv5/modelconfirmed12\n",
      "/kaggle/input/model-dirv5/modelfatal11\n",
      "/kaggle/input/model-dirv5/modelconfirmedc14\n",
      "/kaggle/input/model-dirv5/modelfatalc27\n",
      "/kaggle/input/model-dirv5/modelconfirmed7\n",
      "/kaggle/input/model-dirv5/modelconfirmedc9\n",
      "/kaggle/input/model-dirv5/modelconfirmed29\n",
      "/kaggle/input/model-dirv5/modelconfirmed24\n",
      "/kaggle/input/model-dirv5/modelfatalc18\n",
      "/kaggle/input/model-dirv5/modelconfirmed16\n",
      "/kaggle/input/model-dirv5/modelfatal22\n",
      "/kaggle/input/model-dirv5/modelconfirmed1\n",
      "/kaggle/input/model-dirv5/modelfatal12\n",
      "/kaggle/input/model-dirv5/modelconfirmedc22\n",
      "/kaggle/input/model-dirv5/modelfatal17\n",
      "/kaggle/input/model-dirv5/modelfatal15\n",
      "/kaggle/input/model-dirv5/modelconfirmedc19\n",
      "/kaggle/input/model-dirv5/modelfatal0\n",
      "/kaggle/input/model-dirv5/modelconfirmed20\n",
      "/kaggle/input/model-dirv5/modelconfirmed13\n",
      "/kaggle/input/model-dirv5/modelfatal5\n",
      "/kaggle/input/model-dirv5/modelfatalc3\n",
      "/kaggle/input/model-dirv5/modelconfirmed17\n",
      "/kaggle/input/model-dirv5/modelfatal3\n",
      "/kaggle/input/model-dirv5/modelconfirmed25\n",
      "/kaggle/input/model-dirv5/modelfatalc26\n",
      "/kaggle/input/model-dirv5/modelconfirmed23\n",
      "/kaggle/input/model-dirv5/modelconfirmed18\n",
      "/kaggle/input/model-dirv5/modelconfirmedc18\n",
      "/kaggle/input/model-dirv5/modelfatalc17\n",
      "/kaggle/input/model-dirv5/modelfatalc7\n",
      "/kaggle/input/model-dirv5/modelfatalc20\n",
      "/kaggle/input/model-dirv5/modelconfirmed19\n",
      "/kaggle/input/model-dirv5/modelconfirmed26\n",
      "/kaggle/input/model-dirv5/modelfatalc16\n",
      "/kaggle/input/model-dirv5/modelfatalc2\n",
      "/kaggle/input/model-dirv5/modelfatalc11\n",
      "/kaggle/input/model-dirv5/modelfatalc9\n",
      "/kaggle/input/model-dirv5/modelfatal4\n",
      "/kaggle/input/model-dirv5/modelconfirmedc17\n",
      "/kaggle/input/model-dirv5/modelconfirmedc0\n",
      "/kaggle/input/model-dirv5/modelfatal16\n",
      "/kaggle/input/model-dirv5/modelconfirmed22\n",
      "/kaggle/input/model-dirv5/modelfatalc22\n",
      "/kaggle/input/model-dirv5/modelconfirmedc29\n",
      "/kaggle/input/model-dirv5/modelconfirmedc7\n",
      "/kaggle/input/model-dirv5/modelconfirmedc21\n",
      "/kaggle/input/model-dirv5/modelconfirmedc25\n",
      "/kaggle/input/model-dirv5/modelconfirmedc16\n",
      "/kaggle/input/model-dirv5/modelconfirmedc28\n",
      "/kaggle/input/model-dirv5/modelfatal13\n",
      "/kaggle/input/model-dirv5/modelconfirmed9\n",
      "/kaggle/input/model-dirv5/modelconfirmed0\n",
      "/kaggle/input/model-dirv5/modelconfirmedc12\n",
      "/kaggle/input/model-dirv5/modelfatalc8\n",
      "/kaggle/input/model-dirv5/modelfatal6\n",
      "/kaggle/input/model-dirv5/modelconfirmedc27\n",
      "/kaggle/input/model-dirv5/modelconfirmedc2\n",
      "/kaggle/input/model-dirv5/modelfatal1\n",
      "/kaggle/input/model-dirv5/modelfatalc24\n",
      "/kaggle/input/model-dirv5/modelfatal27\n",
      "/kaggle/input/model-dirv5/modelfatal14\n",
      "/kaggle/input/model-dirv5/modelconfirmedc6\n",
      "/kaggle/input/model-dirv5/modelconfirmed6\n",
      "/kaggle/input/model-dirv5/modelfatal28\n",
      "/kaggle/input/model-dirv5/modelfatal21\n",
      "/kaggle/input/model-dirv5/modelfatal8\n",
      "/kaggle/input/model-dirv5/modelconfirmed28\n",
      "/kaggle/input/model-dirv5/modelfatalc12\n",
      "/kaggle/input/model-dirv5/modelfatalc19\n",
      "/kaggle/input/covid19-global-forecasting-week-4/submission.csv\n",
      "/kaggle/input/covid19-global-forecasting-week-4/test.csv\n",
      "/kaggle/input/covid19-global-forecasting-week-4/train.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "horizon 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:177: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ConfirmedCases Country_Region       Date  Fatalities  Id Province_State  \\\n",
      "0             0.0    Afghanistan 2020-01-22         0.0   1            NaN   \n",
      "1             0.0    Afghanistan 2020-01-23         0.0   2            NaN   \n",
      "2             0.0    Afghanistan 2020-01-24         0.0   3            NaN   \n",
      "3             0.0    Afghanistan 2020-01-25         0.0   4            NaN   \n",
      "4             0.0    Afghanistan 2020-01-26         0.0   5            NaN   \n",
      "\n",
      "               key  \n",
      "0  nan_Afghanistan  \n",
      "1  nan_Afghanistan  \n",
      "2  nan_Afghanistan  \n",
      "3  nan_Afghanistan  \n",
      "4  nan_Afghanistan  \n",
      "3565\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "def rmsle(y, y_pred):\n",
    "        assert len(y) == len(y_pred)\n",
    "        terms_to_sum = [(math.log(y_pred[i] + 1) - math.log(y[i] + 1)) ** 2.0 for i,pred in enumerate(y_pred)]\n",
    "        return (sum(terms_to_sum) * (1.0/len(y))) ** 0.5\n",
    "    \n",
    "\n",
    "def fix_target(frame, key, target, new_target_name=\"target\"):\n",
    "    import numpy as np\n",
    "\n",
    "    corrections = 0\n",
    "    group_keys = frame[ key].values.tolist()\n",
    "    target = frame[target].values.tolist()\n",
    "\n",
    "    for i in range(1, len(group_keys) - 1):\n",
    "        previous_group = group_keys[i - 1]\n",
    "        current_group = group_keys[i]\n",
    "\n",
    "        previous_value = target[i - 1]\n",
    "        current_value = target[i]\n",
    "        if current_group == previous_group:\n",
    "                if current_value<previous_value:\n",
    "                    current_value=previous_value\n",
    "                    target[i] =current_value\n",
    "\n",
    "\n",
    "        target[i] =max(0,target[i] )#correct negative values\n",
    "\n",
    "    frame[new_target_name] = np.array(target)\n",
    "    \n",
    "    \n",
    "def rate(frame, key, target, new_target_name=\"rate\"):\n",
    "    import numpy as np\n",
    "\n",
    "\n",
    "    corrections = 0\n",
    "    group_keys = frame[ key].values.tolist()\n",
    "    target = frame[target].values.tolist()\n",
    "    rate=[1.0 for k in range (len(target))]\n",
    "\n",
    "    for i in range(1, len(group_keys) ):\n",
    "        previous_group = group_keys[i - 1]\n",
    "        current_group = group_keys[i]\n",
    "\n",
    "        previous_value = target[i - 1]\n",
    "        current_value = target[i]\n",
    "         \n",
    "        if current_group == previous_group:\n",
    "                if previous_value!=0.0:\n",
    "                     rate[i]=current_value/previous_value\n",
    "\n",
    "                 \n",
    "        rate[i] =max(1,rate[i] )#correct negative values\n",
    "\n",
    "    frame[new_target_name] = np.array(rate)\n",
    "    \n",
    "def difference(frame, key, target, new_target_name=\"diff\"):\n",
    "    import numpy as np\n",
    "\n",
    "\n",
    "    corrections = 0\n",
    "    group_keys = frame[ key].values.tolist()\n",
    "    target = frame[target].values.tolist()\n",
    "    rate=[0 for k in range (len(target))]\n",
    "\n",
    "    for i in range(1, len(group_keys) ):\n",
    "        previous_group = group_keys[i - 1]\n",
    "        current_group = group_keys[i]\n",
    "\n",
    "        previous_value = target[i - 1]\n",
    "        current_value = target[i]\n",
    "         \n",
    "        if current_group == previous_group:\n",
    "                rate[i]=np.log1p(max(0,current_value-previous_value))#max(0,current_value-previous_value)#\n",
    "          \n",
    "        rate[i] =max(0,rate[i] )\n",
    "\n",
    "    frame[new_target_name] = np.array(rate)    \n",
    "    \n",
    "    \n",
    "def get_data_by_key(dataframe, key, key_value, fields=None):\n",
    "    mini_frame=dataframe[dataframe[key]==key_value]\n",
    "    if not fields is None:                \n",
    "        mini_frame=mini_frame[fields].values\n",
    "        \n",
    "    return mini_frame\n",
    "\n",
    "directory=\"/kaggle/input/covid19-global-forecasting-week-4/\"\n",
    "model_directory=\"/kaggle/input/model-dirv5/model\"\n",
    "us_county=\"/kaggle/input/covid19-us-county-jhu-data-demographics/covid_us_county\"\n",
    "geo_dir=None\n",
    "extra_stable_columns=None\n",
    "group_by_columns=None\n",
    "\n",
    "if not group_by_columns is None and \"continent\" in group_by_columns:\n",
    "    assert not geo_dir is None\n",
    "\n",
    "train=pd.read_csv(directory + \"train.csv\", parse_dates=[\"Date\"] , engine=\"python\")\n",
    "test=pd.read_csv(directory + \"test.csv\", parse_dates=[\"Date\"], engine=\"python\")\n",
    "\n",
    "train[\"key\"]=train[[\"Province_State\",\"Country_Region\"]].apply(lambda row: str(row[0]) + \"_\" + str(row[1]),axis=1)\n",
    "test[\"key\"]=test[[\"Province_State\",\"Country_Region\"]].apply(lambda row: str(row[0]) + \"_\" + str(row[1]),axis=1)\n",
    "if not geo_dir is None:\n",
    "    region_metadata=pd.read_csv(geo_dir+\"region_metadata.csv\")\n",
    "    km=KMeans(n_clusters=30, init='k-means++', n_init=10, max_iter=300, tol=0.0001,\n",
    "              precompute_distances='auto', verbose=0, random_state=1234, copy_x=True, n_jobs=None, algorithm='auto')\n",
    "    region_metadata[\"clusters\"]=km.fit_predict(region_metadata[[\"lat\",\"lon\"]].values)    \n",
    "    \n",
    "    train=pd.merge(train,region_metadata, how=\"left\", left_on=[\"Province_State\",\"Country_Region\"], right_on=[\"Province_State\",\"Country_Region\"] )\n",
    "    test=pd.merge(test,region_metadata, how=\"left\", left_on=[\"Province_State\",\"Country_Region\"], right_on=[\"Province_State\",\"Country_Region\"] )\n",
    "\n",
    "    \n",
    "\n",
    "group_names=None\n",
    "\n",
    "if  not group_by_columns is None and len(group_by_columns)>0:\n",
    "    group_names=[]\n",
    "    for group in group_by_columns:\n",
    "        groupss=train[[\"Date\", group,target1,target2]]\n",
    "        grp=groupss.groupby([\"Date\", group], as_index=False).sum()\n",
    "        grp.columns=[\"Date\", group,group +\"_\" + target1,group +\"_\" + target2 ]\n",
    "        train=pd.merge(train,grp, how=\"left\", left_on=[\"Date\", group,group], right_on=[\"Date\", group,group] )\n",
    "        #group_names+=[group +\"_\" + target1,group +\"_\" + target2]\n",
    "        for gr in [group +\"_\" + target1,group +\"_\" + target2]:\n",
    "            rate(train, key, gr, new_target_name=\"rate_\" +gr ) \n",
    "            group_names+=[\"rate_\" +gr]\n",
    "        train.to_csv(directory + \"train_plus_groups.csv\", index=False)\n",
    "        \n",
    "        \n",
    "    \n",
    "#last day in train\n",
    "max_train_date=train[\"Date\"].max()\n",
    "max_test_date=test[\"Date\"].max()\n",
    "horizon=  (max_test_date-max_train_date).days\n",
    "print (\"horizon\", int(horizon))\n",
    "\n",
    "\n",
    "#test_new=pd.merge(test,train, how=\"left\", left_on=[\"key\",\"Date\"], right_on=[\"key\",\"Date\"] )\n",
    "#train.to_csv(directory + \"transfomed.csv\")\n",
    "\n",
    "target1=\"ConfirmedCases\"\n",
    "target2=\"Fatalities\"\n",
    "\n",
    "key=\"key\"\n",
    "\n",
    "\n",
    "train_supplamanteary2=None\n",
    "new_data_file2=[us_county]\n",
    "\n",
    "if not new_data_file2 is None:\n",
    "    \n",
    "    trains=pd.read_csv(directory + \"train.csv\", parse_dates=[\"Date\"] , engine=\"python\")\n",
    "    tests=pd.read_csv(directory + \"test.csv\", parse_dates=[\"Date\"], engine=\"python\")\n",
    "\n",
    "    trains[\"key\"]=train[[\"Province_State\",\"Country_Region\"]].apply(lambda row: str(row[0]) + \"_\" + str(row[1]),axis=1)\n",
    "    tests[\"key\"]=test[[\"Province_State\",\"Country_Region\"]].apply(lambda row: str(row[0]) + \"_\" + str(row[1]),axis=1)\n",
    "    train_supplamanteary2=None\n",
    "    for new_data_files in new_data_file2:\n",
    "        sumplementary_data=pd.read_csv( new_data_files +\".csv\" , parse_dates=[\"date\"] , engine=\"python\")\n",
    "        sumplementary_data.columns=[\"fips\",\"Province_State\",\"Country_Region\",\"lat\",\"long\",\"Date\",\n",
    "                                    \"ConfirmedCases\",\"state_code\",\"Fatalities\"]\n",
    " \n",
    "        sumplementary_data[\"key\"]=sumplementary_data[[\"Province_State\",\"Country_Region\"]].apply(lambda row: str(row[0]) + \"_\" + str(row[1]),axis=1)              \n",
    "        sumplementary_data=sumplementary_data[[\"Date\",\"key\",\"ConfirmedCases\",\"Fatalities\",\"Province_State\",\"Country_Region\"]]        \n",
    "        sumplementary_data[\"Id\"]=[k for k in range(999999999,sumplementary_data.shape[0]+999999999)]\n",
    "\n",
    "        sumplementary_data=sumplementary_data[sumplementary_data.Date>= '2020-02-20']\n",
    "\n",
    "        if train_supplamanteary2 is None:\n",
    "            train_supplamanteary2=pd.concat([trains.reset_index(drop=True), sumplementary_data.reset_index(drop=True)], axis=0).reset_index(drop=True)\n",
    "        else :\n",
    "            train_supplamanteary2=pd.concat([train_supplamanteary2.reset_index(drop=True), sumplementary_data.reset_index(drop=True)], axis=0).reset_index(drop=True)\n",
    "             \n",
    "    print(train_supplamanteary2.head(5))\n",
    "    train_supplamanteary2 = train_supplamanteary2.sort_values([\"Country_Region\",\"key\",\"Id\"], ascending = (True, True, True))\n",
    "\n",
    "    fix_target(train_supplamanteary2, key, target1, new_target_name=target1)\n",
    "    fix_target(train_supplamanteary2, key, target2, new_target_name=target2)  \n",
    "    rate(train_supplamanteary2, key, target1, new_target_name=\"rate_\" +target1 )\n",
    "    rate(train_supplamanteary2, key, target2, new_target_name=\"rate_\" +target2 ) \n",
    "    difference(train_supplamanteary2, key, target1, new_target_name=\"diff_\" +target1 )\n",
    "    difference(train_supplamanteary2, key, target2, new_target_name=\"diff_\" +target2 )    \n",
    "     \n",
    "    unique_keys_sup2=train_supplamanteary2[key].unique()\n",
    "    print(len(unique_keys_sup2))\n",
    "    if not geo_dir is None:\n",
    "        region_metadata=pd.read_csv(geo_dir+\"region_metadata.csv\")\n",
    "        km=KMeans(n_clusters=30, init='k-means++', n_init=10, max_iter=300, tol=0.0001,\n",
    "              precompute_distances='auto', verbose=0, random_state=1234, copy_x=True, n_jobs=None, algorithm='auto')\n",
    "        region_metadata[\"clusters\"]=km.fit_predict(region_metadata[[\"lat\",\"lon\"]].values)          \n",
    "        train_supplamanteary2=pd.merge(train_supplamanteary2,region_metadata, how=\"left\", left_on=[\"Province_State\",\"Country_Region\"], right_on=[\"Province_State\",\"Country_Region\"] )\n",
    "\n",
    "    if  not group_by_columns is None and len(group_by_columns)>0:\n",
    "        group_names=[]\n",
    "        for group in group_by_columns:\n",
    "            groupss=train_supplamanteary2[[\"Date\", group,target1,target2]]\n",
    "            grp=groupss.groupby([\"Date\", group], as_index=False).sum()\n",
    "            grp.columns=[\"Date\", group,group +\"_\" + target1,group +\"_\" + target2 ]\n",
    "            train_supplamanteary2=pd.merge(train_supplamanteary2,grp, how=\"left\", left_on=[\"Date\", group,group], right_on=[\"Date\", group,group] )\n",
    "            #group_names+=[group +\"_\" + target1,group +\"_\" + target2]\n",
    "            for gr in [group +\"_\" + target1,group +\"_\" + target2]:\n",
    "                rate(train_supplamanteary2, key, gr, new_target_name=\"rate_\" +gr ) \n",
    "                group_names+=[\"rate_\" +gr]           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313\n"
     ]
    }
   ],
   "source": [
    "fix_target(train, key, target1, new_target_name=target1)\n",
    "fix_target(train, key, target2, new_target_name=target2)\n",
    "\n",
    "rate(train, key, target1, new_target_name=\"rate_\" +target1 )\n",
    "rate(train, key, target2, new_target_name=\"rate_\" +target2 )\n",
    "\n",
    "difference(train, key, target1, new_target_name=\"diff_\" +target1 )\n",
    "difference(train, key, target2, new_target_name=\"diff_\" +target2 )\n",
    "\n",
    "\n",
    "unique_keys=train[key].unique()\n",
    "print(len(unique_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_lags(rate_array, current_index, size=20):\n",
    "    lag_confirmed_rate=[-1 for k in range(size)]\n",
    "    for j in range (0, size):\n",
    "        if current_index-j>=0:\n",
    "            lag_confirmed_rate[j]=rate_array[current_index-j]\n",
    "        else :\n",
    "            break\n",
    "    return lag_confirmed_rate\n",
    "\n",
    "def days_ago_thresold_hit(full_array, indx, thresold):\n",
    "        days_ago_confirmed_count_10=-1\n",
    "        if full_array[indx]>thresold: # if currently the count of confirmed is more than 10\n",
    "            for j in range (indx,-1,-1):\n",
    "                entered=False\n",
    "                if full_array[j]<=thresold:\n",
    "                    days_ago_confirmed_count_10=abs(j-indx)\n",
    "                    entered=True\n",
    "                    break\n",
    "                if entered==False:\n",
    "                    days_ago_confirmed_count_10=100 #this value would we don;t know it cross 0      \n",
    "        return days_ago_confirmed_count_10 \n",
    "    \n",
    "    \n",
    "def ewma_vectorized(data, alpha):\n",
    "    sums=sum([ (alpha**(k+1))*data[k] for  k in range(len(data)) ])\n",
    "    counts=sum([ (alpha**(k+1)) for  k in range(len(data)) ])\n",
    "    return sums/counts\n",
    "\n",
    "def generate_ma_std_window(rate_array, current_index, size=20, window=3):\n",
    "    ma_rate_confirmed=[-1 for k in range(size)]\n",
    "    std_rate_confirmed=[-1 for k in range(size)] \n",
    "    \n",
    "    for j in range (0, size):\n",
    "        if current_index-j>=0:\n",
    "            ma_rate_confirmed[j]=np.mean(rate_array[max(0,current_index-j-window+1 ):current_index-j+1])\n",
    "            std_rate_confirmed[j]=np.std(rate_array[max(0,current_index-j-window+1 ):current_index-j+1])           \n",
    "        else :\n",
    "            break\n",
    "    return ma_rate_confirmed, std_rate_confirmed\n",
    "\n",
    "def generate_ewma_window(rate_array, current_index, size=20, window=3, alpha=0.05):\n",
    "    ewma_rate_confirmed=[-1 for k in range(size)]\n",
    "\n",
    "    \n",
    "    for j in range (0, size):\n",
    "        if current_index-j>=0:\n",
    "            ewma_rate_confirmed[j]=ewma_vectorized(rate_array[max(0,current_index-j-window+1 ):current_index-j+1, ], alpha)           \n",
    "        else :\n",
    "            break\n",
    "    \n",
    "    #print(ewma_rate_confirmed)\n",
    "    return ewma_rate_confirmed\n",
    "\n",
    "\n",
    "def get_target(rate_col, indx, horizon=33, average=3, use_hard_rule=False):\n",
    "    target_values=[-1 for k in range(horizon)]\n",
    "    cou=0\n",
    "    for j in range(indx+1, indx+1+horizon):\n",
    "        if j<len(rate_col):\n",
    "            if average==1:\n",
    "                target_values[cou]=rate_col[j]\n",
    "            else :\n",
    "                if use_hard_rule and j +average <=len(rate_col) :\n",
    "                     target_values[cou]=np.mean(rate_col[j:j +average])\n",
    "                else :\n",
    "                    target_values[cou]=np.mean(rate_col[j:min(len(rate_col),j +average)])\n",
    "                   \n",
    "            cou+=1\n",
    "        else :\n",
    "            break\n",
    "    return target_values\n",
    "\n",
    "def get_target_count(rate_col, indx, horizon=33, average=3, use_hard_rule=False):\n",
    "    target_values=[-1 for k in range(horizon)]\n",
    "    cou=0\n",
    "    for j in range(indx+1, indx+1+horizon):\n",
    "        if j<len(rate_col):\n",
    "            if average==1:\n",
    "                target_values[cou]=rate_col[j]\n",
    "            else :\n",
    "                if use_hard_rule and j +average <=len(rate_col) :\n",
    "                     target_values[cou]=np.mean(rate_col[j:j +average])\n",
    "                else :\n",
    "                    target_values[cou]=np.mean(rate_col[j:min(len(rate_col),j +average)])\n",
    "                   \n",
    "            cou+=1\n",
    "        else :\n",
    "            break\n",
    "    return target_values\n",
    "\n",
    "\n",
    "def dereive_features(frame, confirmed, fatalities, rate_confirmed, rate_fatalities, count_confirmed, count_fatalities ,\n",
    "                     horizon ,size=20, windows=[3,7], days_back_confimed=[1,10,100], days_back_fatalities=[1,2,10], \n",
    "                    extra_data=None, groups_data=None, windows_group=[3,7], size_group=20,\n",
    "                    days_back_confimed_group=[1,10,100]):\n",
    "    targets=[]\n",
    "    if not extra_data is None:\n",
    "        assert len(extra_stable_columns)==extra_data.shape[1]\n",
    "        \n",
    "    if not groups_data is None:\n",
    "        assert len(group_names)==groups_data.shape[1]        \n",
    "    names=[]    \n",
    "    #names=[\"lag_confirmed_rate\" + str(k+1) for k in range (size)]\n",
    "    for day in days_back_confimed:\n",
    "        names+=[\"days_ago_confirmed_count_\" + str(day) ]\n",
    "    for window in windows:        \n",
    "        names+=[\"ma\" + str(window) + \"_rate_confirmed\" + str(k+1) for k in range (size)]\n",
    "        #names+=[\"std\" + str(window) + \"_rate_confirmed\" + str(k+1) for k in range (size)] \n",
    "        #names+=[\"ewma\" + str(window) + \"_rate_confirmed\" + str(k+1) for k in range (size)]         \n",
    "        names+=[\"ma\" + str(window) + \"_count_confirmed\" + str(k+1) for k in range (size)]        \n",
    "        \n",
    "    #names+=[\"lag_fatalities_rate\" + str(k+1) for k in range (size)]\n",
    "    for day in days_back_fatalities:\n",
    "        names+=[\"days_ago_fatalitiescount_\" + str(day) ]    \n",
    "    for window in windows:        \n",
    "        names+=[\"ma\" + str(window) + \"_rate_fatalities\" + str(k+1) for k in range (size)]\n",
    "        #names+=[\"std\" + str(window) + \"_rate_fatalities\" + str(k+1) for k in range (size)]  \n",
    "        #names+=[\"ewma\" + str(window) + \"_rate_fatalities\" + str(k+1) for k in range (size)] \n",
    "        names+=[\"ma\" + str(window) + \"_count_fatalities\" + str(k+1) for k in range (size)]\n",
    "        \n",
    "    names+=[\"confirmed_level\"]\n",
    "    names+=[\"fatalities_level\"]   \n",
    "    if not extra_data is None: \n",
    "        names+=[k for k in extra_stable_columns]\n",
    "    if not groups_data is None:  \n",
    "         for gg in range (groups_data.shape[1]):\n",
    "             #names+=[\"lag_rate_group_\"+ str(gg+1) + \"_\" + str(k+1) for k in range (size_group)]    \n",
    "             for day in days_back_confimed_group:\n",
    "                names+=[\"days_ago_grooupcount_\" + str(gg+1) + \"_\" + str(day) ]             \n",
    "             for window in windows_group:        \n",
    "                names+=[\"ma_group_\" + str(gg+1) + \"_\" + str(window) + \"_rate_\" + str(k+1) for k in range (size_group)]\n",
    "                #names+=[\"std_group_\" + str(gg+1)+ \"_\" + str(window) + \"_rate_\" + str(k+1) for k in range (size_group)]  \n",
    "                #names+=[\"ewma_group_\" + str(gg+1) + \"_\" + str(window) + \"_rate_\" + str(k+1) for k in range (size)]  \n",
    "\n",
    "            \n",
    "    names+=[\"confirmed_plus\" + str(k+1) for k in range (horizon)]    \n",
    "    names+=[\"fatalities_plus\" + str(k+1) for k in range (horizon)]  \n",
    "    names+=[\"confirmed_count_plus\" + str(k+1) for k in range (horizon)]    \n",
    "    names+=[\"fatalities_count_plus\" + str(k+1) for k in range (horizon)]      \n",
    "    #names+=[\"current_confirmed\"]\n",
    "    #names+=[\"current_fatalities\"]    \n",
    "    \n",
    "    features=[]\n",
    "    for i in range (len(confirmed)):\n",
    "        row_features=[]\n",
    "        #####################lag_confirmed_rate       \n",
    "        lag_confirmed_rate=get_lags(rate_confirmed, i, size=size)\n",
    "        #row_features+=lag_confirmed_rate\n",
    "        #####################days_ago_confirmed_count_10\n",
    "        for day in days_back_confimed:\n",
    "            days_ago_confirmed_count_10=days_ago_thresold_hit(confirmed, i, day)               \n",
    "            row_features+=[days_ago_confirmed_count_10] \n",
    "        #####################ma_rate_confirmed       \n",
    "        #####################std_rate_confirmed \n",
    "        for window in windows:\n",
    "            ma3_rate_confirmed,std3_rate_confirmed= generate_ma_std_window(rate_confirmed, i, size=size, window=window)\n",
    "            row_features+= ma3_rate_confirmed   \n",
    "            #row_features+= std3_rate_confirmed          \n",
    "            #ewma3_rate_confirmed=generate_ewma_window(rate_confirmed, i, size=size, window=window, alpha=0.05)\n",
    "            #row_features+= ewma3_rate_confirmed \n",
    "            ma3_count_confirmed,std3_count_confirmed= generate_ma_std_window(count_confirmed, i, size=size, window=window)  \n",
    "            row_features+= ma3_count_confirmed               \n",
    "        #####################lag_fatalities_rate   \n",
    "        lag_fatalities_rate=get_lags(rate_fatalities, i, size=size)\n",
    "        #row_features+=lag_fatalities_rate\n",
    "        #####################days_ago_confirmed_count_10\n",
    "        for day in days_back_fatalities:\n",
    "            days_ago_fatalitiescount_2=days_ago_thresold_hit(fatalities, i, day)               \n",
    "            row_features+=[days_ago_fatalitiescount_2]     \n",
    "        #####################ma_rate_fatalities       \n",
    "        #####################std_rate_fatalities \n",
    "        for window in windows:        \n",
    "            ma3_rate_fatalities,std3_rate_fatalities= generate_ma_std_window(rate_fatalities, i, size=size, window=window)\n",
    "            row_features+= ma3_rate_fatalities             \n",
    "            #row_features+= std3_rate_fatalities  \n",
    "            #ewma3_rate_fatalities=generate_ewma_window(rate_fatalities, i, size=size, window=window, alpha=0.05)\n",
    "            #row_features+= ewma3_rate_fatalities\n",
    "            ma3_count_fatalities,std3_count_fatalities= generate_ma_std_window(count_fatalities, i, size=size, window=window)\n",
    "            row_features+= ma3_count_fatalities               \n",
    "            \n",
    "        ##################confirmed_level\n",
    "        confirmed_level=0\n",
    "        \n",
    "        \"\"\"\n",
    "        if confirmed[i]>0 and confirmed[i]<1000:\n",
    "            confirmed_level= confirmed[i]\n",
    "        else :\n",
    "            confirmed_level=2000\n",
    "        \"\"\"   \n",
    "        confirmed_level= confirmed[i]\n",
    "        row_features+=[confirmed_level]\n",
    "        ##################fatalities_is_level\n",
    "        fatalities_is_level=0\n",
    "        \"\"\"\n",
    "        if fatalities[i]>0 and fatalities[i]<100:\n",
    "            fatalities_is_level= fatalities[i]\n",
    "        else :\n",
    "            fatalities_is_level=200            \n",
    "        \"\"\"\n",
    "        fatalities_is_level= fatalities[i]\n",
    "        \n",
    "        row_features+=[fatalities_is_level] \n",
    "        \n",
    "        if not extra_data is None:    \n",
    "            row_features+=extra_data[i].tolist()\n",
    "            \n",
    "        if not groups_data is None:  \n",
    "          for gg in range (groups_data.shape[1]): \n",
    "             ## lags per group\n",
    "             this_group=groups_data[:,gg].tolist()\n",
    "             lag_group_rate=get_lags(this_group, i, size=size_group)\n",
    "             #row_features+=lag_group_rate           \n",
    "             #####################days_ago_confirmed_count_10\n",
    "             for day in days_back_confimed_group:\n",
    "                days_ago_groupcount_2=days_ago_thresold_hit(this_group, i, day)               \n",
    "                row_features+=[days_ago_groupcount_2]     \n",
    "             #####################ma_rate_fatalities       \n",
    "             #####################std_rate_fatalities \n",
    "             for window in windows_group:        \n",
    "                ma3_rate_group,std3_rate_group= generate_ma_std_window(this_group, i, size=size_group, window=window)\n",
    "                row_features+= ma3_rate_group   \n",
    "                #row_features+= std3_rate_group             \n",
    "            \n",
    "            \n",
    "        #######################confirmed_plus target\n",
    "        confirmed_plus=get_target(rate_confirmed, i, horizon=horizon)\n",
    "        row_features+= confirmed_plus          \n",
    "        #######################fatalities_plus target\n",
    "        fatalities_plus=get_target(rate_fatalities, i, horizon=horizon)\n",
    "        row_features+= fatalities_plus \n",
    "            \n",
    "        #######################confirmed_plus target count\n",
    "        confirmed_plus=get_target(count_confirmed, i, horizon=horizon)\n",
    "        row_features+= confirmed_plus          \n",
    "        #######################fatalities_plus target\n",
    "        fatalities_plus=get_target(count_fatalities, i, horizon=horizon)\n",
    "        row_features+= fatalities_plus         \n",
    "        \n",
    "        \n",
    "        ##################current_confirmed\n",
    "        #row_features+=[confirmed[i]]\n",
    "        ##################current_fatalities\n",
    "        #row_features+=[fatalities[i]]        \n",
    "        \n",
    "          \n",
    "\n",
    "        \n",
    "        features.append(row_features)\n",
    "        \n",
    "    new_frame=pd.DataFrame(data=features, columns=names).reset_index(drop=True)\n",
    "    frame=frame.reset_index(drop=True)\n",
    "    frame=pd.concat([frame, new_frame], axis=1)\n",
    "    #print(frame.shape)\n",
    "    return frame\n",
    "    \n",
    "    \n",
    "def feature_engineering_for_single_key(frame, group, key, horizon=33, size=20, windows=[3,7], \n",
    "                                       days_back_confimed=[1,10,100], days_back_fatalities=[1,2,10],\n",
    "                                      extra_stable_=None, group_nams=None,windows_group=[3,7], \n",
    "                                       size_group=20, days_back_confimed_group=[1,10,100]):\n",
    "    \n",
    "    mini_frame=get_data_by_key(frame, group, key, fields=None)\n",
    "    \n",
    "    mini_frame_with_features=dereive_features(mini_frame, mini_frame[\"ConfirmedCases\"].values,\n",
    "                                              mini_frame[\"Fatalities\"].values, mini_frame[\"rate_ConfirmedCases\"].values, \n",
    "                                               mini_frame[\"rate_Fatalities\"].values, mini_frame[\"diff_ConfirmedCases\"].values, \n",
    "                                               mini_frame[\"diff_Fatalities\"].values,\n",
    "                                              \n",
    "                                              \n",
    "                                              \n",
    "                                              horizon ,size=size, windows=windows,\n",
    "                                              days_back_confimed=days_back_confimed, days_back_fatalities=days_back_fatalities,\n",
    "                                              extra_data=mini_frame[extra_stable_].values if not extra_stable_ is None else None,\n",
    "                                              groups_data=mini_frame[group_nams].values if not group_nams is None else None,\n",
    "                                              windows_group=windows_group, size_group=size_group, \n",
    "                                              days_back_confimed_group=days_back_confimed_group)\n",
    "    #print (mini_frame_with_features.shape[0])\n",
    "    return mini_frame_with_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [01:40<00:00,  3.11it/s]\n",
      "  0%|          | 0/3565 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3565/3565 [14:03<00:00,  4.23it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "train_frame=[]\n",
    "size=10\n",
    "windows=[3]\n",
    "days_back_confimed=[1,5,10,20,50,100,250,500,1000]\n",
    "days_back_fatalities=[1,2,5,10,20,50]\n",
    "\n",
    "size_group=10\n",
    "windows_group=[3]\n",
    "days_back_confimed_group=[1,10,100]\n",
    "\n",
    "\n",
    "#print (len(train['key'].unique()))\n",
    "for unique_k in tqdm(unique_keys):\n",
    "    mini_frame=feature_engineering_for_single_key(train, key, unique_k, horizon=horizon, size=size, \n",
    "                                                  windows=windows, days_back_confimed=days_back_confimed,\n",
    "                                                  days_back_fatalities=days_back_fatalities,\n",
    "                                                  extra_stable_=extra_stable_columns if extra_stable_columns is not None and len(extra_stable_columns)>0 else None,\n",
    "                                     group_nams=group_names,windows_group=windows_group, \n",
    "                                     size_group=size_group, days_back_confimed_group=days_back_confimed_group\n",
    "                                                 ).reset_index(drop=True) \n",
    "    #print (mini_frame.shape[0])\n",
    "    train_frame.append(mini_frame)\n",
    "    \n",
    "train_frame = pd.concat(train_frame, axis=0).reset_index(drop=True)\n",
    "#train_frame.to_csv(directory +\"all\" + \".csv\", index=False)\n",
    "new_unique_keys=train_frame['key'].unique()\n",
    "for kee in new_unique_keys:\n",
    "    if kee not in unique_keys:\n",
    "        print (kee , \" is not there \")\n",
    "\n",
    "print (\"==================================================\") #Add USA county data for the count model built later on           \n",
    "train_frame_supplamanteary2=None\n",
    "if not train_supplamanteary2 is None:\n",
    "    train_frame_supplamanteary2=[]\n",
    "    for unique_k in tqdm(unique_keys_sup2):\n",
    "        mini_frame_sup=feature_engineering_for_single_key(train_supplamanteary2, key, unique_k, horizon=horizon, size=size, \n",
    "                                                      windows=windows, days_back_confimed=days_back_confimed,\n",
    "                                                      days_back_fatalities=days_back_fatalities,\n",
    "                                                      extra_stable_=extra_stable_columns if extra_stable_columns is not None and len(extra_stable_columns)>0 else None,\n",
    "                                     group_nams=group_names,windows_group=windows_group, \n",
    "                                     size_group=size_group, days_back_confimed_group=days_back_confimed_group ).reset_index(drop=True) \n",
    "        \n",
    "        train_frame_supplamanteary2.append(mini_frame_sup) \n",
    "    train_frame_supplamanteary2 = pd.concat(train_frame_supplamanteary2, axis=0).reset_index(drop=True)\n",
    "    new_unique_keys_sup2=train_frame_supplamanteary2['key'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "####the training model\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "def bagged_set_train(X_ts,y_cs,wts, seed, estimators,xtest, xt=None,yt=None, output_name=None):\n",
    "   #print (type(yt))\n",
    "   # create array object to hold predictions \n",
    "  \n",
    "   baggedpred=np.array([ 0.0 for d in range(0, xtest.shape[0])]) \n",
    "   #print (y_cs[:10])\n",
    "   #print (yt[:10])  \n",
    "\n",
    "   #loop for as many times as we want bags\n",
    "   for n in range (0, estimators):\n",
    "       \n",
    "       params = {'objective': 'rmse',\n",
    "                'metric': 'rmse',\n",
    "                'boosting': 'gbdt',\n",
    "                'learning_rate': 0.005, #change here    \n",
    "                'drop_rate':0.01,\n",
    "                #'alpha': 0.99, \n",
    "                'skip_drop':0.6,\n",
    "                'uniform_drop':True,               \n",
    "                'verbose': -1,    \n",
    "                'num_leaves': 40, # ~18    \n",
    "                'bagging_fraction': 0.9,    \n",
    "                'bagging_freq': 1,    \n",
    "                'bagging_seed': seed + n,    \n",
    "                'feature_fraction': 0.8,    \n",
    "                'feature_fraction_seed': seed + n,    \n",
    "                'min_data_in_leaf': 10, #30, #56, # 10-50    \n",
    "                'max_bin': 100, # maybe useful with overfit problem    \n",
    "                'max_depth':20,                   \n",
    "                #'reg_lambda': 10,    \n",
    "                'reg_alpha':1,    \n",
    "                'lambda_l2': 10,\n",
    "                #'categorical_feature':'2', # because training data is extremely unbalanced                     \n",
    "                'num_threads':6\n",
    "                }\n",
    "       d_train = lgb.Dataset(X_ts,y_cs, weight=wts, free_raw_data=False)#np.log1p(\n",
    "       if not type(yt) is type(None):           \n",
    "           d_cv = lgb.Dataset(xt,yt, free_raw_data=False, reference=d_train)#, reference=d_train\n",
    "           model = lgb.train(params,d_train,num_boost_round=500,\n",
    "                             valid_sets=d_cv,\n",
    "\n",
    "                             verbose_eval=50 ) #1000                        \n",
    "           \n",
    "       else :\n",
    "           #d_cv = lgb.Dataset(xt, free_raw_data=False, categorical_feature=\"2\")  \n",
    "           model = lgb.train(params,d_train,num_boost_round=500) #1000                              \n",
    "           #importances=model.feature_importance('gain')\n",
    "           #print(importances)\n",
    "       preds=model.predict(xtest)               \n",
    "       # update bag's array\n",
    "       baggedpred+=preds\n",
    "       #np.savetxt(\"preds_lgb\" + str(n)+ \".csv\",baggedpred)   \n",
    "       #if n%5==0:\n",
    "           #print(\"completed: \" + str(n)  )                 \n",
    "\n",
    "   if not output_name is None:\n",
    "        joblib.dump((model), output_name)\n",
    "   \"\"\"\n",
    "   model=Ridge(normalize=True, alpha=0.1, random_state=1)\n",
    "   model.fit(X_ts,y_cs)\n",
    "   preds=model.predict(xtest)\n",
    "   baggedpred+=preds\n",
    "   \"\"\"\n",
    "   # divide with number of bags to create an average estimate  \n",
    "   baggedpred/= estimators\n",
    "     \n",
    "   return baggedpred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the inference part\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "def predict(xtest,input_name=None):\n",
    "   #print (type(yt))\n",
    "   # create array object to hold predictions \n",
    "  \n",
    "   baggedpred=np.array([ 0.0 for d in range(0, xtest.shape[0])]) \n",
    "   model=  joblib.load( input_name) \n",
    "   preds=model.predict(xtest)               \n",
    "   baggedpred+=preds\n",
    "\n",
    "   return baggedpred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features to use in the rate model\n",
    "\n",
    "names=[]\n",
    "#names=[\"lag_confirmed_rate\" + str(k+1) for k in range (size)]\n",
    "for day in days_back_confimed:\n",
    "    names+=[\"days_ago_confirmed_count_\" + str(day) ]\n",
    "for window in windows:        \n",
    "    names+=[\"ma\" + str(window) + \"_rate_confirmed\" + str(k+1) for k in range (size)]\n",
    "    #names+=[\"std\" + str(window) + \"_rate_confirmed\" + str(k+1) for k in range (size)] \n",
    "    #names+=[\"ewma\" + str(window) + \"_rate_confirmed\" + str(k+1) for k in range (size)]         \n",
    "    #names+=[\"ma\" + str(window) + \"_count_confirmed\" + str(k+1) for k in range (size)]  \n",
    "\n",
    "#names+=[\"lag_fatalities_rate\" + str(k+1) for k in range (size)]\n",
    "for day in days_back_fatalities:\n",
    "    names+=[\"days_ago_fatalitiescount_\" + str(day) ]    \n",
    "for window in windows:        \n",
    "    names+=[\"ma\" + str(window) + \"_rate_fatalities\" + str(k+1) for k in range (size)]\n",
    "    #names+=[\"std\" + str(window) + \"_rate_fatalities\" + str(k+1) for k in range (size)]  \n",
    "    #names+=[\"ewma\" + str(window) + \"_rate_fatalities\" + str(k+1) for k in range (size)]    \n",
    "    #names+=[\"ma\" + str(window) + \"_count_fatalities\" + str(k+1) for k in range (size)]    \n",
    "names+=[\"confirmed_level\"]\n",
    "names+=[\"fatalities_level\"]   \n",
    "if not extra_stable_columns is None and len(extra_stable_columns)>0: \n",
    "    names+=[k for k in extra_stable_columns]  \n",
    "    \n",
    "if not group_names is None:  \n",
    "     for gg in range (len(group_names)):\n",
    "         #names+=[\"lag_rate_group_\"+ str(gg+1) + \"_\" + str(k+1) for k in range (size_group)]    \n",
    "         for day in days_back_confimed_group:\n",
    "            names+=[\"days_ago_grooupcount_\" + str(gg+1) + \"_\" + str(day) ]             \n",
    "         for window in windows_group:        \n",
    "            names+=[\"ma_group_\" + str(gg+1) + \"_\" + str(window) + \"_rate_\" + str(k+1) for k in range (size_group)]\n",
    "            #names+=[\"std_group_\" + str(gg+1)+ \"_\" + str(window) + \"_rate_\" + str(k+1) for k in range (size_group)]  \n",
    "            #names+=[\"ewma_group_\" + str(gg+1) + \"_\" + str(window) + \"_rate_\" + str(k+1) for k in range (size)]      \n",
    "      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntr_frame=train_frame\\n\\ntarget_confirmed=[\"confirmed_plus\" + str(k+1) for k in range (horizon)]    \\ntarget_fatalities=[\"fatalities_plus\" + str(k+1) for k in range (horizon)] \\nseed=1412\\n\\ntarget_confirmed_train=tr_frame[target_confirmed].values\\nprint (\"  original shape of train is {}  \".format( target_confirmed_train.shape) )\\n\\ntarget_fatalities_train=tr_frame[target_fatalities].values\\nfeatures_train=tr_frame[names].values   \\n\\nstandard_confirmed_train=tr_frame[\"ConfirmedCases\"].values\\nstandard_fatalities_train=tr_frame[\"Fatalities\"].values\\ncurrent_confirmed_train=tr_frame[\"ConfirmedCases\"].values\\n\\n     \\n\\nfeatures_cv=[]\\nname_cv=[]\\nstandard_confirmed_cv=[]\\nstandard_fatalities_cv=[]\\nnames_=tr_frame[\"key\"].values\\ntraining_horizon=int(features_train.shape[0]/len(unique_keys)) \\nprint(\"training horizon = \",training_horizon)\\nfor dd in range(training_horizon-1,features_train.shape[0],training_horizon):\\n    features_cv.append(features_train[dd])\\n    name_cv.append(names_[dd])\\n    standard_confirmed_cv.append(standard_confirmed_train[dd])\\n    standard_fatalities_cv.append(standard_fatalities_train[dd])\\n    print (name_cv[-1], standard_confirmed_cv[-1], standard_fatalities_cv[-1])\\n    \\n \\n    \\ncurrent_confirmed_train=[k for k in range(len(current_confirmed_train)) if current_confirmed_train[k]>0]\\ntarget_confirmed_train=target_confirmed_train[current_confirmed_train]\\ntarget_fatalities_train=target_fatalities_train[current_confirmed_train]        \\nfeatures_train=features_train[current_confirmed_train]         \\nstandard_confirmed_train=standard_confirmed_train[current_confirmed_train]\\nstandard_fatalities_train=standard_fatalities_train[current_confirmed_train]  \\n    \\nfeatures_cv=np.array(features_cv)\\npreds_confirmed_cv=np.zeros((features_cv.shape[0],horizon))\\npreds_confirmed_standard_cv=np.zeros((features_cv.shape[0],horizon))\\n\\npreds_fatalities_cv=np.zeros((features_cv.shape[0],horizon))\\npreds_fatalities_standard_cv=np.zeros((features_cv.shape[0],horizon))\\n\\noveral_rmsle_metric_confirmed=0.0\\n\\nfor j in range (preds_confirmed_cv.shape[1]):\\n    this_target=target_confirmed_train[:,j]\\n    index_positive=[k for k in range(len(this_target)) if this_target[k]!=-1]\\n    this_features=features_train[index_positive]\\n    this_target=this_target[index_positive]\\n    this_weight=np.log(standard_confirmed_train[index_positive]+3.)\\n    #this_weight=[1. for k in range(len(this_weight))]\\n    this_features_cv=features_cv                          \\n\\n    preds=bagged_set_train(this_features,this_target,this_weight, seed, 1,features_cv, xt=None,yt=None, output_name=model_directory +\"confirmed\"+ str(j))\\n    preds_confirmed_cv[:,j]=preds\\n    print (\" modelling confirmed, case %d, original train %d, and after %d, original cv %d and after %d \"%(\\n    j,target_confirmed_train.shape[0],this_target.shape[0],this_features_cv.shape[0],this_features_cv.shape[0])) \\n\\npredictions=[]\\nfor ii in range (preds_confirmed_cv.shape[0]):\\n    current_prediction=standard_confirmed_cv[ii]\\n    if current_prediction==0 :\\n        current_prediction=0.1    \\n    for j in range (preds_confirmed_cv.shape[1]):\\n                current_prediction*=max(1,preds_confirmed_cv[ii][j])\\n                preds_confirmed_standard_cv[ii][j]=current_prediction\\n\\n\\n\\nfor j in range (preds_confirmed_cv.shape[1]):\\n    this_target=target_fatalities_train[:,j]\\n    index_positive=[k for k in range(len(this_target)) if this_target[k]!=-1]\\n    this_features=features_train[index_positive]\\n    this_target=this_target[index_positive]\\n    this_weight=np.log(standard_confirmed_train[index_positive]+2.)\\n    #this_weight=[1. for k in range(len(this_weight))]\\n    \\n    this_features_cv=features_cv\\n                             \\n    preds=bagged_set_train(this_features,this_target,this_weight, seed, 1,features_cv, xt=None,yt=None, output_name=model_directory +\"fatal\"+ str(j))\\n    preds_fatalities_cv[:,j]=preds\\n    print (\" modelling fatalities, case %d, original train %d, and after %d, original cv %d and after %d \"%(\\n    j,target_confirmed_train.shape[0],this_target.shape[0],this_features_cv.shape[0],this_features_cv.shape[0])) \\n\\npredictions=[]\\nfor ii in range (preds_fatalities_cv.shape[0]):\\n    current_prediction=standard_fatalities_cv[ii]\\n    if current_prediction==0 and standard_confirmed_cv[ii]>400:\\n        current_prediction=0.1\\n    for j in range (preds_fatalities_cv.shape[1]):\\n                if current_prediction==0 and  preds_confirmed_standard_cv[ii][j]>400:\\n                    current_prediction=1.\\n                current_prediction*=max(1,preds_fatalities_cv[ii][j])\\n                preds_fatalities_standard_cv[ii][j]=current_prediction\\n                \\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###############################THIS IS THE TRAINING PART OF MY CODE - ALL FILES ARE SAVED IN the model directory - you need to run it offline ####################\n",
    "#######################You need to uncomment this section to generate the lightgbm models one for each day+x rate for both confirmed and fatalities ##########################\n",
    "#################Full model ######################\n",
    "\"\"\"\n",
    "tr_frame=train_frame\n",
    "\n",
    "target_confirmed=[\"confirmed_plus\" + str(k+1) for k in range (horizon)]    \n",
    "target_fatalities=[\"fatalities_plus\" + str(k+1) for k in range (horizon)] \n",
    "seed=1412\n",
    "\n",
    "target_confirmed_train=tr_frame[target_confirmed].values\n",
    "print (\"  original shape of train is {}  \".format( target_confirmed_train.shape) )\n",
    "\n",
    "target_fatalities_train=tr_frame[target_fatalities].values\n",
    "features_train=tr_frame[names].values   \n",
    "\n",
    "standard_confirmed_train=tr_frame[\"ConfirmedCases\"].values\n",
    "standard_fatalities_train=tr_frame[\"Fatalities\"].values\n",
    "current_confirmed_train=tr_frame[\"ConfirmedCases\"].values\n",
    "\n",
    "     \n",
    "\n",
    "features_cv=[]\n",
    "name_cv=[]\n",
    "standard_confirmed_cv=[]\n",
    "standard_fatalities_cv=[]\n",
    "names_=tr_frame[\"key\"].values\n",
    "training_horizon=int(features_train.shape[0]/len(unique_keys)) \n",
    "print(\"training horizon = \",training_horizon)\n",
    "for dd in range(training_horizon-1,features_train.shape[0],training_horizon):\n",
    "    features_cv.append(features_train[dd])\n",
    "    name_cv.append(names_[dd])\n",
    "    standard_confirmed_cv.append(standard_confirmed_train[dd])\n",
    "    standard_fatalities_cv.append(standard_fatalities_train[dd])\n",
    "    print (name_cv[-1], standard_confirmed_cv[-1], standard_fatalities_cv[-1])\n",
    "    \n",
    " \n",
    "    \n",
    "current_confirmed_train=[k for k in range(len(current_confirmed_train)) if current_confirmed_train[k]>0]\n",
    "target_confirmed_train=target_confirmed_train[current_confirmed_train]\n",
    "target_fatalities_train=target_fatalities_train[current_confirmed_train]        \n",
    "features_train=features_train[current_confirmed_train]         \n",
    "standard_confirmed_train=standard_confirmed_train[current_confirmed_train]\n",
    "standard_fatalities_train=standard_fatalities_train[current_confirmed_train]  \n",
    "    \n",
    "features_cv=np.array(features_cv)\n",
    "preds_confirmed_cv=np.zeros((features_cv.shape[0],horizon))\n",
    "preds_confirmed_standard_cv=np.zeros((features_cv.shape[0],horizon))\n",
    "\n",
    "preds_fatalities_cv=np.zeros((features_cv.shape[0],horizon))\n",
    "preds_fatalities_standard_cv=np.zeros((features_cv.shape[0],horizon))\n",
    "\n",
    "overal_rmsle_metric_confirmed=0.0\n",
    "\n",
    "for j in range (preds_confirmed_cv.shape[1]):\n",
    "    this_target=target_confirmed_train[:,j]\n",
    "    index_positive=[k for k in range(len(this_target)) if this_target[k]!=-1]\n",
    "    this_features=features_train[index_positive]\n",
    "    this_target=this_target[index_positive]\n",
    "    this_weight=np.log(standard_confirmed_train[index_positive]+3.)\n",
    "    #this_weight=[1. for k in range(len(this_weight))]\n",
    "    this_features_cv=features_cv                          \n",
    "\n",
    "    preds=bagged_set_train(this_features,this_target,this_weight, seed, 1,features_cv, xt=None,yt=None, output_name=model_directory +\"confirmed\"+ str(j))\n",
    "    preds_confirmed_cv[:,j]=preds\n",
    "    print (\" modelling confirmed, case %d, original train %d, and after %d, original cv %d and after %d \"%(\n",
    "    j,target_confirmed_train.shape[0],this_target.shape[0],this_features_cv.shape[0],this_features_cv.shape[0])) \n",
    "\n",
    "predictions=[]\n",
    "for ii in range (preds_confirmed_cv.shape[0]):\n",
    "    current_prediction=standard_confirmed_cv[ii]\n",
    "    if current_prediction==0 :\n",
    "        current_prediction=0.1    \n",
    "    for j in range (preds_confirmed_cv.shape[1]):\n",
    "                current_prediction*=max(1,preds_confirmed_cv[ii][j])\n",
    "                preds_confirmed_standard_cv[ii][j]=current_prediction\n",
    "\n",
    "\n",
    "\n",
    "for j in range (preds_confirmed_cv.shape[1]):\n",
    "    this_target=target_fatalities_train[:,j]\n",
    "    index_positive=[k for k in range(len(this_target)) if this_target[k]!=-1]\n",
    "    this_features=features_train[index_positive]\n",
    "    this_target=this_target[index_positive]\n",
    "    this_weight=np.log(standard_confirmed_train[index_positive]+2.)\n",
    "    #this_weight=[1. for k in range(len(this_weight))]\n",
    "    \n",
    "    this_features_cv=features_cv\n",
    "                             \n",
    "    preds=bagged_set_train(this_features,this_target,this_weight, seed, 1,features_cv, xt=None,yt=None, output_name=model_directory +\"fatal\"+ str(j))\n",
    "    preds_fatalities_cv[:,j]=preds\n",
    "    print (\" modelling fatalities, case %d, original train %d, and after %d, original cv %d and after %d \"%(\n",
    "    j,target_confirmed_train.shape[0],this_target.shape[0],this_features_cv.shape[0],this_features_cv.shape[0])) \n",
    "\n",
    "predictions=[]\n",
    "for ii in range (preds_fatalities_cv.shape[0]):\n",
    "    current_prediction=standard_fatalities_cv[ii]\n",
    "    if current_prediction==0 and standard_confirmed_cv[ii]>400:\n",
    "        current_prediction=0.1\n",
    "    for j in range (preds_fatalities_cv.shape[1]):\n",
    "                if current_prediction==0 and  preds_confirmed_standard_cv[ii][j]>400:\n",
    "                    current_prediction=1.\n",
    "                current_prediction*=max(1,preds_fatalities_cv[ii][j])\n",
    "                preds_fatalities_standard_cv[ii][j]=current_prediction\n",
    "                \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training horizon =  84\n",
      "nan_Afghanistan 714.0 23.0\n",
      "nan_Albania 475.0 24.0\n",
      "nan_Algeria 2070.0 326.0\n",
      "nan_Andorra 659.0 31.0\n",
      "nan_Angola 19.0 2.0\n",
      "nan_Antigua and Barbuda 23.0 2.0\n",
      "nan_Argentina 2277.0 102.0\n",
      "nan_Armenia 1067.0 16.0\n",
      "Australian Capital Territory_Australia 103.0 2.0\n",
      "New South Wales_Australia 2870.0 25.0\n",
      "Northern Territory_Australia 28.0 0.0\n",
      "Queensland_Australia 998.0 5.0\n",
      "South Australia_Australia 433.0 4.0\n",
      "Tasmania_Australia 165.0 6.0\n",
      "Victoria_Australia 1291.0 14.0\n",
      "Western Australia_Australia 527.0 6.0\n",
      "nan_Austria 14226.0 384.0\n",
      "nan_Azerbaijan 1197.0 13.0\n",
      "nan_Bahamas 49.0 8.0\n",
      "nan_Bahrain 1528.0 7.0\n",
      "nan_Bangladesh 1012.0 46.0\n",
      "nan_Barbados 72.0 4.0\n",
      "nan_Belarus 3281.0 33.0\n",
      "nan_Belgium 31119.0 4157.0\n",
      "nan_Belize 18.0 2.0\n",
      "nan_Benin 35.0 1.0\n",
      "nan_Bhutan 5.0 0.0\n",
      "nan_Bolivia 354.0 28.0\n",
      "nan_Bosnia and Herzegovina 1083.0 40.0\n",
      "nan_Botswana 13.0 1.0\n",
      "nan_Brazil 25262.0 1532.0\n",
      "nan_Brunei 136.0 1.0\n",
      "nan_Bulgaria 713.0 35.0\n",
      "nan_Burkina Faso 528.0 30.0\n",
      "nan_Burma 63.0 4.0\n",
      "nan_Burundi 5.0 1.0\n",
      "nan_Cabo Verde 11.0 1.0\n",
      "nan_Cambodia 122.0 0.0\n",
      "nan_Cameroon 848.0 14.0\n",
      "Alberta_Canada 1870.0 48.0\n",
      "British Columbia_Canada 1490.0 69.0\n",
      "Manitoba_Canada 246.0 4.0\n",
      "New Brunswick_Canada 116.0 0.0\n",
      "Newfoundland and Labrador_Canada 244.0 3.0\n",
      "Northwest Territories_Canada 5.0 0.0\n",
      "Nova Scotia_Canada 517.0 3.0\n",
      "Ontario_Canada 7953.0 334.0\n",
      "Prince Edward Island_Canada 25.0 1.0\n",
      "Quebec_Canada 14248.0 435.0\n",
      "Saskatchewan_Canada 300.0 4.0\n",
      "Yukon_Canada 8.0 0.0\n",
      "nan_Central African Republic 11.0 0.0\n",
      "nan_Chad 23.0 0.0\n",
      "nan_Chile 7917.0 92.0\n",
      "Anhui_China 991.0 6.0\n",
      "Beijing_China 589.0 8.0\n",
      "Chongqing_China 579.0 6.0\n",
      "Fujian_China 353.0 1.0\n",
      "Gansu_China 139.0 2.0\n",
      "Guangdong_China 1564.0 8.0\n",
      "Guangxi_China 254.0 2.0\n",
      "Guizhou_China 147.0 2.0\n",
      "Hainan_China 168.0 6.0\n",
      "Hebei_China 327.0 6.0\n",
      "Heilongjiang_China 819.0 13.0\n",
      "Henan_China 1276.0 22.0\n",
      "Hong Kong_China 1012.0 4.0\n",
      "Hubei_China 67803.0 3221.0\n",
      "Hunan_China 1019.0 4.0\n",
      "Inner Mongolia_China 190.0 1.0\n",
      "Jiangsu_China 653.0 0.0\n",
      "Jiangxi_China 937.0 1.0\n",
      "Jilin_China 100.0 1.0\n",
      "Liaoning_China 145.0 2.0\n",
      "Macau_China 45.0 0.0\n",
      "Ningxia_China 75.0 0.0\n",
      "Qinghai_China 18.0 0.0\n",
      "Shaanxi_China 256.0 3.0\n",
      "Shandong_China 784.0 7.0\n",
      "Shanghai_China 618.0 7.0\n",
      "Shanxi_China 173.0 0.0\n",
      "Sichuan_China 560.0 3.0\n",
      "Tianjin_China 185.0 3.0\n",
      "Tibet_China 1.0 0.0\n",
      "Xinjiang_China 76.0 3.0\n",
      "Yunnan_China 184.0 2.0\n",
      "Zhejiang_China 1267.0 1.0\n",
      "nan_Colombia 2979.0 127.0\n",
      "nan_Congo (Brazzaville) 60.0 5.0\n",
      "nan_Congo (Kinshasa) 241.0 20.0\n",
      "nan_Costa Rica 618.0 3.0\n",
      "nan_Cote d'Ivoire 638.0 6.0\n",
      "nan_Croatia 1704.0 31.0\n",
      "nan_Cuba 766.0 21.0\n",
      "nan_Cyprus 695.0 12.0\n",
      "nan_Czechia 6111.0 161.0\n",
      "Faroe Islands_Denmark 184.0 0.0\n",
      "Greenland_Denmark 11.0 0.0\n",
      "nan_Denmark 6511.0 299.0\n",
      "nan_Diamond Princess 712.0 12.0\n",
      "nan_Djibouti 363.0 2.0\n",
      "nan_Dominica 16.0 0.0\n",
      "nan_Dominican Republic 3286.0 183.0\n",
      "nan_Ecuador 7603.0 369.0\n",
      "nan_Egypt 2350.0 178.0\n",
      "nan_El Salvador 149.0 6.0\n",
      "nan_Equatorial Guinea 41.0 0.0\n",
      "nan_Eritrea 34.0 0.0\n",
      "nan_Estonia 1373.0 31.0\n",
      "nan_Eswatini 15.0 0.0\n",
      "nan_Ethiopia 82.0 3.0\n",
      "nan_Fiji 16.0 0.0\n",
      "nan_Finland 3161.0 64.0\n",
      "French Guiana_France 86.0 0.0\n",
      "French Polynesia_France 55.0 0.0\n",
      "Guadeloupe_France 145.0 8.0\n",
      "Martinique_France 157.0 6.0\n",
      "Mayotte_France 217.0 3.0\n",
      "New Caledonia_France 18.0 0.0\n",
      "Reunion_France 391.0 0.0\n",
      "Saint Barthelemy_France 6.0 0.0\n",
      "Saint Pierre and Miquelon_France 1.0 0.0\n",
      "St Martin_France 32.0 2.0\n",
      "nan_France 136779.0 15729.0\n",
      "nan_Gabon 57.0 1.0\n",
      "nan_Gambia 9.0 1.0\n",
      "nan_Georgia 300.0 3.0\n",
      "nan_Germany 131359.0 3294.0\n",
      "nan_Ghana 636.0 8.0\n",
      "nan_Greece 2170.0 101.0\n",
      "nan_Grenada 14.0 0.0\n",
      "nan_Guatemala 167.0 5.0\n",
      "nan_Guinea 363.0 0.0\n",
      "nan_Guinea-Bissau 38.0 0.0\n",
      "nan_Guyana 47.0 6.0\n",
      "nan_Haiti 40.0 3.0\n",
      "nan_Holy See 8.0 0.0\n",
      "nan_Honduras 407.0 26.0\n",
      "nan_Hungary 1512.0 122.0\n",
      "nan_Iceland 1720.0 8.0\n",
      "nan_India 11487.0 393.0\n",
      "nan_Indonesia 4839.0 459.0\n",
      "nan_Iran 74877.0 4683.0\n",
      "nan_Iraq 1400.0 78.0\n",
      "nan_Ireland 11479.0 406.0\n",
      "nan_Israel 12046.0 123.0\n",
      "nan_Italy 162488.0 21067.0\n",
      "nan_Jamaica 73.0 4.0\n",
      "nan_Japan 7645.0 143.0\n",
      "nan_Jordan 397.0 7.0\n",
      "nan_Kazakhstan 1232.0 14.0\n",
      "nan_Kenya 216.0 9.0\n",
      "nan_Korea, South 10564.0 222.0\n",
      "nan_Kosovo 387.0 8.0\n",
      "nan_Kuwait 1355.0 3.0\n",
      "nan_Kyrgyzstan 430.0 5.0\n",
      "nan_Laos 19.0 0.0\n",
      "nan_Latvia 657.0 5.0\n",
      "nan_Lebanon 641.0 21.0\n",
      "nan_Liberia 59.0 6.0\n",
      "nan_Libya 35.0 1.0\n",
      "nan_Liechtenstein 79.0 1.0\n",
      "nan_Lithuania 1070.0 29.0\n",
      "nan_Luxembourg 3307.0 69.0\n",
      "nan_MS Zaandam 9.0 2.0\n",
      "nan_Madagascar 108.0 0.0\n",
      "nan_Malawi 16.0 2.0\n",
      "nan_Malaysia 4987.0 82.0\n",
      "nan_Maldives 20.0 0.0\n",
      "nan_Mali 144.0 13.0\n",
      "nan_Malta 393.0 3.0\n",
      "nan_Mauritania 7.0 1.0\n",
      "nan_Mauritius 324.0 9.0\n",
      "nan_Mexico 5014.0 332.0\n",
      "nan_Moldova 1934.0 40.0\n",
      "nan_Monaco 93.0 1.0\n",
      "nan_Mongolia 30.0 0.0\n",
      "nan_Montenegro 283.0 4.0\n",
      "nan_Morocco 1888.0 126.0\n",
      "nan_Mozambique 28.0 0.0\n",
      "nan_Namibia 16.0 0.0\n",
      "nan_Nepal 16.0 0.0\n",
      "Aruba_Netherlands 92.0 0.0\n",
      "Bonaire, Sint Eustatius and Saba_Netherlands 3.0 0.0\n",
      "Curacao_Netherlands 14.0 1.0\n",
      "Sint Maarten_Netherlands 52.0 9.0\n",
      "nan_Netherlands 27419.0 2945.0\n",
      "nan_New Zealand 1366.0 9.0\n",
      "nan_Nicaragua 9.0 1.0\n",
      "nan_Niger 570.0 14.0\n",
      "nan_Nigeria 373.0 11.0\n",
      "nan_North Macedonia 908.0 44.0\n",
      "nan_Norway 6623.0 139.0\n",
      "nan_Oman 813.0 4.0\n",
      "nan_Pakistan 5837.0 96.0\n",
      "nan_Panama 3472.0 94.0\n",
      "nan_Papua New Guinea 2.0 0.0\n",
      "nan_Paraguay 159.0 7.0\n",
      "nan_Peru 10303.0 230.0\n",
      "nan_Philippines 5223.0 335.0\n",
      "nan_Poland 7202.0 263.0\n",
      "nan_Portugal 17448.0 567.0\n",
      "nan_Qatar 3428.0 7.0\n",
      "nan_Romania 6879.0 351.0\n",
      "nan_Russia 21102.0 170.0\n",
      "nan_Rwanda 134.0 0.0\n",
      "nan_Saint Kitts and Nevis 14.0 0.0\n",
      "nan_Saint Lucia 15.0 0.0\n",
      "nan_Saint Vincent and the Grenadines 12.0 0.0\n",
      "nan_San Marino 371.0 36.0\n",
      "nan_Sao Tome and Principe 4.0 0.0\n",
      "nan_Saudi Arabia 5369.0 73.0\n",
      "nan_Senegal 299.0 2.0\n",
      "nan_Serbia 4465.0 94.0\n",
      "nan_Seychelles 11.0 0.0\n",
      "nan_Sierra Leone 11.0 0.0\n",
      "nan_Singapore 3252.0 10.0\n",
      "nan_Slovakia 835.0 2.0\n",
      "nan_Slovenia 1220.0 56.0\n",
      "nan_Somalia 60.0 2.0\n",
      "nan_South Africa 2415.0 27.0\n",
      "nan_South Sudan 4.0 0.0\n",
      "nan_Spain 172541.0 18056.0\n",
      "nan_Sri Lanka 233.0 7.0\n",
      "nan_Sudan 32.0 5.0\n",
      "nan_Suriname 10.0 1.0\n",
      "nan_Sweden 11445.0 1033.0\n",
      "nan_Switzerland 25936.0 1174.0\n",
      "nan_Syria 29.0 2.0\n",
      "nan_Taiwan* 393.0 6.0\n",
      "nan_Tanzania 53.0 3.0\n",
      "nan_Thailand 2613.0 41.0\n",
      "nan_Timor-Leste 6.0 0.0\n",
      "nan_Togo 77.0 3.0\n",
      "nan_Trinidad and Tobago 113.0 8.0\n",
      "nan_Tunisia 747.0 34.0\n",
      "nan_Turkey 65111.0 1403.0\n",
      "Alabama_US 3953.0 114.0\n",
      "Alaska_US 285.0 9.0\n",
      "Arizona_US 3809.0 131.0\n",
      "Arkansas_US 1498.0 32.0\n",
      "California_US 25356.0 768.0\n",
      "Colorado_US 7946.0 327.0\n",
      "Connecticut_US 13989.0 671.0\n",
      "Delaware_US 1926.0 43.0\n",
      "District of Columbia_US 2058.0 67.0\n",
      "Florida_US 21628.0 571.0\n",
      "Georgia_US 14578.0 525.0\n",
      "Guam_US 133.0 5.0\n",
      "Hawaii_US 511.0 9.0\n",
      "Idaho_US 1464.0 33.0\n",
      "Illinois_US 23248.0 868.0\n",
      "Indiana_US 8527.0 387.0\n",
      "Iowa_US 1899.0 49.0\n",
      "Kansas_US 1441.0 69.0\n",
      "Kentucky_US 2048.0 113.0\n",
      "Louisiana_US 21518.0 1013.0\n",
      "Maine_US 735.0 20.0\n",
      "Maryland_US 8936.0 262.0\n",
      "Massachusetts_US 28164.0 844.0\n",
      "Michigan_US 27001.0 1768.0\n",
      "Minnesota_US 1695.0 79.0\n",
      "Mississippi_US 3087.0 111.0\n",
      "Missouri_US 4746.0 149.0\n",
      "Montana_US 399.0 7.0\n",
      "Nebraska_US 897.0 20.0\n",
      "Nevada_US 3134.0 126.0\n",
      "New Hampshire_US 985.0 25.0\n",
      "New Jersey_US 68824.0 2805.0\n",
      "New Mexico_US 1345.0 31.0\n",
      "New York_US 203020.0 10842.0\n",
      "North Carolina_US 5113.0 112.0\n",
      "North Dakota_US 341.0 8.0\n",
      "Ohio_US 7285.0 324.0\n",
      "Oklahoma_US 2184.0 108.0\n",
      "Oregon_US 1633.0 55.0\n",
      "Pennsylvania_US 25465.0 691.0\n",
      "Puerto Rico_US 923.0 45.0\n",
      "Rhode Island_US 3251.0 80.0\n",
      "South Carolina_US 3553.0 97.0\n",
      "South Dakota_US 988.0 6.0\n",
      "Tennessee_US 5827.0 124.0\n",
      "Texas_US 15006.0 342.0\n",
      "Utah_US 2417.0 18.0\n",
      "Vermont_US 752.0 29.0\n",
      "Virgin Islands_US 51.0 1.0\n",
      "Virginia_US 6182.0 154.0\n",
      "Washington_US 10799.0 530.0\n",
      "West Virginia_US 640.0 9.0\n",
      "Wisconsin_US 3555.0 170.0\n",
      "Wyoming_US 282.0 1.0\n",
      "nan_Uganda 55.0 0.0\n",
      "nan_Ukraine 3372.0 98.0\n",
      "nan_United Arab Emirates 4933.0 28.0\n",
      "Anguilla_United Kingdom 3.0 0.0\n",
      "Bermuda_United Kingdom 57.0 5.0\n",
      "British Virgin Islands_United Kingdom 3.0 0.0\n",
      "Cayman Islands_United Kingdom 54.0 1.0\n",
      "Channel Islands_United Kingdom 440.0 13.0\n",
      "Falkland Islands (Malvinas)_United Kingdom 11.0 0.0\n",
      "Gibraltar_United Kingdom 129.0 0.0\n",
      "Isle of Man_United Kingdom 254.0 2.0\n",
      "Montserrat_United Kingdom 11.0 0.0\n",
      "Turks and Caicos Islands_United Kingdom 10.0 1.0\n",
      "nan_United Kingdom 93873.0 12107.0\n",
      "nan_Uruguay 494.0 8.0\n",
      "nan_Uzbekistan 1165.0 4.0\n",
      "nan_Venezuela 189.0 9.0\n",
      "nan_Vietnam 266.0 0.0\n",
      "nan_West Bank and Gaza 308.0 2.0\n",
      "nan_Western Sahara 6.0 0.0\n",
      "nan_Zambia 45.0 2.0\n",
      "nan_Zimbabwe 17.0 3.0\n",
      " modelling confirmed, case 0, , original cv 313 and after 313 \n",
      " modelling confirmed, case 1, , original cv 313 and after 313 \n",
      " modelling confirmed, case 2, , original cv 313 and after 313 \n",
      " modelling confirmed, case 3, , original cv 313 and after 313 \n",
      " modelling confirmed, case 4, , original cv 313 and after 313 \n",
      " modelling confirmed, case 5, , original cv 313 and after 313 \n",
      " modelling confirmed, case 6, , original cv 313 and after 313 \n",
      " modelling confirmed, case 7, , original cv 313 and after 313 \n",
      " modelling confirmed, case 8, , original cv 313 and after 313 \n",
      " modelling confirmed, case 9, , original cv 313 and after 313 \n",
      " modelling confirmed, case 10, , original cv 313 and after 313 \n",
      " modelling confirmed, case 11, , original cv 313 and after 313 \n",
      " modelling confirmed, case 12, , original cv 313 and after 313 \n",
      " modelling confirmed, case 13, , original cv 313 and after 313 \n",
      " modelling confirmed, case 14, , original cv 313 and after 313 \n",
      " modelling confirmed, case 15, , original cv 313 and after 313 \n",
      " modelling confirmed, case 16, , original cv 313 and after 313 \n",
      " modelling confirmed, case 17, , original cv 313 and after 313 \n",
      " modelling confirmed, case 18, , original cv 313 and after 313 \n",
      " modelling confirmed, case 19, , original cv 313 and after 313 \n",
      " modelling confirmed, case 20, , original cv 313 and after 313 \n",
      " modelling confirmed, case 21, , original cv 313 and after 313 \n",
      " modelling confirmed, case 22, , original cv 313 and after 313 \n",
      " modelling confirmed, case 23, , original cv 313 and after 313 \n",
      " modelling confirmed, case 24, , original cv 313 and after 313 \n",
      " modelling confirmed, case 25, , original cv 313 and after 313 \n",
      " modelling confirmed, case 26, , original cv 313 and after 313 \n",
      " modelling confirmed, case 27, , original cv 313 and after 313 \n",
      " modelling confirmed, case 28, , original cv 313 and after 313 \n",
      " modelling confirmed, case 29, , original cv 313 and after 313 \n",
      " modelling fatalities, case 0, original cv 313 and after 313 \n",
      " modelling fatalities, case 1, original cv 313 and after 313 \n",
      " modelling fatalities, case 2, original cv 313 and after 313 \n",
      " modelling fatalities, case 3, original cv 313 and after 313 \n",
      " modelling fatalities, case 4, original cv 313 and after 313 \n",
      " modelling fatalities, case 5, original cv 313 and after 313 \n",
      " modelling fatalities, case 6, original cv 313 and after 313 \n",
      " modelling fatalities, case 7, original cv 313 and after 313 \n",
      " modelling fatalities, case 8, original cv 313 and after 313 \n",
      " modelling fatalities, case 9, original cv 313 and after 313 \n",
      " modelling fatalities, case 10, original cv 313 and after 313 \n",
      " modelling fatalities, case 11, original cv 313 and after 313 \n",
      " modelling fatalities, case 12, original cv 313 and after 313 \n",
      " modelling fatalities, case 13, original cv 313 and after 313 \n",
      " modelling fatalities, case 14, original cv 313 and after 313 \n",
      " modelling fatalities, case 15, original cv 313 and after 313 \n",
      " modelling fatalities, case 16, original cv 313 and after 313 \n",
      " modelling fatalities, case 17, original cv 313 and after 313 \n",
      " modelling fatalities, case 18, original cv 313 and after 313 \n",
      " modelling fatalities, case 19, original cv 313 and after 313 \n",
      " modelling fatalities, case 20, original cv 313 and after 313 \n",
      " modelling fatalities, case 21, original cv 313 and after 313 \n",
      " modelling fatalities, case 22, original cv 313 and after 313 \n",
      " modelling fatalities, case 23, original cv 313 and after 313 \n",
      " modelling fatalities, case 24, original cv 313 and after 313 \n",
      " modelling fatalities, case 25, original cv 313 and after 313 \n",
      " modelling fatalities, case 26, original cv 313 and after 313 \n",
      " modelling fatalities, case 27, original cv 313 and after 313 \n",
      " modelling fatalities, case 28, original cv 313 and after 313 \n",
      " modelling fatalities, case 29, original cv 313 and after 313 \n"
     ]
    }
   ],
   "source": [
    "########################################## THIS IS THE MOST IMPORTANT PART OF MY CODE - THESE RULES CONTROL TH DECAY OF THE RATHER BAD LIGHTGBM MODELS#########################\n",
    "########################## The inference is also fone here ############################\n",
    "\n",
    "def decay_4_first_10_then_1_f(array):\n",
    "    arr=[1.0 for k in range(len(array))]\n",
    "    for j in range(len(array)):\n",
    "        if j<10:\n",
    "            arr[j]=1. + (max(1,array[j])-1.)/4.\n",
    "        else :\n",
    "            arr[j]=1.\n",
    "    return arr\n",
    "\t\n",
    "def decay_16_first_10_then_1_f(array):\n",
    "    arr=[1.0 for k in range(len(array))]\n",
    "    for j in range(len(array)):\n",
    "        if j<10:\n",
    "            arr[j]=1. + (max(1,array[j])-1.)/16.\n",
    "        else :\n",
    "            arr[j]=1.\n",
    "    return arr\t\n",
    "            \n",
    "def decay_2_f(array):\n",
    "    arr=[1.0 for k in range(len(array))]    \n",
    "    for j in range(len(array)):\n",
    "            arr[j]=1. + (max(1,array[j])-1.)/2.\n",
    "    return arr \n",
    "\n",
    "def decay_4_f(array):\n",
    "    arr=[1.0 for k in range(len(array))]    \n",
    "    for j in range(len(array)):\n",
    "            arr[j]=1. + (max(1,array[j])-1.)/4.\n",
    "    return arr \t\n",
    "\t\n",
    "def acceleratorx2_f(array):\n",
    "    arr=[1.0 for k in range(len(array))]    \n",
    "    for j in range(len(array)):\n",
    "            arr[j]=1. + (max(1,array[j])-1.)*2.\n",
    "    return arr \n",
    "\n",
    "\n",
    "\n",
    "def decay_1_5_f(array):\n",
    "    arr=[1.0 for k in range(len(array))]    \n",
    "    for j in range(len(array)):\n",
    "            arr[j]=1. + (max(1,array[j])-1.)/1.5\n",
    "    return arr            \n",
    "\n",
    "def decay_1_2_f(array):\n",
    "    arr=[1.0 for k in range(len(array))]    \n",
    "    for j in range(len(array)):\n",
    "            arr[j]=1. + (max(1,array[j])-1.)/1.2\n",
    "    return arr            \n",
    "  \n",
    "\n",
    "def decay_1_1_f(array):\n",
    "    arr=[1.0 for k in range(len(array))]    \n",
    "    for j in range(len(array)):\n",
    "            arr[j]=1. + (max(1,array[j])-1.)/1.1\n",
    "    return arr            \n",
    "      \n",
    "    \n",
    "    \n",
    "    \n",
    "         \n",
    "def stay_same_f(array):\n",
    "    arr=[1.0 for k in range(len(array))]      \n",
    "    for j in range(len(array)):\n",
    "        arr[j]=1.\n",
    "    return arr   \n",
    "\n",
    "def decay_2_last_12_linear_inter_f(array):\n",
    "    arr=[1.0 for k in range(len(array))]\n",
    "    for j in range(len(array)):\n",
    "        arr[j]=1. + (max(1,array[j])-1.)/2.\n",
    "    arr12= (max(1,arr[-12])-1.)/12. \n",
    "\n",
    "    for j in range(0, 12):\n",
    "        arr[len(arr)-12 +j]= max(1, 1 + ( (arr12*12) - (j+1)*arr12 ))\n",
    "    return arr\n",
    "\n",
    "def decay_1_2_last_12_linear_inter_f(array):\n",
    "    arr=[1.0 for k in range(len(array))]\n",
    "    for j in range(len(array)):\n",
    "        arr[j]=1. + (max(1,array[j])-1.)/1.2\n",
    "    arr12= (max(1,arr[-12])-1.)/12. \n",
    "\n",
    "    for j in range(0, 12):\n",
    "        arr[len(arr)-12 +j]= max(1, 1 + ( (arr12*12) - (j+1)*arr12 ))\n",
    "    return arr\n",
    "\n",
    "def decay_1_5_last_12_linear_inter_f(array):\n",
    "    arr=[1.0 for k in range(len(array))]\n",
    "    for j in range(len(array)):\n",
    "        arr[j]=1. + (max(1,array[j])-1.)/1.5\n",
    "    arr12= (max(1,arr[-12])-1.)/12. \n",
    "\n",
    "    for j in range(0, 12):\n",
    "        arr[len(arr)-12 +j]= max(1, 1 + ( (arr12*12) - (j+1)*arr12 ))\n",
    "    return arr\n",
    "\n",
    "\n",
    "\n",
    "def decay_4_last_12_linear_inter_f(array):\n",
    "    arr=[1.0 for k in range(len(array))]\n",
    "    for j in range(len(array)):\n",
    "        arr[j]=1. + (max(1,array[j])-1.)/4.\n",
    "    arr12= (max(1,arr[-12])-1.)/12. \n",
    "\n",
    "    for j in range(0, 12):\n",
    "        arr[len(arr)-12 +j]= max(1, 1 + ( (arr12*12) - (j+1)*arr12 ))\n",
    "    return arr\n",
    "\n",
    "def decay_8_last_12_linear_inter_f(array):\n",
    "    arr=[1.0 for k in range(len(array))]\n",
    "    for j in range(len(array)):\n",
    "        arr[j]=1. + (max(1,array[j])-1.)/8.\n",
    "    arr12= (max(1,arr[-12])-1.)/12. \n",
    "\n",
    "    for j in range(0, 12):\n",
    "        arr[len(arr)-12 +j]= max(1, 1 + ( (arr12*12) - (j+1)*arr12 ))\n",
    "    return arr\n",
    "\n",
    "\n",
    "\n",
    "def linear_last_12_f(array):\n",
    "    arr=[1.0 for k in range(len(array))]\n",
    "    for j in range(len(array)):\n",
    "        arr[j]=max(1,array[j])\n",
    "    arr12= (max(1,arr[-12])-1.)/12. \n",
    "    \n",
    "    for j in range(0, 12):\n",
    "        arr[len(arr)-12 +j]= max(1, 1 + ( (arr12*12) - (j+1)*arr12 ))\n",
    "    return arr\n",
    "\n",
    "\n",
    "    \n",
    "decay_4_first_10_then_1 =[ \"Jilin_China\",\"Tianjin_China\"]#, \"Hong Kong_China\"\n",
    "decay_4_first_10_then_1_fatality=[]\n",
    "\n",
    "decay_16_first_10_then_1 =[\"Beijing_China\",\"Fujian_China\",\"Jiangsu_China\"]\n",
    "decay_16_first_10_then_1_fatality=[]\n",
    "\n",
    "decay_4=[\"nan_Central African Republic\",\"Shanghai_China\",\"nan_Korea, South\",\"nan_Zambia\"]\n",
    "decay_4_fatality=[\"Heilongjiang_China\",\"Inner Mongolia_China\"]\n",
    "\n",
    "decay_2 =[\"nan_Burundi\",\"Faroe Islands_Denmark\",\"nan_Eritrea\",\"nan_Italy\",\"nan_Brunei\"\"Heilongjiang_China\",\"Inner Mongolia_China\",\n",
    "\"Shanxi_China\",\"nan_Congo (Kinshasa)\",\"nan_Cyprus\",\"Reunion_France\",\"Sint Maarten_Netherlands\",\"nan_Angola\"]\n",
    "decay_2_fatality=[\"nan_Iran\"]\n",
    "\n",
    "stay_same=[\"nan_Diamond Princess\"]\n",
    "stay_same_fatality=[\"Beijing_China\",\"Fujian_China\",\"Qinghai_China\",\"Jiangsu_China\",\"Jilin_China\",\"Macau_China\",\"Ningxia_China\",\"Tianjin_China\"\n",
    ",\"Faroe Islands_Denmark\",\"Shanxi_China\"]#\n",
    "\n",
    "normal=[]\n",
    "normal_fatality=[\"nan_Andorra\",\"New South Wales_Australia\",\"nan_Korea, South\",\"New York_US\",\"nan_Austria\",\"Alberta_Canada\",\"Nova Scotia_Canada\",\n",
    "\"Saskatchewan_Canada\",\"nan_Costa Rica\",\"nan_Croatia\",\"nan_Finland\",\"Mayotte_France\",\"nan_Hungary\",\"nan_Iceland\",\"nan_Latvia\",\"nan_Czechia\"\n",
    ",\"nan_Luxembourg\",\"nan_Malta\",\"nan_Montenegro\",\"nan_New Zealand\",\"nan_Norway\",\"nan_South Africa\",\"Hawaii_US\",\"Ohio_US\",\"Pennsylvania_US\",\"nan_Uruguay\",\"nan_Venezuela\",\n",
    "\"nan_West Bank and Gaza\"]\n",
    "\n",
    "decay_4_last_12_linear_inter =[ \"nan_Dominica\",\"New Caledonia_France\",\n",
    "\"Saint Barthelemy_France\",\"nan_Gambia\",\"nan_Grenada\",\"nan_Holy See\",\n",
    "\"nan_Mauritania\",\"nan_Nicaragua\",\"nan_Saint Lucia\",\"nan_Saint Vincent and the Grenadines\",\n",
    "\"nan_Seychelles\",\"nan_Suriname\",\"nan_Mongolia\",\"nan_Cabo Verde\",\n",
    "\"Montserrat_United Kingdom\",\"Turks and Caicos Islands_United Kingdom\", \"Hong Kong_China\",\"Northern Territory_Australia\",\"Northwest Territories_Canada\",\"Yukon_Canada\",\n",
    "\"Guangdong_China\",\"nan_MS Zaandam\",\"nan_Maldives\",\"nan_Saint Kitts and Nevis\",\"nan_Sao Tome and Principe\",\"nan_South Sudan\",\"nan_Western Sahara\"]\n",
    "decay_4_last_12_linear_inter_fatality=[\"nan_Somalia\"]\n",
    "\n",
    "decay_2_last_12_linear_inter =[ \"nan_Chad\",\"French Polynesia_France\",\"nan_Laos\",\"nan_Nepal\",\"nan_Sudan\",\"Bermuda_United Kingdom\",\"Cayman Islands_United Kingdom\"\n",
    ",\"nan_Eswatini\",\"nan_Guyana\",\"nan_Liberia\",\"nan_Malawi\",\"nan_Mozambique\",\"nan_Timor-Leste\",\n",
    "                               \"Falkland Islands (Malvinas)_United Kingdom\",\"nan_Equatorial Guinea\"]\n",
    "decay_2_last_12_linear_inter_fatality=[]\n",
    "\n",
    "decay_1_5 =[\"nan_Cambodia\",\"nan_Croatia\",\"nan_Denmark\",\n",
    "\"nan_Bahamas\",\"nan_Bahrain\",\"nan_Barbados\" ,\"Manitoba_Canada\",\"New Brunswick_Canada\",\n",
    "\"Saskatchewan_Canada\",\"nan_France\",\"nan_Libya\",\"nan_Malta\",\n",
    "\t\"Aruba_Netherlands\",\"nan_Niger\",\"nan_Spain\",\"Virgin Islands_US\",\n",
    "\t\"Channel Islands_United Kingdom\",\"Gibraltar_United Kingdom\",\"nan_United Kingdom\",'nan_Burma',\"nan_Congo (Brazzaville)\",\n",
    "\t\"French Guiana_France\",\"Martinique_France\",\"Mayotte_France\",\"nan_Georgia\",\"nan_Iran\",\"nan_New Zealand\",\n",
    "\t\"nan_West Bank and Gaza\"]\n",
    "decay_1_5_fatality=[\"nan_Cuba\",\"nan_Morocco\",\"Ohio_US\",\"Pennsylvania_US\",\"Puerto Rico_US\" ,\"nan_Uzbekistan\",\n",
    "                    \"nan_Cyprus\",\"nan_France\",\"nan_Niger\",\"South Dakota_US\",\"nan_Gabon\"]\n",
    "\n",
    "acceleratorx2=[]\n",
    "acceleratorx2_fatality=[\"Newfoundland and Labrador_Canada\"]\n",
    "\n",
    "decay_1_5_last_12_linear_inter =[\"nan_Antigua and Barbuda\",\"nan_Botswana\",\"nan_Haiti\",\"nan_Madagascar\",\"nan_Somalia\",\n",
    "\"nan_Zimbabwe\"]\n",
    "decay_1_5_last_12_linear_inter_fatality =[\"nan_Burma\",\"nan_Cabo Verde\",\"nan_Djibouti\"]\n",
    "\n",
    "decay_1_2 =[\"nan_Albania\",\"nan_Andorra\",\"New South Wales_Australia\",\"nan_Austria\",\"nan_Azerbaijan\",\"nan_Bangladesh\",\"nan_Belize\",\"Alberta_Canada\",\"nan_Mauritius\",\"nan_Monaco\",\"nan_Montenegro\",\n",
    "\"Newfoundland and Labrador_Canada\",\"nan_Costa Rica\",\"nan_Czechia\",\"nan_Dominican Republic\",\"nan_Estonia\",\"Guadeloupe_France\",\"nan_Iceland\",\"nan_Latvia\",\"nan_Liechtenstein\",\"nan_Luxembourg\",\"nan_Norway\",\n",
    "\"nan_Poland\",\"nan_Rwanda\",\"nan_South Africa\",\"nan_Trinidad and Tobago\",\"Arizona_US\",\"California_US\",\"Colorado_US\",\"Connecticut_US\",\"Georgia_US\",\"Hawaii_US\",\"Maine_US\",\"New York_US\",\"Oklahoma_US\",\n",
    "\"Oregon_US\",\"Pennsylvania_US\",\"Tennessee_US\",\"Washington_US\",\"nan_Uruguay\",\"nan_Venezuela\",\"nan_Vietnam\",\n",
    "            \"nan_Gabon\",\"nan_Syria\",\"nan_Djibouti\"]\n",
    "\n",
    "decay_1_2_fatality=[\"nan_Afghanistan\",\"nan_Belarus\",\"nan_Mali\",\"nan_Mexico\",\"nan_Peru\",\"nan_Serbia\",\"Kentucky_US\",\"Puerto Rico_US\",\"Texas_US\",\"nan_Ukraine\"]\n",
    "\n",
    "decay_1_1 =[\"nan_Algeria\",\"nan_Argentina\",\"Tasmania_Australia\",\"Nova Scotia_Canada\",\"nan_Finland\",\"nan_Hungary\",\"nan_Iraq\",\n",
    "\"nan_Jordan\",\"nan_Kyrgyzstan\",\"nan_Malaysia\",\"Alabama_US\",\"Iowa_US\",\"Michigan_US\",\"Missouri_US\",\"Nebraska_US\",\"North Dakota_US \",\"Ohio_US\",\"South Carolina_US\"]\n",
    "decay_1_1_fatality=[\"nan_Ecuador\",\"nan_Kenya\",\"nan_Korea, South\",\"nan_Kosovo\",\"Indiana_US\",\"New Mexico_US\",\"Oregon_US\",\"Rhode Island_US\",\"Tennessee_US\",\"nan_United Arab Emirates\"]\n",
    " \n",
    "decay_1_2_last_12_linear_inter =[]\n",
    "decay_1_2_last_12_linear_inter_fatality=[\"nan_Angola\" ]\n",
    "\n",
    "linear_last_12=[\"nan_Afghanistan\",\"nan_Ireland\",\"nan_Benin\",\"nan_Tanzania\"]\n",
    "linear_last_12_fatality=[\"nan_Guatemala\",\"nan_Ireland\"]\n",
    "\n",
    "decay_8_last_12_linear_inter =[ \"nan_Bhutan\",\"Prince Edward Island_Canada\",\"Greenland_Denmark\",\"nan_Fiji\",\"Saint Pierre and Miquelon_France\",\"St Martin_France\",\"nan_Namibia\",\n",
    "\"Bonaire, Sint Eustatius and Saba_Netherlands\",\"Curacao_Netherlands\",\"nan_Papua New Guinea\",\"Anguilla_United Kingdom\",\"British Virgin Islands_United Kingdom\",]\n",
    "\n",
    "decay_8_last_12_linear_inter_fatality=[]\n",
    "\n",
    "tr_frame=train_frame\n",
    "\n",
    "features_train=tr_frame[names].values   \n",
    "\n",
    "standard_confirmed_train=tr_frame[\"ConfirmedCases\"].values\n",
    "standard_fatalities_train=tr_frame[\"Fatalities\"].values\n",
    "current_confirmed_train=tr_frame[\"ConfirmedCases\"].values\n",
    "\n",
    "     \n",
    "\n",
    "features_cv=[]\n",
    "name_cv=[]\n",
    "standard_confirmed_cv=[]\n",
    "standard_fatalities_cv=[]\n",
    "names_=tr_frame[\"key\"].values\n",
    "training_horizon=int(features_train.shape[0]/len(unique_keys)) \n",
    "print(\"training horizon = \",training_horizon)\n",
    "for dd in range(training_horizon-1,features_train.shape[0],training_horizon):\n",
    "    features_cv.append(features_train[dd])\n",
    "    name_cv.append(names_[dd])\n",
    "    standard_confirmed_cv.append(standard_confirmed_train[dd])\n",
    "    standard_fatalities_cv.append(standard_fatalities_train[dd])\n",
    "    print (name_cv[-1], standard_confirmed_cv[-1], standard_fatalities_cv[-1])\n",
    "    \n",
    " \n",
    "\n",
    "features_cv=np.array(features_cv)\n",
    "preds_confirmed_cv=np.zeros((features_cv.shape[0],horizon))\n",
    "preds_confirmed_standard_cv=np.zeros((features_cv.shape[0],horizon))\n",
    "\n",
    "preds_fatalities_cv=np.zeros((features_cv.shape[0],horizon))\n",
    "preds_fatalities_standard_cv=np.zeros((features_cv.shape[0],horizon))\n",
    "\n",
    "overal_rmsle_metric_confirmed=0.0\n",
    "\n",
    "for j in range (preds_confirmed_cv.shape[1]):\n",
    "\n",
    "    this_features_cv=features_cv                          \n",
    "\n",
    "    preds=predict(features_cv, input_name=model_directory +\"confirmed\"+ str(j))\n",
    "    preds_confirmed_cv[:,j]=preds\n",
    "    print (\" modelling confirmed, case %d, , original cv %d and after %d \"%(j,this_features_cv.shape[0],this_features_cv.shape[0])) \n",
    "\n",
    "predictions=[] \n",
    "for ii in range (preds_confirmed_cv.shape[0]):\n",
    "    current_prediction=standard_confirmed_cv[ii]\n",
    "    if current_prediction==0 :\n",
    "        current_prediction=0.1   \n",
    "    this_preds=preds_confirmed_cv[ii].tolist()\n",
    "    name=name_cv[ii]\n",
    "    reserve=this_preds[0]\n",
    "    #overrides\n",
    "\n",
    "\n",
    "    if name in normal:\n",
    "        this_preds=this_preds\t\n",
    "\t\n",
    "    elif name in decay_4_first_10_then_1:\n",
    "        this_preds=decay_4_first_10_then_1_f(this_preds)\n",
    "\t\t\n",
    "    elif name in decay_16_first_10_then_1:\n",
    "        this_preds=decay_16_first_10_then_1_f(this_preds)\t\t\n",
    "\t\t\n",
    "    elif name in decay_4_last_12_linear_inter:\n",
    "        this_preds=decay_4_last_12_linear_inter_f(this_preds)\t\t  \n",
    "        \n",
    "    elif name in decay_8_last_12_linear_inter:\n",
    "        this_preds=decay_8_last_12_linear_inter_f(this_preds)\t\t          \n",
    "\n",
    "          \n",
    "    elif name in decay_4:\n",
    "        this_preds=decay_4_f(this_preds)\n",
    "\n",
    "    \n",
    "    elif name in decay_2:\n",
    "        this_preds=decay_2_f(this_preds)\n",
    "        \n",
    "    elif name in decay_2_last_12_linear_inter:\n",
    "        this_preds=decay_2_last_12_linear_inter_f(this_preds)\n",
    "        \n",
    "    elif name in decay_1_5:\n",
    "        this_preds=decay_1_5_f(this_preds)  \n",
    "\n",
    "    elif name in decay_1_5_last_12_linear_inter:\n",
    "        this_preds=decay_1_5_last_12_linear_inter_f(this_preds)  \n",
    "                     \n",
    "    elif name in decay_1_2:\n",
    "        this_preds=decay_1_2_f(this_preds) \n",
    "\n",
    "    elif name in decay_1_2_last_12_linear_inter:\n",
    "        this_preds=decay_1_2_last_12_linear_inter_f(this_preds)         \n",
    "        \n",
    "    elif name in decay_1_1:\n",
    "        this_preds=decay_1_1_f(this_preds)            \n",
    "        \n",
    "    elif name in linear_last_12:\n",
    "        this_preds=linear_last_12_f(this_preds)\n",
    "        \n",
    "    elif name in acceleratorx2:\n",
    "        this_preds=acceleratorx2_f(this_preds)         \n",
    "\n",
    "        \n",
    "    elif name in stay_same or  \"China\" in name:\n",
    "        this_preds=stay_same_f(this_preds)      \n",
    "\n",
    "        \n",
    "        \n",
    "    for j in range (preds_confirmed_cv.shape[1]):\n",
    "                \n",
    "                if j==0 and \"nan_Congo (Brazzaville)\" in name:\n",
    "                     current_prediction=117.\n",
    "                        \n",
    "                if j==0 and \"nan_Cabo Verde\" in name:\n",
    "                      current_prediction=56.                   \n",
    "        \n",
    "                current_prediction*=max(1,this_preds[j])\n",
    "                preds_confirmed_standard_cv[ii][j]=current_prediction\n",
    "\n",
    "\n",
    "for j in range (preds_confirmed_cv.shape[1]):\n",
    "\n",
    "    this_features_cv=features_cv\n",
    "                             \n",
    "    preds=predict(features_cv, input_name=model_directory +\"fatal\"+ str(j))\n",
    "    preds_fatalities_cv[:,j]=preds\n",
    "    print (\" modelling fatalities, case %d, original cv %d and after %d \"%( j,this_features_cv.shape[0],this_features_cv.shape[0])) \n",
    "\n",
    "predictions=[]\n",
    "for ii in range (preds_fatalities_cv.shape[0]):\n",
    "    current_prediction=standard_fatalities_cv[ii]\n",
    "        \n",
    "    this_preds=preds_fatalities_cv[ii].tolist()\n",
    "    name=name_cv[ii]\n",
    "    reserve=this_preds[0]\n",
    "    #overrides\n",
    "   \n",
    "    ####fatality special\n",
    "\n",
    "    if name in normal_fatality:\n",
    "        this_preds=this_preds\t \t\n",
    "\t\n",
    "    elif name in decay_4_first_10_then_1_fatality:\n",
    "        this_preds=decay_4_first_10_then_1_f(this_preds) \n",
    "\t\t\n",
    "    elif name in decay_16_first_10_then_1_fatality:\n",
    "        this_preds=decay_16_first_10_then_1_f(this_preds)\n",
    "        \n",
    "    elif name in decay_4_last_12_linear_inter_fatality:\n",
    "        this_preds=decay_4_last_12_linear_inter_f(this_preds)\t\t      \n",
    "        \n",
    "    elif name in decay_8_last_12_linear_inter_fatality:\n",
    "        this_preds=decay_8_last_12_linear_inter_f(this_preds)\t\t            \n",
    "\n",
    "    elif name in decay_4_fatality:\n",
    "        this_preds=decay_4_f(this_preds)\t\t\n",
    "\t\t\n",
    "    elif name in decay_2_fatality:\n",
    "        this_preds=decay_2_f(this_preds) \n",
    "                \n",
    "\n",
    "    elif name in decay_2_last_12_linear_inter_fatality:\n",
    "        this_preds=decay_2_last_12_linear_inter_f(this_preds)\n",
    "        \n",
    "    elif name in decay_1_5_fatality:\n",
    "        this_preds=decay_1_5_f(this_preds)   \t\n",
    "        \n",
    "    elif name in decay_1_5_last_12_linear_inter_fatality:\n",
    "        this_preds=decay_1_5_last_12_linear_inter_f(this_preds)         \n",
    "        \n",
    "    elif name in decay_1_2_fatality:\n",
    "        this_preds=decay_1_2_f(this_preds) \n",
    "        \n",
    "    elif name in decay_1_2_last_12_linear_inter_fatality:\n",
    "        this_preds=decay_1_2_last_12_linear_inter_f(this_preds)             \n",
    "        \n",
    "    elif name in decay_1_1_fatality:\n",
    "        this_preds=decay_1_1_f(this_preds)            \n",
    " \n",
    "    elif name in linear_last_12_fatality:\n",
    "        this_preds=linear_last_12_f(this_preds) \n",
    "         \n",
    "    elif name in acceleratorx2_fatality:\n",
    "        this_preds=acceleratorx2_f(this_preds)\n",
    "        \n",
    "    elif name in stay_same_fatality:     \n",
    "        this_preds=stay_same_f(this_preds) \n",
    "        \n",
    "     \n",
    "    ####general   \n",
    "    elif name in normal:\n",
    "        this_preds=this_preds\t   \n",
    "\t\n",
    "    elif name in decay_4_first_10_then_1:\n",
    "        this_preds=decay_4_first_10_then_1_f(this_preds)\n",
    "\t\t\n",
    "    elif name in decay_16_first_10_then_1:\n",
    "        this_preds=decay_16_first_10_then_1_f(this_preds)\t\t\n",
    "        \n",
    "    elif name in decay_4_last_12_linear_inter:\n",
    "        this_preds=decay_4_last_12_linear_inter_f(this_preds)\t\t     \n",
    "        \n",
    "    elif name in decay_8_last_12_linear_inter:\n",
    "        this_preds=decay_8_last_12_linear_inter_f(this_preds)\t\t           \n",
    "\n",
    "    elif name in decay_4:\n",
    "        this_preds=decay_4_f(this_preds)        \n",
    "\t\t\n",
    "    elif name in decay_2:\n",
    "        this_preds=decay_2_f(this_preds)\n",
    "        \n",
    "    elif name in decay_2_last_12_linear_inter:\n",
    "        this_preds=decay_2_last_12_linear_inter_f(this_preds)\n",
    "        \n",
    "    elif name in decay_1_5:\n",
    "        this_preds=decay_1_5_f(this_preds) \n",
    "        \n",
    "    elif name in decay_1_5_last_12_linear_inter:\n",
    "        this_preds=decay_1_5_last_12_linear_inter_f(this_preds)         \n",
    "         \n",
    "    elif name in decay_1_2:\n",
    "        this_preds=decay_1_2_f(this_preds) \n",
    "        \n",
    "    elif name in decay_1_2_last_12_linear_inter:\n",
    "        this_preds=decay_1_2_last_12_linear_inter_f(this_preds)             \n",
    "        \n",
    "    elif name in decay_1_1:\n",
    "        this_preds=decay_1_1_f(this_preds)            \n",
    "        \n",
    "    elif name in linear_last_12:\n",
    "        this_preds=linear_last_12_f(this_preds) \n",
    "        \n",
    "    elif name in acceleratorx2:\n",
    "        this_preds=acceleratorx2_f(this_preds)                 \n",
    "        \n",
    "    elif name in stay_same or  \"China\" in name:\n",
    "        this_preds=stay_same_f(this_preds)         \n",
    "        \n",
    "  \n",
    "    for j in range (preds_fatalities_cv.shape[1]):\n",
    "                if current_prediction==0 and  preds_confirmed_standard_cv[ii][j]>400 and (name not in stay_same and name not in stay_same_fatality and \"China\" not in name ):\n",
    "                    current_prediction=1.\n",
    "                if j==0 and \"Aruba_Netherlands\" in name:\n",
    "                     current_prediction=1.\n",
    "                if j==0 and \"nan_Monaco\" in name:\n",
    "                     current_prediction=3.\n",
    "                if j==0 and 'nan_Costa Rica' in name or \"Isle of Man_United Kingdom\" in name:\n",
    "                    current_prediction=4. \n",
    "                if j==0 and \"nan_Somalia\" in name:\n",
    "                    current_prediction=5\n",
    "                if j==0 and \"Martinique_France\" in name:\n",
    "                    current_prediction=8. \n",
    "                  \n",
    "                current_prediction*=max(1,this_preds[j])\n",
    "                preds_fatalities_standard_cv[ii][j]=current_prediction     \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313 313 313 313\n",
      "(313, 30) (313, 30) (313, 30) (313, 30)\n"
     ]
    }
   ],
   "source": [
    "# CREATE MAPPINGS \n",
    "\n",
    "key_to_confirmed_rate={}\n",
    "key_to_fatality_rate={}\n",
    "key_to_confirmed={}\n",
    "key_to_fatality={}\n",
    "print(len(features_cv), len(name_cv),len(standard_confirmed_cv),len(standard_fatalities_cv)) \n",
    "print(preds_confirmed_cv.shape,preds_confirmed_standard_cv.shape,preds_fatalities_cv.shape,preds_fatalities_standard_cv.shape) \n",
    "\n",
    "for j in range (len(name_cv)):\n",
    "    \n",
    "    key_to_confirmed_rate[name_cv[j]]=preds_confirmed_cv[j,:].tolist()\n",
    "    #print(key_to_confirmed_rate[name_cv[j]])\n",
    "    key_to_fatality_rate[name_cv[j]]=preds_fatalities_cv[j,:].tolist()\n",
    "    key_to_confirmed[name_cv[j]]  =preds_confirmed_standard_cv[j,:].tolist()  \n",
    "    key_to_fatality[name_cv[j]]=preds_fatalities_standard_cv[j,:].tolist()  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ForecastId</th>\n",
       "      <th>Province_State</th>\n",
       "      <th>Country_Region</th>\n",
       "      <th>Date</th>\n",
       "      <th>key</th>\n",
       "      <th>ConfirmedCases</th>\n",
       "      <th>Fatalities</th>\n",
       "      <th>rate_ConfirmedCases</th>\n",
       "      <th>rate_Fatalities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-04-02</td>\n",
       "      <td>nan_Afghanistan</td>\n",
       "      <td>273.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.151899</td>\n",
       "      <td>1.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-04-03</td>\n",
       "      <td>nan_Afghanistan</td>\n",
       "      <td>281.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.029304</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-04-04</td>\n",
       "      <td>nan_Afghanistan</td>\n",
       "      <td>299.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.064057</td>\n",
       "      <td>1.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-04-05</td>\n",
       "      <td>nan_Afghanistan</td>\n",
       "      <td>349.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.167224</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-04-06</td>\n",
       "      <td>nan_Afghanistan</td>\n",
       "      <td>367.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.051576</td>\n",
       "      <td>1.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13454</th>\n",
       "      <td>13455</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2020-05-10</td>\n",
       "      <td>nan_Zimbabwe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13455</th>\n",
       "      <td>13456</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2020-05-11</td>\n",
       "      <td>nan_Zimbabwe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13456</th>\n",
       "      <td>13457</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2020-05-12</td>\n",
       "      <td>nan_Zimbabwe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13457</th>\n",
       "      <td>13458</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2020-05-13</td>\n",
       "      <td>nan_Zimbabwe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13458</th>\n",
       "      <td>13459</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2020-05-14</td>\n",
       "      <td>nan_Zimbabwe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13459 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ForecastId Province_State Country_Region       Date              key  \\\n",
       "0               1            NaN    Afghanistan 2020-04-02  nan_Afghanistan   \n",
       "1               2            NaN    Afghanistan 2020-04-03  nan_Afghanistan   \n",
       "2               3            NaN    Afghanistan 2020-04-04  nan_Afghanistan   \n",
       "3               4            NaN    Afghanistan 2020-04-05  nan_Afghanistan   \n",
       "4               5            NaN    Afghanistan 2020-04-06  nan_Afghanistan   \n",
       "...           ...            ...            ...        ...              ...   \n",
       "13454       13455            NaN       Zimbabwe 2020-05-10     nan_Zimbabwe   \n",
       "13455       13456            NaN       Zimbabwe 2020-05-11     nan_Zimbabwe   \n",
       "13456       13457            NaN       Zimbabwe 2020-05-12     nan_Zimbabwe   \n",
       "13457       13458            NaN       Zimbabwe 2020-05-13     nan_Zimbabwe   \n",
       "13458       13459            NaN       Zimbabwe 2020-05-14     nan_Zimbabwe   \n",
       "\n",
       "       ConfirmedCases  Fatalities  rate_ConfirmedCases  rate_Fatalities  \n",
       "0               273.0         6.0             1.151899         1.500000  \n",
       "1               281.0         6.0             1.029304         1.000000  \n",
       "2               299.0         7.0             1.064057         1.166667  \n",
       "3               349.0         7.0             1.167224         1.000000  \n",
       "4               367.0        11.0             1.051576         1.571429  \n",
       "...               ...         ...                  ...              ...  \n",
       "13454             NaN         NaN                  NaN              NaN  \n",
       "13455             NaN         NaN                  NaN              NaN  \n",
       "13456             NaN         NaN                  NaN              NaN  \n",
       "13457             NaN         NaN                  NaN              NaN  \n",
       "13458             NaN         NaN                  NaN              NaN  \n",
       "\n",
       "[13459 rows x 9 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_new=train[[\"Date\",\"ConfirmedCases\",\"Fatalities\",\"key\",\"rate_ConfirmedCases\",\"rate_Fatalities\"]]\n",
    "\n",
    "test_new=pd.merge(test,train_new, how=\"left\", left_on=[\"key\",\"Date\"], right_on=[\"key\",\"Date\"] ).reset_index(drop=True)\n",
    "test_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 13 30\n",
      "13459 13459 13 30 313\n",
      "13459 13459 13 30 313\n",
      "13459 13459 13 30 313\n",
      "13459 13459 13 30 313\n"
     ]
    }
   ],
   "source": [
    "def fillin_columns(frame,key_column, original_name, training_horizon, test_horizon, unique_values, key_to_values):\n",
    "    keys=frame[key_column].values\n",
    "    original_values=frame[original_name].values.tolist()\n",
    "    print(len(keys), len(original_values), training_horizon ,test_horizon,len(key_to_values))\n",
    "    \n",
    "    for j in range(unique_values):\n",
    "        current_index=(j * (training_horizon +test_horizon )) +training_horizon \n",
    "        current_key=keys[current_index]\n",
    "        values=key_to_values[current_key]\n",
    "        co=0\n",
    "        for g in range(current_index, current_index + test_horizon):\n",
    "            original_values[g]=values[co]\n",
    "            co+=1\n",
    "    \n",
    "    frame[original_name]=original_values\n",
    " \n",
    "\n",
    "all_days=int(test_new.shape[0]/len(unique_keys))\n",
    "\n",
    "tr_horizon=all_days-horizon\n",
    "print(all_days,tr_horizon, horizon )\n",
    "\n",
    "fillin_columns(test_new,\"key\", 'ConfirmedCases', tr_horizon, horizon, len(unique_keys), key_to_confirmed)    \n",
    "fillin_columns(test_new,\"key\", 'Fatalities', tr_horizon, horizon, len(unique_keys), key_to_fatality)   \n",
    "fillin_columns(test_new,\"key\", 'rate_ConfirmedCases', tr_horizon, horizon, len(unique_keys), key_to_confirmed_rate)   \n",
    "fillin_columns(test_new,\"key\", 'rate_Fatalities', tr_horizon, horizon, len(unique_keys), key_to_fatality_rate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef fillin_columns(frame,key_column, original_name, training_horizon, test_horizon, unique_values, key_to_values):\\n    keys=frame[key_column].values\\n    original_values=frame[original_name].values.tolist()\\n    print(len(keys), len(original_values), training_horizon ,test_horizon,len(key_to_values))\\n    \\n    for j in range(unique_values):\\n        current_index=(j * (training_horizon +test_horizon )) +training_horizon \\n        current_key=keys[current_index]\\n        values=key_to_values[current_key]\\n        co=0\\n        for g in range(current_index, current_index + test_horizon):\\n            original_values[g]=values[co]\\n            co+=1\\n    \\n    frame[original_name]=original_values\\n \\n\\nall_days=int(test_new.shape[0]/len(unique_keys))\\n\\ntr_horizon=all_days-horizon\\nprint(all_days,tr_horizon, horizon )\\n\\nfillin_columns(test_new,\"key\", \\'ConfirmedCases\\', tr_horizon, horizon, len(unique_keys), key_to_confirmed)    \\nfillin_columns(test_new,\"key\", \\'Fatalities\\', tr_horizon, horizon, len(unique_keys), key_to_fatality)   \\nsubmission=test_new[[\"ForecastId\",\"ConfirmedCases\",\"Fatalities\"]]\\n\\nsubmission.to_csv(\"submission.csv\", index=False)\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def fillin_columns(frame,key_column, original_name, training_horizon, test_horizon, unique_values, key_to_values):\n",
    "    keys=frame[key_column].values\n",
    "    original_values=frame[original_name].values.tolist()\n",
    "    print(len(keys), len(original_values), training_horizon ,test_horizon,len(key_to_values))\n",
    "    \n",
    "    for j in range(unique_values):\n",
    "        current_index=(j * (training_horizon +test_horizon )) +training_horizon \n",
    "        current_key=keys[current_index]\n",
    "        values=key_to_values[current_key]\n",
    "        co=0\n",
    "        for g in range(current_index, current_index + test_horizon):\n",
    "            original_values[g]=values[co]\n",
    "            co+=1\n",
    "    \n",
    "    frame[original_name]=original_values\n",
    " \n",
    "\n",
    "all_days=int(test_new.shape[0]/len(unique_keys))\n",
    "\n",
    "tr_horizon=all_days-horizon\n",
    "print(all_days,tr_horizon, horizon )\n",
    "\n",
    "fillin_columns(test_new,\"key\", 'ConfirmedCases', tr_horizon, horizon, len(unique_keys), key_to_confirmed)    \n",
    "fillin_columns(test_new,\"key\", 'Fatalities', tr_horizon, horizon, len(unique_keys), key_to_fatality)   \n",
    "submission=test_new[[\"ForecastId\",\"ConfirmedCases\",\"Fatalities\"]]\n",
    "\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################### model to use to model (log) counts#############################\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "def bagged_set_trainc(X_ts,y_cs,wts, seed, estimators,xtest, xt=None,yt=None, output_name=None):\n",
    "   #print (type(yt))\n",
    "   # create array object to hold predictions \n",
    "  \n",
    "   baggedpred=np.array([ 0.0 for d in range(0, xtest.shape[0])]) \n",
    "   #print (y_cs[:10])\n",
    "   #print (yt[:10])  \n",
    "\n",
    "   #loop for as many times as we want bags\n",
    "   for n in range (0, estimators):\n",
    "       \n",
    "       params = {'objective': 'rmse',\n",
    "                'metric': 'rmse',\n",
    "                'boosting': 'gbdt',\n",
    "                'learning_rate': 0.005, #change here    \n",
    "                'drop_rate':0.01,\n",
    "                #'alpha': 0.99, \n",
    "                'skip_drop':0.6,\n",
    "                'uniform_drop':True,               \n",
    "                'verbose': -1,    \n",
    "                'num_leaves': 30, # ~18    \n",
    "                'bagging_fraction': 0.9,    \n",
    "                'bagging_freq': 1,    \n",
    "                'bagging_seed': seed + n,    \n",
    "                'feature_fraction': 0.8,    \n",
    "                'feature_fraction_seed': seed + n,    \n",
    "                'min_data_in_leaf': 10, #30, #56, # 10-50    \n",
    "                'max_bin': 100, # maybe useful with overfit problem    \n",
    "                'max_depth':20,                   \n",
    "                #'reg_lambda': 10,    \n",
    "                'reg_alpha':1,    \n",
    "                'lambda_l2': 10,\n",
    "                #'categorical_feature':'2', # because training data is extremely unbalanced                     \n",
    "                'num_threads':6\n",
    "                }\n",
    "       d_train = lgb.Dataset(X_ts,y_cs, weight=wts, free_raw_data=False)#np.log1p(\n",
    "       if not type(yt) is type(None):           \n",
    "           d_cv = lgb.Dataset(xt,yt, free_raw_data=False, reference=d_train)#, reference=d_train\n",
    "           model = lgb.train(params,d_train,num_boost_round=500,\n",
    "                             valid_sets=d_cv,\n",
    "\n",
    "                             verbose_eval=50 ) #1000                        \n",
    "           \n",
    "       else :\n",
    "           #d_cv = lgb.Dataset(xt, free_raw_data=False, categorical_feature=\"2\")  \n",
    "           model = lgb.train(params,d_train,num_boost_round=500) #1000                              \n",
    "           #importances=model.feature_importance('gain')\n",
    "           #print(importances)\n",
    "       preds=model.predict(xtest)               \n",
    "       # update bag's array\n",
    "       baggedpred+=preds\n",
    "       #np.savetxt(\"preds_lgb\" + str(n)+ \".csv\",baggedpred)   \n",
    "       #if n%5==0:\n",
    "           #print(\"completed: \" + str(n)  )                 \n",
    "\n",
    "   if not output_name is None:\n",
    "        joblib.dump((model), output_name)\n",
    "   \"\"\"\n",
    "   model=Ridge(normalize=True, alpha=0.1, random_state=1)\n",
    "   model.fit(X_ts,y_cs)\n",
    "   preds=model.predict(xtest)\n",
    "   baggedpred+=preds\n",
    "   \"\"\"\n",
    "   # divide with number of bags to create an average estimate  \n",
    "   baggedpred/= estimators\n",
    "     \n",
    "   return baggedpred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################features for the count model ###########################\n",
    "names=[]\n",
    "#names=[\"lag_confirmed_rate\" + str(k+1) for k in range (size)]\n",
    "for day in days_back_confimed:\n",
    "    names+=[\"days_ago_confirmed_count_\" + str(day) ]\n",
    "for window in windows:        \n",
    "    names+=[\"ma\" + str(window) + \"_rate_confirmed\" + str(k+1) for k in range (size)]\n",
    "    #names+=[\"std\" + str(window) + \"_rate_confirmed\" + str(k+1) for k in range (size)] \n",
    "    #names+=[\"ewma\" + str(window) + \"_rate_confirmed\" + str(k+1) for k in range (size)]         \n",
    "    names+=[\"ma\" + str(window) + \"_count_confirmed\" + str(k+1) for k in range (size)]  \n",
    "\n",
    "#names+=[\"lag_fatalities_rate\" + str(k+1) for k in range (size)]\n",
    "for day in days_back_fatalities:\n",
    "    names+=[\"days_ago_fatalitiescount_\" + str(day) ]    \n",
    "for window in windows:        \n",
    "    names+=[\"ma\" + str(window) + \"_rate_fatalities\" + str(k+1) for k in range (size)]\n",
    "    #names+=[\"std\" + str(window) + \"_rate_fatalities\" + str(k+1) for k in range (size)]  \n",
    "    #names+=[\"ewma\" + str(window) + \"_rate_fatalities\" + str(k+1) for k in range (size)]    \n",
    "    names+=[\"ma\" + str(window) + \"_count_fatalities\" + str(k+1) for k in range (size)]    \n",
    "names+=[\"confirmed_level\"]\n",
    "names+=[\"fatalities_level\"]   \n",
    "if not extra_stable_columns is None and len(extra_stable_columns)>0: \n",
    "    names+=[k for k in extra_stable_columns]  \n",
    "    \n",
    "if not group_names is None:  \n",
    "     for gg in range (len(group_names)):\n",
    "         #names+=[\"lag_rate_group_\"+ str(gg+1) + \"_\" + str(k+1) for k in range (size_group)]    \n",
    "         for day in days_back_confimed_group:\n",
    "            names+=[\"days_ago_grooupcount_\" + str(gg+1) + \"_\" + str(day) ]             \n",
    "         for window in windows_group:        \n",
    "            names+=[\"ma_group_\" + str(gg+1) + \"_\" + str(window) + \"_rate_\" + str(k+1) for k in range (size_group)]\n",
    "            #names+=[\"std_group_\" + str(gg+1)+ \"_\" + str(window) + \"_rate_\" + str(k+1) for k in range (size_group)]  \n",
    "            #names+=[\"ewma_group_\" + str(gg+1) + \"_\" + str(window) + \"_rate_\" + str(k+1) for k in range (size)]      \n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntr_frame=train_frame\\nif not train_frame_supplamanteary2 is None:\\n    tr_frame=train_frame_supplamanteary2\\ntarget_confirmed=[\"confirmed_count_plus\" + str(k+1) for k in range (horizon)]    \\ntarget_fatalities=[\"fatalities_count_plus\" + str(k+1) for k in range (horizon)] \\nseed=1412\\n\\ntarget_confirmed_train=tr_frame[target_confirmed].values\\nprint (\"  original shape of train is {}  \".format( target_confirmed_train.shape) )\\n\\ntarget_fatalities_train=tr_frame[target_fatalities].values\\nfeatures_train=tr_frame[names].values   \\n\\nstandard_confirmed_train=tr_frame[\"ConfirmedCases\"].values\\nstandard_fatalities_train=tr_frame[\"Fatalities\"].values\\ncurrent_confirmed_train=tr_frame[\"ConfirmedCases\"].values\\n\\n     \\n\\nfeatures_cv=[]\\nname_cv=[]\\nstandard_confirmed_cv=[]\\nstandard_fatalities_cv=[]\\nnames_=tr_frame[\"key\"].values\\ntraining_horizon=int(features_train.shape[0]/len(unique_keys)) \\nprint(\"training horizon = \",training_horizon)\\nfor dd in range(training_horizon-1,features_train.shape[0],training_horizon):\\n    features_cv.append(features_train[dd])\\n    name_cv.append(names_[dd])\\n    standard_confirmed_cv.append(standard_confirmed_train[dd])\\n    standard_fatalities_cv.append(standard_fatalities_train[dd])\\n    print (name_cv[-1], standard_confirmed_cv[-1], standard_fatalities_cv[-1])\\n    \\n \\n    \\ncurrent_confirmed_train=[k for k in range(len(current_confirmed_train)) if current_confirmed_train[k]>0]\\ntarget_confirmed_train=target_confirmed_train[current_confirmed_train]\\ntarget_fatalities_train=target_fatalities_train[current_confirmed_train]        \\nfeatures_train=features_train[current_confirmed_train]         \\nstandard_confirmed_train=standard_confirmed_train[current_confirmed_train]\\nstandard_fatalities_train=standard_fatalities_train[current_confirmed_train]  \\n    \\nfeatures_cv=np.array(features_cv)\\npreds_confirmed_cv=np.zeros((features_cv.shape[0],horizon))\\npreds_confirmed_standard_cv=np.zeros((features_cv.shape[0],horizon))\\n\\npreds_fatalities_cv=np.zeros((features_cv.shape[0],horizon))\\npreds_fatalities_standard_cv=np.zeros((features_cv.shape[0],horizon))\\n\\noveral_rmsle_metric_confirmed=0.0\\n\\nfor j in range (preds_confirmed_cv.shape[1]):\\n    this_target=target_confirmed_train[:,j]\\n    index_positive=[k for k in range(len(this_target)) if this_target[k]!=-1]\\n    this_features=features_train[index_positive]\\n    this_target=this_target[index_positive]\\n    this_weight=np.log(standard_confirmed_train[index_positive]+3.)\\n    this_weight=[1. for k in range(len(this_weight))]\\n    this_features_cv=features_cv                          \\n\\n    preds=bagged_set_trainc(this_features,this_target,this_weight, seed, 1,features_cv, xt=None,yt=None, output_name=model_directory +\"confirmedc\"+ str(j))\\n    preds_confirmed_cv[:,j]=preds\\n    print (\" modelling count confirmed, case %d, original train %d, and after %d, original cv %d and after %d \"%(\\n    j,target_confirmed_train.shape[0],this_target.shape[0],this_features_cv.shape[0],this_features_cv.shape[0])) \\n\\npredictions=[]\\nfor ii in range (preds_confirmed_cv.shape[0]):\\n    current_prediction=standard_confirmed_cv[ii]  \\n    for j in range (preds_confirmed_cv.shape[1]):\\n                current_prediction+=np.expm1(max(0,preds_confirmed_cv[ii][j]))\\n                preds_confirmed_standard_cv[ii][j]=current_prediction\\n\\n\\n\\nfor j in range (preds_confirmed_cv.shape[1]):\\n    this_target=target_fatalities_train[:,j]\\n    index_positive=[k for k in range(len(this_target)) if this_target[k]!=-1]\\n    this_features=features_train[index_positive]\\n    this_target=this_target[index_positive]\\n    this_weight=np.log(standard_confirmed_train[index_positive]+2.)\\n    this_weight=[1. for k in range(len(this_weight))]\\n    \\n    this_features_cv=features_cv\\n                             \\n    preds=bagged_set_trainc(this_features,this_target,this_weight, seed, 1,features_cv, xt=None,yt=None, output_name=model_directory +\"fatalc\"+ str(j))\\n    preds_fatalities_cv[:,j]=preds\\n    print (\" modelling count fatalities, case %d, original train %d, and after %d, original cv %d and after %d \"%(\\n    j,target_confirmed_train.shape[0],this_target.shape[0],this_features_cv.shape[0],this_features_cv.shape[0])) \\n\\npredictions=[]\\nfor ii in range (preds_fatalities_cv.shape[0]):\\n    current_prediction=standard_fatalities_cv[ii]\\n    for j in range (preds_fatalities_cv.shape[1]):\\n\\n                current_prediction+=np.expm1(max(0,preds_fatalities_cv[ii][j]))\\n                preds_fatalities_standard_cv[ii][j]=current_prediction\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###############################THIS IS THE TRAINING PART OF MY CODE - ALL FILES ARE SAVED IN the model directory - you need to run it offline ####################\n",
    "#######################You need to uncomment this section to generate the lightgbm models one for each day+x log_count difference for both confirmed and fatalities ##########################\n",
    "#################Full model for counts ######################\n",
    "\"\"\"\n",
    "tr_frame=train_frame\n",
    "if not train_frame_supplamanteary2 is None:\n",
    "    tr_frame=train_frame_supplamanteary2\n",
    "target_confirmed=[\"confirmed_count_plus\" + str(k+1) for k in range (horizon)]    \n",
    "target_fatalities=[\"fatalities_count_plus\" + str(k+1) for k in range (horizon)] \n",
    "seed=1412\n",
    "\n",
    "target_confirmed_train=tr_frame[target_confirmed].values\n",
    "print (\"  original shape of train is {}  \".format( target_confirmed_train.shape) )\n",
    "\n",
    "target_fatalities_train=tr_frame[target_fatalities].values\n",
    "features_train=tr_frame[names].values   \n",
    "\n",
    "standard_confirmed_train=tr_frame[\"ConfirmedCases\"].values\n",
    "standard_fatalities_train=tr_frame[\"Fatalities\"].values\n",
    "current_confirmed_train=tr_frame[\"ConfirmedCases\"].values\n",
    "\n",
    "     \n",
    "\n",
    "features_cv=[]\n",
    "name_cv=[]\n",
    "standard_confirmed_cv=[]\n",
    "standard_fatalities_cv=[]\n",
    "names_=tr_frame[\"key\"].values\n",
    "training_horizon=int(features_train.shape[0]/len(unique_keys)) \n",
    "print(\"training horizon = \",training_horizon)\n",
    "for dd in range(training_horizon-1,features_train.shape[0],training_horizon):\n",
    "    features_cv.append(features_train[dd])\n",
    "    name_cv.append(names_[dd])\n",
    "    standard_confirmed_cv.append(standard_confirmed_train[dd])\n",
    "    standard_fatalities_cv.append(standard_fatalities_train[dd])\n",
    "    print (name_cv[-1], standard_confirmed_cv[-1], standard_fatalities_cv[-1])\n",
    "    \n",
    " \n",
    "    \n",
    "current_confirmed_train=[k for k in range(len(current_confirmed_train)) if current_confirmed_train[k]>0]\n",
    "target_confirmed_train=target_confirmed_train[current_confirmed_train]\n",
    "target_fatalities_train=target_fatalities_train[current_confirmed_train]        \n",
    "features_train=features_train[current_confirmed_train]         \n",
    "standard_confirmed_train=standard_confirmed_train[current_confirmed_train]\n",
    "standard_fatalities_train=standard_fatalities_train[current_confirmed_train]  \n",
    "    \n",
    "features_cv=np.array(features_cv)\n",
    "preds_confirmed_cv=np.zeros((features_cv.shape[0],horizon))\n",
    "preds_confirmed_standard_cv=np.zeros((features_cv.shape[0],horizon))\n",
    "\n",
    "preds_fatalities_cv=np.zeros((features_cv.shape[0],horizon))\n",
    "preds_fatalities_standard_cv=np.zeros((features_cv.shape[0],horizon))\n",
    "\n",
    "overal_rmsle_metric_confirmed=0.0\n",
    "\n",
    "for j in range (preds_confirmed_cv.shape[1]):\n",
    "    this_target=target_confirmed_train[:,j]\n",
    "    index_positive=[k for k in range(len(this_target)) if this_target[k]!=-1]\n",
    "    this_features=features_train[index_positive]\n",
    "    this_target=this_target[index_positive]\n",
    "    this_weight=np.log(standard_confirmed_train[index_positive]+3.)\n",
    "    this_weight=[1. for k in range(len(this_weight))]\n",
    "    this_features_cv=features_cv                          \n",
    "\n",
    "    preds=bagged_set_trainc(this_features,this_target,this_weight, seed, 1,features_cv, xt=None,yt=None, output_name=model_directory +\"confirmedc\"+ str(j))\n",
    "    preds_confirmed_cv[:,j]=preds\n",
    "    print (\" modelling count confirmed, case %d, original train %d, and after %d, original cv %d and after %d \"%(\n",
    "    j,target_confirmed_train.shape[0],this_target.shape[0],this_features_cv.shape[0],this_features_cv.shape[0])) \n",
    "\n",
    "predictions=[]\n",
    "for ii in range (preds_confirmed_cv.shape[0]):\n",
    "    current_prediction=standard_confirmed_cv[ii]  \n",
    "    for j in range (preds_confirmed_cv.shape[1]):\n",
    "                current_prediction+=np.expm1(max(0,preds_confirmed_cv[ii][j]))\n",
    "                preds_confirmed_standard_cv[ii][j]=current_prediction\n",
    "\n",
    "\n",
    "\n",
    "for j in range (preds_confirmed_cv.shape[1]):\n",
    "    this_target=target_fatalities_train[:,j]\n",
    "    index_positive=[k for k in range(len(this_target)) if this_target[k]!=-1]\n",
    "    this_features=features_train[index_positive]\n",
    "    this_target=this_target[index_positive]\n",
    "    this_weight=np.log(standard_confirmed_train[index_positive]+2.)\n",
    "    this_weight=[1. for k in range(len(this_weight))]\n",
    "    \n",
    "    this_features_cv=features_cv\n",
    "                             \n",
    "    preds=bagged_set_trainc(this_features,this_target,this_weight, seed, 1,features_cv, xt=None,yt=None, output_name=model_directory +\"fatalc\"+ str(j))\n",
    "    preds_fatalities_cv[:,j]=preds\n",
    "    print (\" modelling count fatalities, case %d, original train %d, and after %d, original cv %d and after %d \"%(\n",
    "    j,target_confirmed_train.shape[0],this_target.shape[0],this_features_cv.shape[0],this_features_cv.shape[0])) \n",
    "\n",
    "predictions=[]\n",
    "for ii in range (preds_fatalities_cv.shape[0]):\n",
    "    current_prediction=standard_fatalities_cv[ii]\n",
    "    for j in range (preds_fatalities_cv.shape[1]):\n",
    "\n",
    "                current_prediction+=np.expm1(max(0,preds_fatalities_cv[ii][j]))\n",
    "                preds_fatalities_standard_cv[ii][j]=current_prediction\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training horizon =  84\n",
      "nan_Afghanistan 714.0 23.0\n",
      "nan_Albania 475.0 24.0\n",
      "nan_Algeria 2070.0 326.0\n",
      "nan_Andorra 659.0 31.0\n",
      "nan_Angola 19.0 2.0\n",
      "nan_Antigua and Barbuda 23.0 2.0\n",
      "nan_Argentina 2277.0 102.0\n",
      "nan_Armenia 1067.0 16.0\n",
      "Australian Capital Territory_Australia 103.0 2.0\n",
      "New South Wales_Australia 2870.0 25.0\n",
      "Northern Territory_Australia 28.0 0.0\n",
      "Queensland_Australia 998.0 5.0\n",
      "South Australia_Australia 433.0 4.0\n",
      "Tasmania_Australia 165.0 6.0\n",
      "Victoria_Australia 1291.0 14.0\n",
      "Western Australia_Australia 527.0 6.0\n",
      "nan_Austria 14226.0 384.0\n",
      "nan_Azerbaijan 1197.0 13.0\n",
      "nan_Bahamas 49.0 8.0\n",
      "nan_Bahrain 1528.0 7.0\n",
      "nan_Bangladesh 1012.0 46.0\n",
      "nan_Barbados 72.0 4.0\n",
      "nan_Belarus 3281.0 33.0\n",
      "nan_Belgium 31119.0 4157.0\n",
      "nan_Belize 18.0 2.0\n",
      "nan_Benin 35.0 1.0\n",
      "nan_Bhutan 5.0 0.0\n",
      "nan_Bolivia 354.0 28.0\n",
      "nan_Bosnia and Herzegovina 1083.0 40.0\n",
      "nan_Botswana 13.0 1.0\n",
      "nan_Brazil 25262.0 1532.0\n",
      "nan_Brunei 136.0 1.0\n",
      "nan_Bulgaria 713.0 35.0\n",
      "nan_Burkina Faso 528.0 30.0\n",
      "nan_Burma 63.0 4.0\n",
      "nan_Burundi 5.0 1.0\n",
      "nan_Cabo Verde 11.0 1.0\n",
      "nan_Cambodia 122.0 0.0\n",
      "nan_Cameroon 848.0 14.0\n",
      "Alberta_Canada 1870.0 48.0\n",
      "British Columbia_Canada 1490.0 69.0\n",
      "Manitoba_Canada 246.0 4.0\n",
      "New Brunswick_Canada 116.0 0.0\n",
      "Newfoundland and Labrador_Canada 244.0 3.0\n",
      "Northwest Territories_Canada 5.0 0.0\n",
      "Nova Scotia_Canada 517.0 3.0\n",
      "Ontario_Canada 7953.0 334.0\n",
      "Prince Edward Island_Canada 25.0 1.0\n",
      "Quebec_Canada 14248.0 435.0\n",
      "Saskatchewan_Canada 300.0 4.0\n",
      "Yukon_Canada 8.0 0.0\n",
      "nan_Central African Republic 11.0 0.0\n",
      "nan_Chad 23.0 0.0\n",
      "nan_Chile 7917.0 92.0\n",
      "Anhui_China 991.0 6.0\n",
      "Beijing_China 589.0 8.0\n",
      "Chongqing_China 579.0 6.0\n",
      "Fujian_China 353.0 1.0\n",
      "Gansu_China 139.0 2.0\n",
      "Guangdong_China 1564.0 8.0\n",
      "Guangxi_China 254.0 2.0\n",
      "Guizhou_China 147.0 2.0\n",
      "Hainan_China 168.0 6.0\n",
      "Hebei_China 327.0 6.0\n",
      "Heilongjiang_China 819.0 13.0\n",
      "Henan_China 1276.0 22.0\n",
      "Hong Kong_China 1012.0 4.0\n",
      "Hubei_China 67803.0 3221.0\n",
      "Hunan_China 1019.0 4.0\n",
      "Inner Mongolia_China 190.0 1.0\n",
      "Jiangsu_China 653.0 0.0\n",
      "Jiangxi_China 937.0 1.0\n",
      "Jilin_China 100.0 1.0\n",
      "Liaoning_China 145.0 2.0\n",
      "Macau_China 45.0 0.0\n",
      "Ningxia_China 75.0 0.0\n",
      "Qinghai_China 18.0 0.0\n",
      "Shaanxi_China 256.0 3.0\n",
      "Shandong_China 784.0 7.0\n",
      "Shanghai_China 618.0 7.0\n",
      "Shanxi_China 173.0 0.0\n",
      "Sichuan_China 560.0 3.0\n",
      "Tianjin_China 185.0 3.0\n",
      "Tibet_China 1.0 0.0\n",
      "Xinjiang_China 76.0 3.0\n",
      "Yunnan_China 184.0 2.0\n",
      "Zhejiang_China 1267.0 1.0\n",
      "nan_Colombia 2979.0 127.0\n",
      "nan_Congo (Brazzaville) 60.0 5.0\n",
      "nan_Congo (Kinshasa) 241.0 20.0\n",
      "nan_Costa Rica 618.0 3.0\n",
      "nan_Cote d'Ivoire 638.0 6.0\n",
      "nan_Croatia 1704.0 31.0\n",
      "nan_Cuba 766.0 21.0\n",
      "nan_Cyprus 695.0 12.0\n",
      "nan_Czechia 6111.0 161.0\n",
      "Faroe Islands_Denmark 184.0 0.0\n",
      "Greenland_Denmark 11.0 0.0\n",
      "nan_Denmark 6511.0 299.0\n",
      "nan_Diamond Princess 712.0 12.0\n",
      "nan_Djibouti 363.0 2.0\n",
      "nan_Dominica 16.0 0.0\n",
      "nan_Dominican Republic 3286.0 183.0\n",
      "nan_Ecuador 7603.0 369.0\n",
      "nan_Egypt 2350.0 178.0\n",
      "nan_El Salvador 149.0 6.0\n",
      "nan_Equatorial Guinea 41.0 0.0\n",
      "nan_Eritrea 34.0 0.0\n",
      "nan_Estonia 1373.0 31.0\n",
      "nan_Eswatini 15.0 0.0\n",
      "nan_Ethiopia 82.0 3.0\n",
      "nan_Fiji 16.0 0.0\n",
      "nan_Finland 3161.0 64.0\n",
      "French Guiana_France 86.0 0.0\n",
      "French Polynesia_France 55.0 0.0\n",
      "Guadeloupe_France 145.0 8.0\n",
      "Martinique_France 157.0 6.0\n",
      "Mayotte_France 217.0 3.0\n",
      "New Caledonia_France 18.0 0.0\n",
      "Reunion_France 391.0 0.0\n",
      "Saint Barthelemy_France 6.0 0.0\n",
      "Saint Pierre and Miquelon_France 1.0 0.0\n",
      "St Martin_France 32.0 2.0\n",
      "nan_France 136779.0 15729.0\n",
      "nan_Gabon 57.0 1.0\n",
      "nan_Gambia 9.0 1.0\n",
      "nan_Georgia 300.0 3.0\n",
      "nan_Germany 131359.0 3294.0\n",
      "nan_Ghana 636.0 8.0\n",
      "nan_Greece 2170.0 101.0\n",
      "nan_Grenada 14.0 0.0\n",
      "nan_Guatemala 167.0 5.0\n",
      "nan_Guinea 363.0 0.0\n",
      "nan_Guinea-Bissau 38.0 0.0\n",
      "nan_Guyana 47.0 6.0\n",
      "nan_Haiti 40.0 3.0\n",
      "nan_Holy See 8.0 0.0\n",
      "nan_Honduras 407.0 26.0\n",
      "nan_Hungary 1512.0 122.0\n",
      "nan_Iceland 1720.0 8.0\n",
      "nan_India 11487.0 393.0\n",
      "nan_Indonesia 4839.0 459.0\n",
      "nan_Iran 74877.0 4683.0\n",
      "nan_Iraq 1400.0 78.0\n",
      "nan_Ireland 11479.0 406.0\n",
      "nan_Israel 12046.0 123.0\n",
      "nan_Italy 162488.0 21067.0\n",
      "nan_Jamaica 73.0 4.0\n",
      "nan_Japan 7645.0 143.0\n",
      "nan_Jordan 397.0 7.0\n",
      "nan_Kazakhstan 1232.0 14.0\n",
      "nan_Kenya 216.0 9.0\n",
      "nan_Korea, South 10564.0 222.0\n",
      "nan_Kosovo 387.0 8.0\n",
      "nan_Kuwait 1355.0 3.0\n",
      "nan_Kyrgyzstan 430.0 5.0\n",
      "nan_Laos 19.0 0.0\n",
      "nan_Latvia 657.0 5.0\n",
      "nan_Lebanon 641.0 21.0\n",
      "nan_Liberia 59.0 6.0\n",
      "nan_Libya 35.0 1.0\n",
      "nan_Liechtenstein 79.0 1.0\n",
      "nan_Lithuania 1070.0 29.0\n",
      "nan_Luxembourg 3307.0 69.0\n",
      "nan_MS Zaandam 9.0 2.0\n",
      "nan_Madagascar 108.0 0.0\n",
      "nan_Malawi 16.0 2.0\n",
      "nan_Malaysia 4987.0 82.0\n",
      "nan_Maldives 20.0 0.0\n",
      "nan_Mali 144.0 13.0\n",
      "nan_Malta 393.0 3.0\n",
      "nan_Mauritania 7.0 1.0\n",
      "nan_Mauritius 324.0 9.0\n",
      "nan_Mexico 5014.0 332.0\n",
      "nan_Moldova 1934.0 40.0\n",
      "nan_Monaco 93.0 1.0\n",
      "nan_Mongolia 30.0 0.0\n",
      "nan_Montenegro 283.0 4.0\n",
      "nan_Morocco 1888.0 126.0\n",
      "nan_Mozambique 28.0 0.0\n",
      "nan_Namibia 16.0 0.0\n",
      "nan_Nepal 16.0 0.0\n",
      "Aruba_Netherlands 92.0 0.0\n",
      "Bonaire, Sint Eustatius and Saba_Netherlands 3.0 0.0\n",
      "Curacao_Netherlands 14.0 1.0\n",
      "Sint Maarten_Netherlands 52.0 9.0\n",
      "nan_Netherlands 27419.0 2945.0\n",
      "nan_New Zealand 1366.0 9.0\n",
      "nan_Nicaragua 9.0 1.0\n",
      "nan_Niger 570.0 14.0\n",
      "nan_Nigeria 373.0 11.0\n",
      "nan_North Macedonia 908.0 44.0\n",
      "nan_Norway 6623.0 139.0\n",
      "nan_Oman 813.0 4.0\n",
      "nan_Pakistan 5837.0 96.0\n",
      "nan_Panama 3472.0 94.0\n",
      "nan_Papua New Guinea 2.0 0.0\n",
      "nan_Paraguay 159.0 7.0\n",
      "nan_Peru 10303.0 230.0\n",
      "nan_Philippines 5223.0 335.0\n",
      "nan_Poland 7202.0 263.0\n",
      "nan_Portugal 17448.0 567.0\n",
      "nan_Qatar 3428.0 7.0\n",
      "nan_Romania 6879.0 351.0\n",
      "nan_Russia 21102.0 170.0\n",
      "nan_Rwanda 134.0 0.0\n",
      "nan_Saint Kitts and Nevis 14.0 0.0\n",
      "nan_Saint Lucia 15.0 0.0\n",
      "nan_Saint Vincent and the Grenadines 12.0 0.0\n",
      "nan_San Marino 371.0 36.0\n",
      "nan_Sao Tome and Principe 4.0 0.0\n",
      "nan_Saudi Arabia 5369.0 73.0\n",
      "nan_Senegal 299.0 2.0\n",
      "nan_Serbia 4465.0 94.0\n",
      "nan_Seychelles 11.0 0.0\n",
      "nan_Sierra Leone 11.0 0.0\n",
      "nan_Singapore 3252.0 10.0\n",
      "nan_Slovakia 835.0 2.0\n",
      "nan_Slovenia 1220.0 56.0\n",
      "nan_Somalia 60.0 2.0\n",
      "nan_South Africa 2415.0 27.0\n",
      "nan_South Sudan 4.0 0.0\n",
      "nan_Spain 172541.0 18056.0\n",
      "nan_Sri Lanka 233.0 7.0\n",
      "nan_Sudan 32.0 5.0\n",
      "nan_Suriname 10.0 1.0\n",
      "nan_Sweden 11445.0 1033.0\n",
      "nan_Switzerland 25936.0 1174.0\n",
      "nan_Syria 29.0 2.0\n",
      "nan_Taiwan* 393.0 6.0\n",
      "nan_Tanzania 53.0 3.0\n",
      "nan_Thailand 2613.0 41.0\n",
      "nan_Timor-Leste 6.0 0.0\n",
      "nan_Togo 77.0 3.0\n",
      "nan_Trinidad and Tobago 113.0 8.0\n",
      "nan_Tunisia 747.0 34.0\n",
      "nan_Turkey 65111.0 1403.0\n",
      "Alabama_US 3953.0 114.0\n",
      "Alaska_US 285.0 9.0\n",
      "Arizona_US 3809.0 131.0\n",
      "Arkansas_US 1498.0 32.0\n",
      "California_US 25356.0 768.0\n",
      "Colorado_US 7946.0 327.0\n",
      "Connecticut_US 13989.0 671.0\n",
      "Delaware_US 1926.0 43.0\n",
      "District of Columbia_US 2058.0 67.0\n",
      "Florida_US 21628.0 571.0\n",
      "Georgia_US 14578.0 525.0\n",
      "Guam_US 133.0 5.0\n",
      "Hawaii_US 511.0 9.0\n",
      "Idaho_US 1464.0 33.0\n",
      "Illinois_US 23248.0 868.0\n",
      "Indiana_US 8527.0 387.0\n",
      "Iowa_US 1899.0 49.0\n",
      "Kansas_US 1441.0 69.0\n",
      "Kentucky_US 2048.0 113.0\n",
      "Louisiana_US 21518.0 1013.0\n",
      "Maine_US 735.0 20.0\n",
      "Maryland_US 8936.0 262.0\n",
      "Massachusetts_US 28164.0 844.0\n",
      "Michigan_US 27001.0 1768.0\n",
      "Minnesota_US 1695.0 79.0\n",
      "Mississippi_US 3087.0 111.0\n",
      "Missouri_US 4746.0 149.0\n",
      "Montana_US 399.0 7.0\n",
      "Nebraska_US 897.0 20.0\n",
      "Nevada_US 3134.0 126.0\n",
      "New Hampshire_US 985.0 25.0\n",
      "New Jersey_US 68824.0 2805.0\n",
      "New Mexico_US 1345.0 31.0\n",
      "New York_US 203020.0 10842.0\n",
      "North Carolina_US 5113.0 112.0\n",
      "North Dakota_US 341.0 8.0\n",
      "Ohio_US 7285.0 324.0\n",
      "Oklahoma_US 2184.0 108.0\n",
      "Oregon_US 1633.0 55.0\n",
      "Pennsylvania_US 25465.0 691.0\n",
      "Puerto Rico_US 923.0 45.0\n",
      "Rhode Island_US 3251.0 80.0\n",
      "South Carolina_US 3553.0 97.0\n",
      "South Dakota_US 988.0 6.0\n",
      "Tennessee_US 5827.0 124.0\n",
      "Texas_US 15006.0 342.0\n",
      "Utah_US 2417.0 18.0\n",
      "Vermont_US 752.0 29.0\n",
      "Virgin Islands_US 51.0 1.0\n",
      "Virginia_US 6182.0 154.0\n",
      "Washington_US 10799.0 530.0\n",
      "West Virginia_US 640.0 9.0\n",
      "Wisconsin_US 3555.0 170.0\n",
      "Wyoming_US 282.0 1.0\n",
      "nan_Uganda 55.0 0.0\n",
      "nan_Ukraine 3372.0 98.0\n",
      "nan_United Arab Emirates 4933.0 28.0\n",
      "Anguilla_United Kingdom 3.0 0.0\n",
      "Bermuda_United Kingdom 57.0 5.0\n",
      "British Virgin Islands_United Kingdom 3.0 0.0\n",
      "Cayman Islands_United Kingdom 54.0 1.0\n",
      "Channel Islands_United Kingdom 440.0 13.0\n",
      "Falkland Islands (Malvinas)_United Kingdom 11.0 0.0\n",
      "Gibraltar_United Kingdom 129.0 0.0\n",
      "Isle of Man_United Kingdom 254.0 2.0\n",
      "Montserrat_United Kingdom 11.0 0.0\n",
      "Turks and Caicos Islands_United Kingdom 10.0 1.0\n",
      "nan_United Kingdom 93873.0 12107.0\n",
      "nan_Uruguay 494.0 8.0\n",
      "nan_Uzbekistan 1165.0 4.0\n",
      "nan_Venezuela 189.0 9.0\n",
      "nan_Vietnam 266.0 0.0\n",
      "nan_West Bank and Gaza 308.0 2.0\n",
      "nan_Western Sahara 6.0 0.0\n",
      "nan_Zambia 45.0 2.0\n",
      "nan_Zimbabwe 17.0 3.0\n",
      " modelling confirmed, case 0, , original cv 313 and after 313 \n",
      " modelling confirmed, case 1, , original cv 313 and after 313 \n",
      " modelling confirmed, case 2, , original cv 313 and after 313 \n",
      " modelling confirmed, case 3, , original cv 313 and after 313 \n",
      " modelling confirmed, case 4, , original cv 313 and after 313 \n",
      " modelling confirmed, case 5, , original cv 313 and after 313 \n",
      " modelling confirmed, case 6, , original cv 313 and after 313 \n",
      " modelling confirmed, case 7, , original cv 313 and after 313 \n",
      " modelling confirmed, case 8, , original cv 313 and after 313 \n",
      " modelling confirmed, case 9, , original cv 313 and after 313 \n",
      " modelling confirmed, case 10, , original cv 313 and after 313 \n",
      " modelling confirmed, case 11, , original cv 313 and after 313 \n",
      " modelling confirmed, case 12, , original cv 313 and after 313 \n",
      " modelling confirmed, case 13, , original cv 313 and after 313 \n",
      " modelling confirmed, case 14, , original cv 313 and after 313 \n",
      " modelling confirmed, case 15, , original cv 313 and after 313 \n",
      " modelling confirmed, case 16, , original cv 313 and after 313 \n",
      " modelling confirmed, case 17, , original cv 313 and after 313 \n",
      " modelling confirmed, case 18, , original cv 313 and after 313 \n",
      " modelling confirmed, case 19, , original cv 313 and after 313 \n",
      " modelling confirmed, case 20, , original cv 313 and after 313 \n",
      " modelling confirmed, case 21, , original cv 313 and after 313 \n",
      " modelling confirmed, case 22, , original cv 313 and after 313 \n",
      " modelling confirmed, case 23, , original cv 313 and after 313 \n",
      " modelling confirmed, case 24, , original cv 313 and after 313 \n",
      " modelling confirmed, case 25, , original cv 313 and after 313 \n",
      " modelling confirmed, case 26, , original cv 313 and after 313 \n",
      " modelling confirmed, case 27, , original cv 313 and after 313 \n",
      " modelling confirmed, case 28, , original cv 313 and after 313 \n",
      " modelling confirmed, case 29, , original cv 313 and after 313 \n",
      " modelling fatalities, case 0, original cv 313 and after 313 \n",
      " modelling fatalities, case 1, original cv 313 and after 313 \n",
      " modelling fatalities, case 2, original cv 313 and after 313 \n",
      " modelling fatalities, case 3, original cv 313 and after 313 \n",
      " modelling fatalities, case 4, original cv 313 and after 313 \n",
      " modelling fatalities, case 5, original cv 313 and after 313 \n",
      " modelling fatalities, case 6, original cv 313 and after 313 \n",
      " modelling fatalities, case 7, original cv 313 and after 313 \n",
      " modelling fatalities, case 8, original cv 313 and after 313 \n",
      " modelling fatalities, case 9, original cv 313 and after 313 \n",
      " modelling fatalities, case 10, original cv 313 and after 313 \n",
      " modelling fatalities, case 11, original cv 313 and after 313 \n",
      " modelling fatalities, case 12, original cv 313 and after 313 \n",
      " modelling fatalities, case 13, original cv 313 and after 313 \n",
      " modelling fatalities, case 14, original cv 313 and after 313 \n",
      " modelling fatalities, case 15, original cv 313 and after 313 \n",
      " modelling fatalities, case 16, original cv 313 and after 313 \n",
      " modelling fatalities, case 17, original cv 313 and after 313 \n",
      " modelling fatalities, case 18, original cv 313 and after 313 \n",
      " modelling fatalities, case 19, original cv 313 and after 313 \n",
      " modelling fatalities, case 20, original cv 313 and after 313 \n",
      " modelling fatalities, case 21, original cv 313 and after 313 \n",
      " modelling fatalities, case 22, original cv 313 and after 313 \n",
      " modelling fatalities, case 23, original cv 313 and after 313 \n",
      " modelling fatalities, case 24, original cv 313 and after 313 \n",
      " modelling fatalities, case 25, original cv 313 and after 313 \n",
      " modelling fatalities, case 26, original cv 313 and after 313 \n",
      " modelling fatalities, case 27, original cv 313 and after 313 \n",
      " modelling fatalities, case 28, original cv 313 and after 313 \n",
      " modelling fatalities, case 29, original cv 313 and after 313 \n"
     ]
    }
   ],
   "source": [
    "############################################## Once again, thees rules are the important bit ################################\n",
    "\n",
    "\n",
    "def decay_4_first_10_then_1_f(array):\n",
    "    arr=[k for k in array]\n",
    "    for j in range(len(array)):\n",
    "        if j<10:\n",
    "            arr[j]*=1./4.\n",
    "        else :\n",
    "            arr[j]=0\n",
    "    return arr\n",
    "\n",
    "\t\n",
    "def decay_16_first_10_then_1_f(array):\n",
    "    arr=[k for k in array]\n",
    "    for j in range(len(array)):\n",
    "        if j<10:\n",
    "            arr[j]*=1./16.\n",
    "        else :\n",
    "            arr[j]=0\n",
    "    return arr\n",
    "            \n",
    "def decay_2_f(array):\n",
    "    arr=[k for k in array]    \n",
    "    for j in range(len(array)):\n",
    "            arr[j]*=1./2.\n",
    "    return arr \n",
    "\n",
    "def decay_4_f(array):\n",
    "    arr=[k for k in array]    \n",
    "    for j in range(len(array)):\n",
    "            arr[j]*=1./4.\n",
    "    return arr \n",
    "\t\n",
    "def acceleratorx2_f(array):\n",
    "    arr=[k for k in array]    \n",
    "    for j in range(len(array)):\n",
    "            arr[j]*=2.\n",
    "    return arr \n",
    "\n",
    "\n",
    "def decay_1_5_f(array):\n",
    "    arr=[k for k in array]    \n",
    "    for j in range(len(array)):\n",
    "            arr[j]*=1./1.5\n",
    "    return arr          \n",
    "\n",
    "def decay_1_2_f(array):\n",
    "    arr=[k for k in array]    \n",
    "    for j in range(len(array)):\n",
    "            arr[j]*=1./1.2\n",
    "    return arr   \n",
    "\n",
    "def decay_1_1_f(array):\n",
    "    arr=[k for k in array]    \n",
    "    for j in range(len(array)):\n",
    "            arr[j]*=1./1.1\n",
    "    return arr   \n",
    "         \n",
    "def stay_same_f(array):\n",
    "    arr=[0.0 for k in array]      \n",
    "    return arr   \n",
    "\n",
    "def decay_2_last_12_linear_inter_f(array):\n",
    "    arr=[k for k in array]\n",
    "    for j in range(len(array)):\n",
    "        arr[j]*=1/2.\n",
    "    arr12= arr[-12]\n",
    "\n",
    "    for j in range(0, 12):\n",
    "        arr[len(arr)-12 +j]= arr12/(j+1.)\n",
    "    return arr\n",
    "\n",
    "def decay_1_5_last_12_linear_inter_f(array):\n",
    "    arr=[k for k in array]\n",
    "    for j in range(len(array)):\n",
    "        arr[j]*=1/(1.5)\n",
    "    arr12= arr[-12]\n",
    "\n",
    "    for j in range(0, 12):\n",
    "        arr[len(arr)-12 +j]= arr12/(j+1.)\n",
    "    return arr\n",
    "\n",
    "\n",
    "\n",
    "def decay_1_2_last_12_linear_inter_f(array):\n",
    "    arr=[k for k in array]\n",
    "    for j in range(len(array)):\n",
    "        arr[j]*=1./(1.2)\n",
    "    arr12= arr[-12]\n",
    "\n",
    "    for j in range(0, 12):\n",
    "        arr[len(arr)-12 +j]= arr12/(j+1.)\n",
    "    return arr\n",
    "\n",
    "\n",
    "\n",
    "def decay_4_last_12_linear_inter_f(array):\n",
    "    arr=[k for k in array]\n",
    "    for j in range(len(array)):\n",
    "        arr[j]*=1/4.\n",
    "    arr12= arr[-12]\n",
    "\n",
    "    for j in range(0, 12):\n",
    "        arr[len(arr)-12 +j]= arr12/(j+1.)\n",
    "    return arr\n",
    "\n",
    "\n",
    "def decay_8_last_12_linear_inter_f(array):\n",
    "    arr=[k for k in array]\n",
    "    for j in range(len(array)):\n",
    "        arr[j]*=1/8.\n",
    "    arr12= arr[-12]\n",
    "\n",
    "    for j in range(0, 12):\n",
    "        arr[len(arr)-12 +j]= arr12/(j+1.)\n",
    "    return arr\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def linear_last_12_f(array):\n",
    "    arr=[k for k in array]\n",
    "    for j in range(len(array)):\n",
    "        arr[j]=array[j]\n",
    "    arr12=  arr[-12] \n",
    "    \n",
    "    for j in range(0, 12):\n",
    "        arr[len(arr)-12 +j]= arr12/(j+1.)\n",
    "    return arr\n",
    "    \n",
    "decay_4_first_10_then_1 =[ \"Jilin_China\",\"Tianjin_China\"]#, \"Hong Kong_China\"\n",
    "decay_4_first_10_then_1_fatality=[]\n",
    "\n",
    "decay_16_first_10_then_1 =[\"Beijing_China\",\"Fujian_China\",\"Jiangsu_China\"]\n",
    "decay_16_first_10_then_1_fatality=[]\n",
    "\n",
    "decay_4=[\"nan_Central African Republic\",\"Shanghai_China\",\"nan_Equatorial Guinea\",\"nan_Korea, South\",\"nan_Zambia\"]\n",
    "decay_4_fatality=[\"Heilongjiang_China\",\"Inner Mongolia_China\"]\n",
    "\n",
    "decay_2 =[\"nan_Burundi\",\"Faroe Islands_Denmark\",\"nan_Eritrea\",\"nan_Gabon\",\"nan_Italy\",\"nan_Brunei\"\"Heilongjiang_China\",\"Inner Mongolia_China\",\n",
    "\"Shanxi_China\",\"nan_Congo (Kinshasa)\",\"nan_Cyprus\",\"Reunion_France\",\"Sint Maarten_Netherlands\",\"nan_Angola\"]\n",
    "decay_2_fatality=[\"nan_Iran\"]\n",
    "\n",
    "stay_same=[\"nan_Diamond Princess\"]\n",
    "stay_same_fatality=[\"Beijing_China\",\"Fujian_China\",\"Qinghai_China\",\"Jiangsu_China\",\"Jilin_China\",\"Macau_China\",\"Ningxia_China\",\"Tianjin_China\"\n",
    ",\"Faroe Islands_Denmark\",\"Shanxi_China\"]#\n",
    "\n",
    "normal=[]\n",
    "normal_fatality=[\"nan_Andorra\",\"New South Wales_Australia\",\"nan_Korea, South\",\"New York_US\",\"nan_Austria\",\"Alberta_Canada\",\"Nova Scotia_Canada\",\n",
    "\"Saskatchewan_Canada\",\"nan_Costa Rica\",\"nan_Croatia\",\"nan_Finland\",\"Martinique_France\",\"Mayotte_France\",\"nan_Hungary\",\"nan_Iceland\",\"nan_Latvia\",\"nan_Czechia\"\n",
    ",\"nan_Luxembourg\",\"nan_Malta\",\"nan_Montenegro\",\"nan_New Zealand\",\"nan_Norway\",\"nan_South Africa\",\"Hawaii_US\",\"Ohio_US\",\"Pennsylvania_US\",\"nan_Uruguay\",\"nan_Venezuela\",\n",
    "\"nan_West Bank and Gaza\"]\n",
    "\n",
    "decay_4_last_12_linear_inter =[ \"nan_Dominica\",\"New Caledonia_France\",\n",
    "\"Saint Barthelemy_France\",\"nan_Gambia\",\"nan_Grenada\",\"nan_Holy See\",\n",
    "\"nan_Mauritania\",\"nan_Nicaragua\",\"nan_Saint Lucia\",\"nan_Saint Vincent and the Grenadines\",\n",
    "\"nan_Seychelles\",\"nan_Suriname\",\"nan_Mongolia\",\n",
    "\"Montserrat_United Kingdom\",\"Turks and Caicos Islands_United Kingdom\", \"Hong Kong_China\",\"Northern Territory_Australia\",\"Northwest Territories_Canada\",\"Yukon_Canada\"\n",
    ",\"Guangdong_China\",\"nan_MS Zaandam\",\"nan_Maldives\",\"nan_Saint Kitts and Nevis\",\"nan_Sao Tome and Principe\",\"nan_South Sudan\",\"nan_Western Sahara\"]\n",
    "decay_4_last_12_linear_inter_fatality=[\"nan_Somalia\"]\n",
    "\n",
    "decay_2_last_12_linear_inter =[ \"nan_Chad\",\"French Polynesia_France\",\"nan_Laos\",\"nan_Nepal\",\"nan_Sudan\",\"nan_Tanzania\",\"Bermuda_United Kingdom\",\"Cayman Islands_United Kingdom\"\n",
    ",\"nan_Cabo Verde\",\"nan_Eswatini\",\"nan_Guyana\",\"nan_Liberia\",\"nan_Malawi\",\"nan_Mozambique\",\"nan_Somalia\",\"nan_Timor-Leste\",\"Falkland Islands (Malvinas)_United Kingdom\",\n",
    "\"nan_Zimbabwe\"]\n",
    "decay_2_last_12_linear_inter_fatality=[]\n",
    "\n",
    "\n",
    "decay_1_5 =[\"nan_Cambodia\",\"nan_Croatia\",\"nan_Denmark\",\n",
    "\"nan_Bahamas\",\"nan_Bahrain\",\"nan_Barbados\" ,\"Manitoba_Canada\",\"New Brunswick_Canada\",\n",
    "\"Saskatchewan_Canada\",\"nan_France\",\"nan_Libya\",\"nan_Malta\",\n",
    "\t\"Aruba_Netherlands\",\"nan_Niger\",\"nan_Spain\",\"Virgin Islands_US\",\n",
    "\t\"Channel Islands_United Kingdom\",\"Gibraltar_United Kingdom\",\"nan_United Kingdom\",'nan_Burma',\"nan_Congo (Brazzaville)\",\n",
    "\t\"French Guiana_France\",\"Martinique_France\",\"Mayotte_France\",\"nan_Georgia\",\"nan_Iran\",\"nan_New Zealand\",\"nan_Syria\",\n",
    "\t\"nan_West Bank and Gaza\"]\n",
    "decay_1_5_fatality=[\"nan_Cuba\",\"nan_Morocco\",\"Ohio_US\",\"Pennsylvania_US\",\"Puerto Rico_US\" ,\"nan_Uzbekistan\",\"nan_Cyprus\",\"nan_France\",\"nan_Niger\",\"South Dakota_US\"]\n",
    "\n",
    "acceleratorx2=[]\n",
    "acceleratorx2_fatality=[\"Newfoundland and Labrador_Canada\"]\n",
    "\n",
    "decay_1_5_last_12_linear_inter =[\"nan_Antigua and Barbuda\",\"nan_Botswana\",\"nan_Haiti\",\"nan_Madagascar\"]\n",
    "decay_1_5_last_12_linear_inter_fatality =[]\n",
    "\n",
    "decay_1_2 =[\"nan_Albania\",\"nan_Andorra\",\"New South Wales_Australia\",\"nan_Austria\",\"nan_Azerbaijan\",\"nan_Bangladesh\",\"nan_Belize\",\"Alberta_Canada\",\"nan_Mauritius\",\"nan_Monaco\",\"nan_Montenegro\",\n",
    "\"Newfoundland and Labrador_Canada\",\"nan_Costa Rica\",\"nan_Czechia\",\"nan_Dominican Republic\",\"nan_Estonia\",\"Guadeloupe_France\",\"nan_Iceland\",\"nan_Latvia\",\"nan_Liechtenstein\",\"nan_Luxembourg\",\"nan_Norway\",\n",
    "\"nan_Poland\",\"nan_Rwanda\",\"nan_South Africa\",\"nan_Trinidad and Tobago\",\"Arizona_US\",\"California_US\",\"Colorado_US\",\"Connecticut_US\",\"Georgia_US\",\"Hawaii_US\",\"Maine_US\",\"New York_US\",\"Oklahoma_US\",\n",
    "\"Oregon_US\",\"Pennsylvania_US\",\"Tennessee_US\",\"Washington_US\",\"nan_Uruguay\",\"nan_Venezuela\",\"nan_Vietnam\"]\n",
    "decay_1_2_fatality=[\"nan_Afghanistan\",\"nan_Belarus\",\"nan_Djibouti\",\"nan_Mali\",\"nan_Mexico\",\"nan_Peru\",\"nan_Serbia\",\"Kentucky_US\",\"Puerto Rico_US\",\"Texas_US\",\"nan_Ukraine\"]\n",
    "\n",
    "decay_1_1 =[\"nan_Algeria\",\"nan_Argentina\",\"Tasmania_Australia\",\"nan_Bolivia\",\"Nova Scotia_Canada\",\"nan_Finland\",\"nan_Hungary\",\"nan_Iraq\",\n",
    "\"nan_Jordan\",\"nan_Kyrgyzstan\",\"nan_Malaysia\",\"Alabama_US\",\"Iowa_US\",\"Michigan_US\",\"Missouri_US\",\"Nebraska_US\",\"North Dakota_US \",\"Ohio_US\",\"South Carolina_US\"]\n",
    "decay_1_1_fatality=[\"nan_Afghanistan\",\"nan_Ecuador\",\"nan_Kenya\",\"nan_Korea, South\",\"nan_Kosovo\",\"Indiana_US\",\"New Mexico_US\",\"Oregon_US\",\"Rhode Island_US\",\"Tennessee_US\",\"nan_United Arab Emirates\"]\n",
    " \n",
    "decay_1_2_last_12_linear_inter =[]\n",
    "decay_1_2_last_12_linear_inter_fatality=[\"nan_Angola\" ]\n",
    "\n",
    "linear_last_12=[\"nan_Afghanistan\",\"nan_Ireland\",\"nan_Benin\"]\n",
    "linear_last_12_fatality=[\"nan_Guatemala\",\"nan_Ireland\"]\n",
    "\n",
    "decay_8_last_12_linear_inter =[ \"nan_Bhutan\",\"Prince Edward Island_Canada\",\"Greenland_Denmark\",\"nan_Fiji\",\"Saint Pierre and Miquelon_France\",\"St Martin_France\",\"nan_Namibia\",\n",
    "\"Bonaire, Sint Eustatius and Saba_Netherlands\",\"Curacao_Netherlands\",\"nan_Papua New Guinea\",\"Anguilla_United Kingdom\",\"British Virgin Islands_United Kingdom\",]\n",
    "\n",
    "decay_8_last_12_linear_inter_fatality=[]\n",
    "\n",
    "\n",
    "tr_frame=train_frame\n",
    "\n",
    "features_train=tr_frame[names].values   \n",
    "\n",
    "standard_confirmed_train=tr_frame[\"ConfirmedCases\"].values\n",
    "standard_fatalities_train=tr_frame[\"Fatalities\"].values\n",
    "current_confirmed_train=tr_frame[\"ConfirmedCases\"].values\n",
    "\n",
    "     \n",
    "\n",
    "features_cv=[]\n",
    "name_cv=[]\n",
    "standard_confirmed_cv=[]\n",
    "standard_fatalities_cv=[]\n",
    "names_=tr_frame[\"key\"].values\n",
    "training_horizon=int(features_train.shape[0]/len(unique_keys)) \n",
    "print(\"training horizon = \",training_horizon)\n",
    "for dd in range(training_horizon-1,features_train.shape[0],training_horizon):\n",
    "    features_cv.append(features_train[dd])\n",
    "    name_cv.append(names_[dd])\n",
    "    standard_confirmed_cv.append(standard_confirmed_train[dd])\n",
    "    standard_fatalities_cv.append(standard_fatalities_train[dd])\n",
    "    print (name_cv[-1], standard_confirmed_cv[-1], standard_fatalities_cv[-1])\n",
    "    \n",
    " \n",
    "\n",
    "features_cv=np.array(features_cv)\n",
    "preds_confirmed_cv=np.zeros((features_cv.shape[0],horizon))\n",
    "preds_confirmed_standard_cv=np.zeros((features_cv.shape[0],horizon))\n",
    "\n",
    "preds_fatalities_cv=np.zeros((features_cv.shape[0],horizon))\n",
    "preds_fatalities_standard_cv=np.zeros((features_cv.shape[0],horizon))\n",
    "\n",
    "overal_rmsle_metric_confirmed=0.0\n",
    "\n",
    "for j in range (preds_confirmed_cv.shape[1]):\n",
    "\n",
    "    this_features_cv=features_cv                          \n",
    "\n",
    "    preds=predict(features_cv, input_name=model_directory +\"confirmedc\"+ str(j))\n",
    "    preds_confirmed_cv[:,j]=preds\n",
    "    print (\" modelling confirmed, case %d, , original cv %d and after %d \"%(j,this_features_cv.shape[0],this_features_cv.shape[0])) \n",
    "\n",
    "predictions=[] \n",
    "for ii in range (preds_confirmed_cv.shape[0]):\n",
    "    current_prediction=standard_confirmed_cv[ii]\n",
    "    if current_prediction==0 :\n",
    "        current_prediction=0.1   \n",
    "    this_preds=preds_confirmed_cv[ii].tolist()\n",
    "    name=name_cv[ii]\n",
    "    reserve=this_preds[0]\n",
    "    #overrides\n",
    "\n",
    "\t\n",
    "    if name in normal:\n",
    "        this_preds=this_preds\t\n",
    "\t\n",
    "    elif name in decay_4_first_10_then_1:\n",
    "        this_preds=decay_4_first_10_then_1_f(this_preds)\n",
    "\t\t\n",
    "    elif name in decay_16_first_10_then_1:\n",
    "        this_preds=decay_16_first_10_then_1_f(this_preds)\t\t\n",
    "\t\t\n",
    "    elif name in decay_4_last_12_linear_inter:\n",
    "        this_preds=decay_4_last_12_linear_inter_f(this_preds)\t\t   \n",
    "        \n",
    "    elif name in decay_8_last_12_linear_inter:\n",
    "        this_preds=decay_8_last_12_linear_inter_f(this_preds)\t\t           \n",
    "        \n",
    "    elif name in decay_4:\n",
    "        this_preds=decay_4_f(this_preds)\n",
    "    \n",
    "    elif name in decay_2:\n",
    "        this_preds=decay_2_f(this_preds)\n",
    "        \n",
    "    elif name in decay_2_last_12_linear_inter:\n",
    "        this_preds=decay_2_last_12_linear_inter_f(this_preds)\n",
    "        \n",
    "    elif name in decay_1_5:\n",
    "        this_preds=decay_1_5_f(this_preds)   \n",
    "        \n",
    "    elif name in decay_1_5_last_12_linear_inter:\n",
    "        this_preds=decay_1_5_last_12_linear_inter_f(this_preds)           \n",
    "        \n",
    "        \n",
    "    elif name in decay_1_2:\n",
    "        this_preds=decay_1_2_f(this_preds)  \n",
    "        \n",
    "    elif name in decay_1_2_last_12_linear_inter:\n",
    "        this_preds=decay_1_2_last_12_linear_inter_f(this_preds)          \n",
    "        \n",
    "    elif name in decay_1_1:\n",
    "        this_preds=decay_1_1_f(this_preds)  \n",
    "        \n",
    "    elif name in linear_last_12:\n",
    "        this_preds=linear_last_12_f(this_preds)\n",
    "        \n",
    "    elif name in acceleratorx2:\n",
    "        this_preds=acceleratorx2_f(this_preds)         \n",
    "\n",
    "        \n",
    "    elif name in stay_same or  \"China\" in name:\n",
    "        this_preds=stay_same_f(this_preds)      \n",
    "\n",
    "\n",
    "    for j in range (preds_confirmed_cv.shape[1]):\n",
    "                current_prediction+=np.expm1(max(0,this_preds[j]))\n",
    "                preds_confirmed_standard_cv[ii][j]=current_prediction\n",
    "\n",
    "\n",
    "for j in range (preds_confirmed_cv.shape[1]):\n",
    "\n",
    "    this_features_cv=features_cv\n",
    "                             \n",
    "    preds=predict(features_cv, input_name=model_directory +\"fatalc\"+ str(j))\n",
    "    preds_fatalities_cv[:,j]=preds\n",
    "    print (\" modelling fatalities, case %d, original cv %d and after %d \"%( j,this_features_cv.shape[0],this_features_cv.shape[0])) \n",
    "\n",
    "predictions=[]\n",
    "for ii in range (preds_fatalities_cv.shape[0]):\n",
    "    current_prediction=standard_fatalities_cv[ii]\n",
    "        \n",
    "    this_preds=preds_fatalities_cv[ii].tolist()\n",
    "    name=name_cv[ii]\n",
    "    reserve=this_preds[0]\n",
    "    #overrides\n",
    "   \n",
    "    ####fatality special\n",
    "\t\n",
    "    if name in normal_fatality:\n",
    "        this_preds=this_preds\t \t\n",
    "\t\n",
    "    elif name in decay_4_first_10_then_1_fatality:\n",
    "        this_preds=decay_4_first_10_then_1_f(this_preds) \n",
    "\t\t\n",
    "    elif name in decay_16_first_10_then_1_fatality:\n",
    "        this_preds=decay_16_first_10_then_1_f(this_preds)\n",
    "        \n",
    "    elif name in decay_4_last_12_linear_inter_fatality:\n",
    "        this_preds=decay_4_last_12_linear_inter_f(this_preds)\t\t   \n",
    "        \n",
    "    elif name in decay_8_last_12_linear_inter_fatality:\n",
    "        this_preds=decay_8_last_12_linear_inter_f(this_preds)\t\t             \n",
    "\n",
    "    elif name in decay_4_fatality:\n",
    "        this_preds=decay_4_f(this_preds)\t\t\n",
    "\t\t\n",
    "    elif name in decay_2_fatality:\n",
    "        this_preds=decay_2_f(this_preds)        \n",
    "\n",
    "    elif name in decay_2_last_12_linear_inter_fatality:\n",
    "        this_preds=decay_2_last_12_linear_inter_f(this_preds)\n",
    "        \n",
    "    elif name in decay_1_5_fatality:\n",
    "        this_preds=decay_1_5_f(this_preds)   \t\n",
    "        \n",
    "    elif name in decay_1_5_last_12_linear_inter_fatality:\n",
    "        this_preds=decay_1_5_last_12_linear_inter_f(this_preds)         \n",
    "               \n",
    "    elif name in decay_1_2_fatality:\n",
    "        this_preds=decay_1_2_f(this_preds)   \n",
    "        \n",
    "    elif name in decay_1_2_last_12_linear_inter_fatality:\n",
    "        this_preds=decay_1_2_last_12_linear_inter_f(this_preds)        \n",
    "        \n",
    "    elif name in decay_1_1_fatality:\n",
    "        this_preds=decay_1_1_f(this_preds)         \n",
    " \n",
    "    elif name in linear_last_12_fatality:\n",
    "        this_preds=linear_last_12_f(this_preds) \n",
    "         \n",
    "    elif name in acceleratorx2_fatality:\n",
    "        this_preds=acceleratorx2_f(this_preds)\n",
    "        \n",
    "    elif name in stay_same_fatality:     \n",
    "        this_preds=stay_same_f(this_preds) \n",
    "        \n",
    "     \n",
    "    ####general   \n",
    "    elif name in normal:\n",
    "        this_preds=this_preds\t   \n",
    "\t\n",
    "    elif name in decay_4_first_10_then_1:\n",
    "        this_preds=decay_4_first_10_then_1_f(this_preds)\n",
    "\t\t\n",
    "    elif name in decay_16_first_10_then_1:\n",
    "        this_preds=decay_16_first_10_then_1_f(this_preds)\t\t\n",
    "        \n",
    "    elif name in decay_4_last_12_linear_inter:\n",
    "        this_preds=decay_4_last_12_linear_inter_f(this_preds)\t\t       \n",
    "        \n",
    "    elif name in decay_8_last_12_linear_inter:\n",
    "        this_preds=decay_8_last_12_linear_inter_f(this_preds)\t\t          \n",
    "\n",
    "    elif name in decay_4:\n",
    "        this_preds=decay_4_f(this_preds)        \n",
    "\t\t\n",
    "    elif name in decay_2:\n",
    "        this_preds=decay_2_f(this_preds)\n",
    "        \n",
    "    elif name in decay_2_last_12_linear_inter:\n",
    "        this_preds=decay_2_last_12_linear_inter_f(this_preds)\n",
    "        \n",
    "    elif name in decay_1_5:\n",
    "        this_preds=decay_1_5_f(this_preds)  \n",
    "        \n",
    "    elif name in decay_1_5_last_12_linear_inter:\n",
    "        this_preds=decay_1_5_last_12_linear_inter_f(this_preds)            \n",
    "        \n",
    "    elif name in decay_1_2:\n",
    "        this_preds=decay_1_2_f(this_preds) \n",
    "        \n",
    "    elif name in decay_1_2_last_12_linear_inter:\n",
    "        this_preds=decay_1_2_last_12_linear_inter_f(this_preds)        \n",
    "        \n",
    "    elif name in decay_1_1:\n",
    "        this_preds=decay_1_1_f(this_preds)            \n",
    "        \n",
    "    elif name in linear_last_12:\n",
    "        this_preds=linear_last_12_f(this_preds) \n",
    "        \n",
    "    elif name in acceleratorx2:\n",
    "        this_preds=acceleratorx2_f(this_preds)                 \n",
    "        \n",
    "    elif name in stay_same or  \"China\" in name:\n",
    "        this_preds=stay_same_f(this_preds)         \n",
    "          \n",
    "    \n",
    "    for j in range (preds_fatalities_cv.shape[1]):\n",
    "                    current_prediction+=np.expm1(max(0,this_preds[j]))\n",
    "                    preds_fatalities_standard_cv[ii][j]=current_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313 313 313 313\n",
      "(313, 30) (313, 30) (313, 30) (313, 30)\n"
     ]
    }
   ],
   "source": [
    "key_to_confirmed_rate={}\n",
    "key_to_fatality_rate={}\n",
    "key_to_confirmed={}\n",
    "key_to_fatality={}\n",
    "print(len(features_cv), len(name_cv),len(standard_confirmed_cv),len(standard_fatalities_cv)) \n",
    "print(preds_confirmed_cv.shape,preds_confirmed_standard_cv.shape,preds_fatalities_cv.shape,preds_fatalities_standard_cv.shape) \n",
    "\n",
    "for j in range (len(name_cv)):\n",
    "    \n",
    "    key_to_confirmed_rate[name_cv[j]]=preds_confirmed_cv[j,:].tolist()\n",
    "    #print(key_to_confirmed_rate[name_cv[j]])\n",
    "    key_to_fatality_rate[name_cv[j]]=preds_fatalities_cv[j,:].tolist()\n",
    "    key_to_confirmed[name_cv[j]]  =preds_confirmed_standard_cv[j,:].tolist()  \n",
    "    key_to_fatality[name_cv[j]]=preds_fatalities_standard_cv[j,:].tolist()  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ForecastId</th>\n",
       "      <th>Province_State</th>\n",
       "      <th>Country_Region</th>\n",
       "      <th>Date</th>\n",
       "      <th>key</th>\n",
       "      <th>ConfirmedCases</th>\n",
       "      <th>Fatalities</th>\n",
       "      <th>rate_ConfirmedCases</th>\n",
       "      <th>rate_Fatalities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-04-02</td>\n",
       "      <td>nan_Afghanistan</td>\n",
       "      <td>273.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.151899</td>\n",
       "      <td>1.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-04-03</td>\n",
       "      <td>nan_Afghanistan</td>\n",
       "      <td>281.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.029304</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-04-04</td>\n",
       "      <td>nan_Afghanistan</td>\n",
       "      <td>299.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.064057</td>\n",
       "      <td>1.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-04-05</td>\n",
       "      <td>nan_Afghanistan</td>\n",
       "      <td>349.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.167224</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-04-06</td>\n",
       "      <td>nan_Afghanistan</td>\n",
       "      <td>367.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.051576</td>\n",
       "      <td>1.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13454</th>\n",
       "      <td>13455</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2020-05-10</td>\n",
       "      <td>nan_Zimbabwe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13455</th>\n",
       "      <td>13456</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2020-05-11</td>\n",
       "      <td>nan_Zimbabwe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13456</th>\n",
       "      <td>13457</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2020-05-12</td>\n",
       "      <td>nan_Zimbabwe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13457</th>\n",
       "      <td>13458</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2020-05-13</td>\n",
       "      <td>nan_Zimbabwe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13458</th>\n",
       "      <td>13459</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2020-05-14</td>\n",
       "      <td>nan_Zimbabwe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13459 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ForecastId Province_State Country_Region       Date              key  \\\n",
       "0               1            NaN    Afghanistan 2020-04-02  nan_Afghanistan   \n",
       "1               2            NaN    Afghanistan 2020-04-03  nan_Afghanistan   \n",
       "2               3            NaN    Afghanistan 2020-04-04  nan_Afghanistan   \n",
       "3               4            NaN    Afghanistan 2020-04-05  nan_Afghanistan   \n",
       "4               5            NaN    Afghanistan 2020-04-06  nan_Afghanistan   \n",
       "...           ...            ...            ...        ...              ...   \n",
       "13454       13455            NaN       Zimbabwe 2020-05-10     nan_Zimbabwe   \n",
       "13455       13456            NaN       Zimbabwe 2020-05-11     nan_Zimbabwe   \n",
       "13456       13457            NaN       Zimbabwe 2020-05-12     nan_Zimbabwe   \n",
       "13457       13458            NaN       Zimbabwe 2020-05-13     nan_Zimbabwe   \n",
       "13458       13459            NaN       Zimbabwe 2020-05-14     nan_Zimbabwe   \n",
       "\n",
       "       ConfirmedCases  Fatalities  rate_ConfirmedCases  rate_Fatalities  \n",
       "0               273.0         6.0             1.151899         1.500000  \n",
       "1               281.0         6.0             1.029304         1.000000  \n",
       "2               299.0         7.0             1.064057         1.166667  \n",
       "3               349.0         7.0             1.167224         1.000000  \n",
       "4               367.0        11.0             1.051576         1.571429  \n",
       "...               ...         ...                  ...              ...  \n",
       "13454             NaN         NaN                  NaN              NaN  \n",
       "13455             NaN         NaN                  NaN              NaN  \n",
       "13456             NaN         NaN                  NaN              NaN  \n",
       "13457             NaN         NaN                  NaN              NaN  \n",
       "13458             NaN         NaN                  NaN              NaN  \n",
       "\n",
       "[13459 rows x 9 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_new=train[[\"Date\",\"ConfirmedCases\",\"Fatalities\",\"key\",\"rate_ConfirmedCases\",\"rate_Fatalities\"]]\n",
    "\n",
    "test_new_count=pd.merge(test,train_new, how=\"left\", left_on=[\"key\",\"Date\"], right_on=[\"key\",\"Date\"] ).reset_index(drop=True)\n",
    "test_new_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 13 30\n",
      "13459 13459 13 30 313\n",
      "13459 13459 13 30 313\n",
      "13459 13459 13 30 313\n",
      "13459 13459 13 30 313\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ForecastId</th>\n",
       "      <th>Province_State</th>\n",
       "      <th>Country_Region</th>\n",
       "      <th>Date</th>\n",
       "      <th>key</th>\n",
       "      <th>ConfirmedCases</th>\n",
       "      <th>Fatalities</th>\n",
       "      <th>rate_ConfirmedCases</th>\n",
       "      <th>rate_Fatalities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-04-02</td>\n",
       "      <td>nan_Afghanistan</td>\n",
       "      <td>273.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.151899</td>\n",
       "      <td>1.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-04-03</td>\n",
       "      <td>nan_Afghanistan</td>\n",
       "      <td>281.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.029304</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-04-04</td>\n",
       "      <td>nan_Afghanistan</td>\n",
       "      <td>299.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.064057</td>\n",
       "      <td>1.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-04-05</td>\n",
       "      <td>nan_Afghanistan</td>\n",
       "      <td>349.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.167224</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-04-06</td>\n",
       "      <td>nan_Afghanistan</td>\n",
       "      <td>367.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1.051576</td>\n",
       "      <td>1.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13454</th>\n",
       "      <td>13455</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2020-05-10</td>\n",
       "      <td>nan_Zimbabwe</td>\n",
       "      <td>103.820455</td>\n",
       "      <td>9.316568</td>\n",
       "      <td>1.113139</td>\n",
       "      <td>1.070582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13455</th>\n",
       "      <td>13456</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2020-05-11</td>\n",
       "      <td>nan_Zimbabwe</td>\n",
       "      <td>105.820628</td>\n",
       "      <td>9.458326</td>\n",
       "      <td>1.115761</td>\n",
       "      <td>1.085178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13456</th>\n",
       "      <td>13457</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2020-05-12</td>\n",
       "      <td>nan_Zimbabwe</td>\n",
       "      <td>107.179766</td>\n",
       "      <td>9.554270</td>\n",
       "      <td>1.117193</td>\n",
       "      <td>1.087802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13457</th>\n",
       "      <td>13458</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2020-05-13</td>\n",
       "      <td>nan_Zimbabwe</td>\n",
       "      <td>107.868063</td>\n",
       "      <td>9.602728</td>\n",
       "      <td>1.091605</td>\n",
       "      <td>1.079062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13458</th>\n",
       "      <td>13459</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2020-05-14</td>\n",
       "      <td>nan_Zimbabwe</td>\n",
       "      <td>107.868063</td>\n",
       "      <td>9.602728</td>\n",
       "      <td>1.105040</td>\n",
       "      <td>1.128450</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13459 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ForecastId Province_State Country_Region       Date              key  \\\n",
       "0               1            NaN    Afghanistan 2020-04-02  nan_Afghanistan   \n",
       "1               2            NaN    Afghanistan 2020-04-03  nan_Afghanistan   \n",
       "2               3            NaN    Afghanistan 2020-04-04  nan_Afghanistan   \n",
       "3               4            NaN    Afghanistan 2020-04-05  nan_Afghanistan   \n",
       "4               5            NaN    Afghanistan 2020-04-06  nan_Afghanistan   \n",
       "...           ...            ...            ...        ...              ...   \n",
       "13454       13455            NaN       Zimbabwe 2020-05-10     nan_Zimbabwe   \n",
       "13455       13456            NaN       Zimbabwe 2020-05-11     nan_Zimbabwe   \n",
       "13456       13457            NaN       Zimbabwe 2020-05-12     nan_Zimbabwe   \n",
       "13457       13458            NaN       Zimbabwe 2020-05-13     nan_Zimbabwe   \n",
       "13458       13459            NaN       Zimbabwe 2020-05-14     nan_Zimbabwe   \n",
       "\n",
       "       ConfirmedCases  Fatalities  rate_ConfirmedCases  rate_Fatalities  \n",
       "0          273.000000    6.000000             1.151899         1.500000  \n",
       "1          281.000000    6.000000             1.029304         1.000000  \n",
       "2          299.000000    7.000000             1.064057         1.166667  \n",
       "3          349.000000    7.000000             1.167224         1.000000  \n",
       "4          367.000000   11.000000             1.051576         1.571429  \n",
       "...               ...         ...                  ...              ...  \n",
       "13454      103.820455    9.316568             1.113139         1.070582  \n",
       "13455      105.820628    9.458326             1.115761         1.085178  \n",
       "13456      107.179766    9.554270             1.117193         1.087802  \n",
       "13457      107.868063    9.602728             1.091605         1.079062  \n",
       "13458      107.868063    9.602728             1.105040         1.128450  \n",
       "\n",
       "[13459 rows x 9 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fillin_columns(frame,key_column, original_name, training_horizon, test_horizon, unique_values, key_to_values):\n",
    "    keys=frame[key_column].values\n",
    "    original_values=frame[original_name].values.tolist()\n",
    "    print(len(keys), len(original_values), training_horizon ,test_horizon,len(key_to_values))\n",
    "    \n",
    "    for j in range(unique_values):\n",
    "        current_index=(j * (training_horizon +test_horizon )) +training_horizon \n",
    "        current_key=keys[current_index]\n",
    "        values=key_to_values[current_key]\n",
    "        co=0\n",
    "        for g in range(current_index, current_index + test_horizon):\n",
    "            original_values[g]=values[co]\n",
    "            co+=1\n",
    "    \n",
    "    frame[original_name]=original_values\n",
    " \n",
    "\n",
    "all_days=int(test_new.shape[0]/len(unique_keys))\n",
    "\n",
    "tr_horizon=all_days-horizon\n",
    "print(all_days,tr_horizon, horizon )\n",
    "\n",
    "fillin_columns(test_new_count,\"key\", 'ConfirmedCases', tr_horizon, horizon, len(unique_keys), key_to_confirmed)    \n",
    "fillin_columns(test_new_count,\"key\", 'Fatalities', tr_horizon, horizon, len(unique_keys), key_to_fatality)   \n",
    "fillin_columns(test_new_count,\"key\", 'rate_ConfirmedCases', tr_horizon, horizon, len(unique_keys), key_to_confirmed_rate)   \n",
    "fillin_columns(test_new_count,\"key\", 'rate_Fatalities', tr_horizon, horizon, len(unique_keys), key_to_fatality_rate)   \n",
    "\n",
    "######################HERE WE ADD THE COUNT MODEL, WHERE THE COUNTS IN THE RATE MODEL ARE ZERO ##################\n",
    "\n",
    "test_dat=test_new['Fatalities'].values.tolist()\n",
    "test_new_countfat=test_new_count['Fatalities'].values.tolist()\n",
    "for j in range(len(test_dat)):\n",
    "    if test_dat[j]<1:\n",
    "        test_dat[j]=test_new_countfat[j]        \n",
    "\n",
    "test_new2= test_new.copy()\n",
    "test_new2['Fatalities']=test_dat\n",
    "test_new2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "submission=test_new2[[\"ForecastId\",\"ConfirmedCases\",\"Fatalities\"]]\n",
    "\n",
    "submission.to_csv(\"submission.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
