{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Necessary Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-01-26T11:55:33.496042Z",
     "iopub.status.busy": "2023-01-26T11:55:33.49553Z",
     "iopub.status.idle": "2023-01-26T11:55:34.625979Z",
     "shell.execute_reply": "2023-01-26T11:55:34.625067Z",
     "shell.execute_reply.started": "2023-01-26T11:55:33.495941Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('max_columns', 100)\n",
    "sns.set_theme(style='dark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-01-26T11:55:34.629036Z",
     "iopub.status.busy": "2023-01-26T11:55:34.628212Z",
     "iopub.status.idle": "2023-01-26T11:55:41.233659Z",
     "shell.execute_reply": "2023-01-26T11:55:41.232419Z",
     "shell.execute_reply.started": "2023-01-26T11:55:34.62899Z"
    }
   },
   "outputs": [],
   "source": [
    "BASE_DIR = 'playground-series-s3e4.csv'\n",
    "train = pd.read_csv(BASE_DIR+'train.csv')\n",
    "test = pd.read_csv(BASE_DIR+'test.csv')\n",
    "sub = pd.read_csv(BASE_DIR+'sample_submission.csv')\n",
    "\n",
    "train['Source'] = 'Train'\n",
    "test['Source'] = 'Test'\n",
    "all_ = pd.concat([train, test], axis=0).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transactions between Train and Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By converting the `Time` feature into Day and Hour, we find that the train and test set comprise of the transactions of 2 days.\n",
    "\n",
    "##### The train set has all Day 1 transactions, and some of Day 2\n",
    "##### Whiel the test set has only Day 2 transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-01-26T11:55:41.235407Z",
     "iopub.status.busy": "2023-01-26T11:55:41.23503Z",
     "iopub.status.idle": "2023-01-26T11:55:41.586732Z",
     "shell.execute_reply": "2023-01-26T11:55:41.585396Z",
     "shell.execute_reply.started": "2023-01-26T11:55:41.235375Z"
    }
   },
   "outputs": [],
   "source": [
    "seconds_per_day = 3600*24\n",
    "\n",
    "all_[\"Day\"] = all_[\"Time\"].apply(lambda x: 1 if x<seconds_per_day else 2)\n",
    "all_[\"Hour\"] = all_[\"Time\"].apply(lambda x: (x%seconds_per_day)//3600 + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-01-26T14:04:07.315446Z",
     "iopub.status.busy": "2023-01-26T14:04:07.314953Z",
     "iopub.status.idle": "2023-01-26T14:04:07.724776Z",
     "shell.execute_reply": "2023-01-26T14:04:07.722869Z",
     "shell.execute_reply.started": "2023-01-26T14:04:07.315404Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.countplot(data=all_, x='Day', hue='Source', palette=['#86A3B8',\"#F55050\"]);\n",
    "plt.ylabel('Transactions Count')\n",
    "plt.title('Transaction per Day');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If we look further into the Hour distribution of transactions among both datasets, we find that\n",
    "##### 1. Test set is comprised of all Day 2 transactions starting from Hour 10 (Possibly 10.5)\n",
    "##### 2. Train set is comprised of all Day 1 transactions, and Day 2 transactions until Hour 10 (Again possibly 10.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-01-26T14:05:52.56166Z",
     "iopub.status.busy": "2023-01-26T14:05:52.560604Z",
     "iopub.status.idle": "2023-01-26T14:05:53.505139Z",
     "shell.execute_reply": "2023-01-26T14:05:53.50392Z",
     "shell.execute_reply.started": "2023-01-26T14:05:52.561612Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(25, 5), sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "sns.countplot(data=all_.query('Source == \"Train\"'), palette=['#86A3B8',\"#F55050\"], x='Hour', hue='Day', ax=axes[0]);\n",
    "axes[0].set_title('Transactions per Hour in Train')\n",
    "axes[0].set_ylabel('Transactions Count')\n",
    "axes[0].set_xticklabels([str(i) for i in range(1, 25)])\n",
    "\n",
    "sns.countplot(data=all_.query('Source == \"Test\"'), palette=[\"#F55050\"], x='Hour', hue='Day', ax=axes[1]);\n",
    "axes[1].set_title('Transactions per Hour Test')\n",
    "axes[1].set_ylabel('')\n",
    "axes[1].set_xticklabels([str(i) for i in range(10, 25)]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we know that the Test set comprises of the last 14 hours of Day 2, and we have a similar subset in the Train set which is the last 14 hours of Day 1. \n",
    "\n",
    "##### Let's categorize the transactions based on this and check the distribution of fraudulent transactions over the Train set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-26T14:12:33.229992Z",
     "iopub.status.busy": "2023-01-26T14:12:33.229538Z",
     "iopub.status.idle": "2023-01-26T14:12:33.291066Z",
     "shell.execute_reply": "2023-01-26T14:12:33.289643Z",
     "shell.execute_reply.started": "2023-01-26T14:12:33.229953Z"
    }
   },
   "outputs": [],
   "source": [
    "all_.loc[(all_['Day'] == 1) & (all_['Hour'] <= 10), 'time_category'] = 'Day1_0'\n",
    "all_.loc[(all_['Day'] == 1) & (all_['Hour'] > 10), 'time_category'] = 'Day1_1'\n",
    "all_.loc[(all_['Day'] == 2) & (all_['Hour'] <= 10), 'time_category'] = 'Day2_0'\n",
    "all_.loc[(all_['Day'] == 2) & (all_['Hour'] > 10), 'time_category'] = 'Day2_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-01-26T14:20:21.297331Z",
     "iopub.status.busy": "2023-01-26T14:20:21.29688Z",
     "iopub.status.idle": "2023-01-26T14:20:21.546291Z",
     "shell.execute_reply": "2023-01-26T14:20:21.544949Z",
     "shell.execute_reply.started": "2023-01-26T14:20:21.297283Z"
    }
   },
   "outputs": [],
   "source": [
    "all_.query('Source == \"Train\"').groupby('time_category').Class.sum().plot(kind='barh', color='#F55050', title='Fraudulent Transactions Over Day Periods', xlabel='Count', ylabel='Day Period', figsize=(10, 6));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Out of 372 fraud transactions on Day1, 310 appear on the second partition. \n",
    "\n",
    "##### So should we still cross validate over the **ENTIRE** Train set?\n",
    "##### I don't think so, because take a look at the distribution of fraudulent transactions over the Train set.\n",
    "\n",
    "Since the bulk of the transactions happen on Day1_1, then the cross validation should be only on Day1_1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing CV and LB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's train 2 models (LogisticRegression) using this approach.\n",
    "\n",
    "##### In the first model, I'll use StratifiedKfold on Day1_1, and I'll add Day1_0 and Day2_1 to all folds for training, and in the second I'll do the same while dropping some features, then I'll blend their results and compare CV with LB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-26T12:48:36.607483Z",
     "iopub.status.busy": "2023-01-26T12:48:36.606256Z",
     "iopub.status.idle": "2023-01-26T12:48:36.614003Z",
     "shell.execute_reply": "2023-01-26T12:48:36.612906Z",
     "shell.execute_reply.started": "2023-01-26T12:48:36.607423Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-26T13:05:12.911555Z",
     "iopub.status.busy": "2023-01-26T13:05:12.91113Z",
     "iopub.status.idle": "2023-01-26T13:06:34.500227Z",
     "shell.execute_reply": "2023-01-26T13:06:34.498997Z",
     "shell.execute_reply.started": "2023-01-26T13:05:12.911521Z"
    }
   },
   "outputs": [],
   "source": [
    "drop_feats = ['V1', 'V3', 'V7', 'V10', 'V16', 'V21', 'V22', 'V23', 'V25', 'V28']\n",
    "\n",
    "X_day1_1 = all_.query('Source == \"Train\" and time_category == \"Day1_1\"').drop(['Source', 'time_category', 'Class'], axis=1).copy().reset_index(drop=True)\n",
    "y_day1_1 = all_.query('Source == \"Train\" and time_category == \"Day1_1\"').Class.copy().reset_index(drop=True)\n",
    "\n",
    "X_rest = all_.query('Source == \"Train\" and time_category in [\"Day1_0\", \"Day2_0\"]').drop(['Source', 'time_category', 'Class'], axis=1).copy().reset_index(drop=True)\n",
    "y_rest = all_.query('Source == \"Train\" and time_category in [\"Day1_0\", \"Day2_0\"]').Class.copy().reset_index(drop=True)\n",
    "\n",
    "X_test = all_.query('Source == \"Test\"').drop(['Source', 'time_category', 'Class'], axis=1).copy().reset_index(drop=True)\n",
    "\n",
    "n_splits = 10\n",
    "\n",
    "skf = StratifiedKFold(n_splits = n_splits)\n",
    "\n",
    "oof_preds1 = np.zeros_like(y_day1_1, dtype=np.float64)\n",
    "oof_preds2 = np.zeros_like(y_day1_1, dtype=np.float64)\n",
    "\n",
    "oof_test1 = np.zeros((len(test)), dtype=np.float64)\n",
    "oof_test2 = np.zeros((len(test)), dtype=np.float64)\n",
    "\n",
    "scores1 = []\n",
    "scores2 = []\n",
    "\n",
    "for i, (train_idx, val_idx) in enumerate(tqdm(skf.split(X_day1_1, y_day1_1), total=10)):\n",
    "    # Split the data\n",
    "    X_train, X_val = X_day1_1.loc[train_idx], X_day1_1.loc[val_idx]\n",
    "    y_train, y_val = y_day1_1.loc[train_idx], y_day1_1.loc[val_idx]\n",
    "    \n",
    "    # Create two datasets\n",
    "    X_train_2 = X_train.drop(drop_feats, axis=1)\n",
    "    X_val_2 = X_val.drop(drop_feats, axis=1)\n",
    "    \n",
    "    # Fit robust scaler\n",
    "    scaler1 = RobustScaler().fit(X_train)\n",
    "    scaler2 = RobustScaler().fit(X_train_2)\n",
    "    \n",
    "    # Append Day1_0 and Day2_0\n",
    "    X_train = pd.concat([X_train, X_rest], axis=0)\n",
    "    X_train_2 = pd.concat([X_train_2, X_rest.drop(drop_feats, axis=1)], axis=0)    \n",
    "    y_train = pd.concat([y_train, y_rest], axis=0)\n",
    "    \n",
    "    # Scale the datasets\n",
    "    X_train = scaler1.transform(X_train)\n",
    "    X_val = scaler1.transform(X_val)\n",
    "    \n",
    "    X_train_2 = scaler2.transform(X_train_2)\n",
    "    X_val_2 = scaler2.transform(X_val_2)\n",
    "    \n",
    "    # Instantitate and fit model 1\n",
    "    model1 = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    model1.fit(X_train, y_train)\n",
    "    \n",
    "    # Validate model 1\n",
    "    val_preds1 = model1.predict_proba(X_val)[:, 1]\n",
    "    score1 = roc_auc_score(y_val, val_preds1)\n",
    "    scores1.append(score1)\n",
    "        \n",
    "    # Instantitate and fit model 2 \n",
    "    model2 = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    model2.fit(X_train_2, y_train)\n",
    "    \n",
    "    # Validate model 2\n",
    "    val_preds2 = model2.predict_proba(X_val_2)[:, 1]\n",
    "    score2 = roc_auc_score(y_val, val_preds2)\n",
    "    scores2.append(score2)\n",
    "    \n",
    "    # OOF preds\n",
    "    oof_preds1[val_idx] += val_preds1\n",
    "    oof_preds2[val_idx] += val_preds2\n",
    "    \n",
    "    # OOF test\n",
    "    X_test1 = scaler1.transform(X_test)\n",
    "    X_test2 = scaler2.transform(X_test.drop(drop_feats, axis=1))\n",
    "    \n",
    "    oof_test1 += model1.predict_proba(X_test1)[:, 1] / n_splits\n",
    "    oof_test2 += model2.predict_proba(X_test2)[:, 1] / n_splits\n",
    "    \n",
    "    print(f'Fold {i} Score with features:', score1)\n",
    "    print(f'Fold {i} Score without features:', score2)\n",
    "    print()\n",
    "    \n",
    "print(f'OOF Score with features:', roc_auc_score(y_day1_1, oof_preds1))\n",
    "print(f'OOF Score without features:', roc_auc_score(y_day1_1, oof_preds2))\n",
    "print(f'OOF Score blend:', roc_auc_score(y_day1_1, (oof_preds1+oof_preds2)/2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So dropping these features reduced the performance of the model. I now wonder if dropping them would result in the same over the LB, and that would also help me to understand whether this validation scheme is coherent with the LB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-26T13:06:34.502454Z",
     "iopub.status.busy": "2023-01-26T13:06:34.502087Z",
     "iopub.status.idle": "2023-01-26T13:06:35.585147Z",
     "shell.execute_reply": "2023-01-26T13:06:35.584131Z",
     "shell.execute_reply.started": "2023-01-26T13:06:34.50242Z"
    }
   },
   "outputs": [],
   "source": [
    "sub['Class'] = oof_test1\n",
    "sub.to_csv('submission1.csv', index=False)\n",
    "\n",
    "sub['Class'] = oof_test2\n",
    "sub.to_csv('submission2.csv', index=False)\n",
    "\n",
    "sub['Class'] = (oof_test1 + oof_test2) / 2\n",
    "sub.to_csv('submission3.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing OOF with LB results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-26T13:10:02.482812Z",
     "iopub.status.busy": "2023-01-26T13:10:02.482409Z",
     "iopub.status.idle": "2023-01-26T13:10:02.741422Z",
     "shell.execute_reply": "2023-01-26T13:10:02.740223Z",
     "shell.execute_reply.started": "2023-01-26T13:10:02.482779Z"
    }
   },
   "outputs": [],
   "source": [
    "oof = [0.783, 0.746, 0.779]\n",
    "lb = [0.814, 0.778, 0.809]\n",
    "\n",
    "plt.plot(oof, color='b')\n",
    "plt.plot(lb, color='r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This CV is perfectly correlated with the LB."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
